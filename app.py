# =========================
# OPS2TBM â€” ì™„ì „ ë¬´ë£Œ ì•ˆì •íŒ (DOCX ë‚´ë³´ë‚´ê¸° ì˜¤ë¥˜ ìˆ˜ì • í¬í•¨)
# - í…ìŠ¤íŠ¸ PDF / í…ìŠ¤íŠ¸ ë¶™ì—¬ë„£ê¸° / ZIP(ì—¬ëŸ¬ PDF) ì§€ì›
# - í…œí”Œë¦¿: ìë™/ì‚¬ê³ ì‚¬ë¡€í˜•/ê°€ì´ë“œí˜• (ìˆ˜ë™ ì„ íƒ ê°€ëŠ¥)
# - AI ìš”ì•½: ìˆœìˆ˜ NumPy TextRank + MMR(ì¤‘ë³µ ì–µì œ)  â† ë¬´ë£Œ
# - ìì—°ìŠ¤ëŸ¬ìš´ êµìœ¡ëŒ€ë³¸(ë¬´ë£Œ ëª¨ë“œ): ê·œì¹™ ê¸°ë°˜ ë¬¸ì²´ ë³€í™˜/êµ¬ì„±  â† ë¬´ë£Œ(LLM/API ç„¡)
# - í—¤ë”/ì¤‘ë³µ ì œê±°, ë™ì‚¬ ìš°ì„  ì„ ë³„, ì§ˆë¬¸í˜• ë³€í™˜
# - AIë¡œ ë½‘íŒ ë¬¸ì¥ â­[AI] ê°•ì¡° í‘œì‹œ
# - ğŸ§¹ ì´ˆê¸°í™” ë²„íŠ¼(ì—…ë¡œë” í¬í•¨)
# - âœ… DOCX ë‚´ë³´ë‚´ê¸° ì‹œ XML ê¸ˆì§€ë¬¸ì ì œê±°(ì˜¤ë¥˜ í•´ê²°)
# =========================

import io
import zipfile
from typing import List, Tuple, Dict

import streamlit as st
from docx import Document
from docx.shared import Pt
from pdfminer.high_level import extract_text as pdf_extract_text
import pypdfium2 as pdfium
import numpy as np
import regex as rxx

# ----------------------------
# ì„¸ì…˜ ìƒíƒœ í‚¤ (ì—…ë¡œë” ì´ˆê¸°í™”ìš©)
# ----------------------------
if "uploader_key" not in st.session_state:
    st.session_state.uploader_key = 0

# ----------------------------
# ê³µí†µ ìœ í‹¸
# ----------------------------
HEADER_HINTS = [
    "ì˜ˆë°©ì¡°ì¹˜", "5ëŒ€ ê¸°ë³¸ìˆ˜ì¹™", "ì‘ê¸‰ì¡°ì¹˜", "ë¯¼ê°êµ°", "ì²´ê°ì˜¨ë„",
    "ë¬¼Â·ê·¸ëŠ˜Â·íœ´ì‹", "ë¬¼, ê·¸ëŠ˜, íœ´ì‹", "ë¬¼ ê·¸ëŠ˜ íœ´ì‹",
    "ë¬¼Â·ë°”ëŒÂ·íœ´ì‹", "ë¬¼, ë°”ëŒ, íœ´ì‹", "ë¬¼ ë°”ëŒ íœ´ì‹",
    "ìœ„ê¸°íƒˆì¶œ ì•ˆì „ë³´ê±´ ì•±", "ì²´ê°ì˜¨ë„ ê³„ì‚°ê¸°"
]
ACTION_VERBS = [
    "ì„¤ì¹˜","ë°°ì¹˜","ì°©ìš©","ì ê²€","í™•ì¸","ì¸¡ì •","ê¸°ë¡","í‘œì‹œ",
    "ì œê³µ","ë¹„ì¹˜","ë³´ê³ ","ì‹ ê³ ","êµìœ¡","ì£¼ì§€","ì¤‘ì§€","í†µì œ",
    "ì§€ì›","íœ´ì‹","íœ´ê²Œ","ì´ë™","í›„ì†¡","ëƒ‰ê°","ê³µê¸‰","í‘œì§€","í‘œì‹œ"
]

def normalize_text(text: str) -> str:
    text = text.replace("\x0c", "\n")
    text = rxx.sub(r"[ \t]+\n", "\n", text)
    text = rxx.sub(r"\n{3,}", "\n\n", text)
    return text.strip()

def read_pdf_text(file_bytes: bytes) -> str:
    try:
        with io.BytesIO(file_bytes) as bio:
            text = pdf_extract_text(bio) or ""
    except Exception:
        text = ""
    text = normalize_text(text)

    if len(text.strip()) < 10:
        try:
            with io.BytesIO(file_bytes) as bio:
                pdf = pdfium.PdfDocument(bio)
                pages = len(pdf)
            if pages > 0 and not text.strip():
                st.warning("ì´ PDFëŠ” ì´ë¯¸ì§€/ìŠ¤ìº” ê¸°ë°˜ìœ¼ë¡œ ë³´ì—¬ìš”. í˜„ì¬ ë²„ì „ì€ OCR ì—†ì´ 'í…ìŠ¤íŠ¸'ë§Œ ì²˜ë¦¬í•©ë‹ˆë‹¤.")
        except Exception:
            pass
    return text

def split_sentences_ko(text: str) -> List[str]:
    raw = rxx.split(r"(?<=[\.!\?]|ë‹¤\.)\s+|\n+", text)
    sents = [s.strip(" -â€¢â—â–ªâ–¶â–·\t") for s in raw if s and len(s.strip()) > 1]
    sents = [s for s in sents if len(s) >= 6]
    return sents

def simple_tokens(s: str) -> List[str]:
    s = s.lower()
    return rxx.findall(r"[ê°€-í£a-z0-9]{2,}", s)

def has_action_verb(s: str) -> bool:
    return any(v in s for v in ACTION_VERBS) or bool(rxx.search(r"(í•´ì•¼\s*í•©ë‹ˆë‹¤|ì‹­ì‹œì˜¤|í•©ì‹œë‹¤|í•˜ì„¸ìš”)", s))

def is_header_like(s: str) -> bool:
    if len(s) <= 10 and not has_action_verb(s):
        return True
    if not has_action_verb(s) and any(h in s for h in HEADER_HINTS):
        return True
    if not rxx.search(r"[\.!\?ë‹¤]$", s) and not has_action_verb(s) and len(s) < 20:
        return True
    return False

def normalize_for_dedup(s: str) -> str:
    s2 = rxx.sub(r"\s+", "", s)
    s2 = rxx.sub(r"(..)\1{1,}", r"\1", s2)  # 2ê¸€ì ì´ìƒ ë°˜ë³µ ì¶•ì•½
    return s2

# ----------------------------
# ìˆœìˆ˜ NumPy TextRank + MMR ë‹¤ì–‘ì„± (ë¬´ë£Œ)
# ----------------------------
def sentence_tfidf_vectors(sents: List[str]) -> Tuple[np.ndarray, List[str]]:
    toks_list = [simple_tokens(s) for s in sents]
    vocab: Dict[str, int] = {}
    for toks in toks_list:
        for t in toks:
            if t not in vocab:
                vocab[t] = len(vocab)
    if not vocab:
        return np.zeros((len(sents), 0), dtype=np.float32), []
    mat = np.zeros((len(sents), len(vocab)), dtype=np.float32)
    df = np.zeros((len(vocab),), dtype=np.float32)
    for i, toks in enumerate(toks_list):
        for t in toks:
            j = vocab[t]
            mat[i, j] += 1.0
        for t in set(toks):
            df[vocab[t]] += 1.0
    N = float(len(sents))
    idf = np.log((N + 1.0) / (df + 1.0)) + 1.0
    mat *= idf
    norms = np.linalg.norm(mat, axis=1, keepdims=True) + 1e-8
    mat = mat / norms
    return mat, list(vocab.keys())

def cosine_sim_matrix(X: np.ndarray) -> np.ndarray:
    if X.size == 0:
        return np.zeros((X.shape[0], X.shape[0]), dtype=np.float32)
    sim = np.clip(X @ X.T, 0.0, 1.0)
    np.fill_diagonal(sim, 0.0)
    return sim

def textrank_scores(sents: List[str], d: float = 0.85, max_iter: int = 60, tol: float = 1e-4) -> List[float]:
    n = len(sents)
    if n == 0: return []
    if n == 1: return [1.0]
    X, _ = sentence_tfidf_vectors(sents)
    W = cosine_sim_matrix(X)
    row_sums = W.sum(axis=1, keepdims=True)
    P = np.divide(W, row_sums, out=np.zeros_like(W), where=row_sums > 0)
    r = np.ones((n, 1), dtype=np.float32) / n
    teleport = np.ones((n, 1), dtype=np.float32) / n
    for _ in range(max_iter):
        r_new = d * (P.T @ r) + (1 - d) * teleport
        if np.linalg.norm(r_new - r, ord=1) < tol:
            r = r_new; break
        r = r_new
    return [float(v) for v in r.flatten()]

def mmr_select(cands: List[str], scores: List[float], X: np.ndarray, k: int, lambda_: float = 0.7) -> List[int]:
    if not cands: return []
    selected: List[int] = []
    remaining = set(range(len(cands)))
    sim = cosine_sim_matrix(X)
    while remaining and len(selected) < k:
        best_idx, best_val = None, -1e9
        for i in remaining:
            rel = scores[i]
            div = max((sim[i, j] for j in selected), default=0.0)
            mmr = lambda_ * rel - (1 - lambda_) * div
            if mmr > best_val:
                best_val, best_idx = mmr, i
        selected.append(best_idx)
        remaining.remove(best_idx)
    return selected

# ----------------------------
# ì„ íƒ ë¡œì§ (ê·œì¹™/AI) â€” í—¤ë” ì œê±°/ë™ì‚¬ ìš°ì„ /ì¤‘ë³µ ì œê±° + AI ê°•ì¡°
# ----------------------------
def filter_candidates(sents: List[str]) -> List[str]:
    out = []
    seen = set()
    for s in sents:
        if is_header_like(s): continue
        key = normalize_for_dedup(s)
        if key in seen: continue
        seen.add(key)
        out.append(s.strip())
    return out

def pick_rule(sents: List[str], keywords: List[str], limit: int) -> Tuple[List[str], List[bool]]:
    base = filter_candidates(sents)
    base = sorted(base, key=lambda x: (not has_action_verb(x), len(x)))  # ë™ì‚¬O ë¨¼ì €
    hits = [s for s in base if any(k in s for k in keywords)]
    out = hits[:limit]
    flags = [False] * len(out)
    if len(out) < limit:
        remain = [s for s in base if s not in out]
        remain = sorted(remain, key=lambda x: (not has_action_verb(x), len(x)))
        add = remain[: max(0, limit - len(out))]
        out.extend(add); flags.extend([False]*len(add))
    return out, flags

def pick_tr(sents: List[str], keywords: List[str], limit: int) -> Tuple[List[str], List[bool]]:
    base = filter_candidates(sents)
    if not base: return [], []
    scores = textrank_scores(base)
    scores = np.array(scores, dtype=np.float32)
    if keywords:
        for i, s in enumerate(base):
            if any(k in s for k in keywords): scores[i] += 0.2
            if has_action_verb(s): scores[i] += 0.1
    X, _ = sentence_tfidf_vectors(base)
    idx = mmr_select(base, scores.tolist(), X, limit, lambda_=0.7)
    out = [base[i] for i in idx]
    flags = [True] * len(out)
    return out, flags

def render_with_marks(lines: List[str], ai_flags: List[bool]) -> List[str]:
    return [f"- {'â­[AI] ' if ai else ''}{s}" for s, ai in zip(lines, ai_flags)]

# ----------------------------
# í…œí”Œë¦¿/í‚¤ì›Œë“œ + ìë™ ë¶„ë¥˜ ë³´ì •
# ----------------------------
KW_GUIDE_CORE = ["ê°€ì´ë“œ","ì•ˆë‚´","ë³´í˜¸","ê±´ê°•","ëŒ€ì‘","ì ˆì°¨","ì§€ì¹¨","ë§¤ë‰´ì–¼","ì˜ˆë°©","ìƒë‹´","ì§€ì›","ì¡´ì¤‘","ë¯¼ê°êµ°"]
KW_GUIDE_STEP = ["ì ˆì°¨","ìˆœì„œ","ë°©ë²•","ì ê²€","í™•ì¸","ë³´ê³ ","ì¡°ì¹˜","ê¸°ë¡","íœ´ì‹","ê³µê¸‰","ì œê³µ","ë¹„ì¹˜"]
KW_GUIDE_QA   = ["ì§ˆë¬¸","ì™œ","ì–´ë–»ê²Œ","ë¬´ì—‡","ì£¼ì˜","í™•ì¸í• ", "í† ì˜"]

KW_ACC_CORE = ["ì‚¬ê³ ","ì¬í•´","ìœ„í—˜","ì›ì¸","ì˜ˆë°©","ëŒ€ì±…","ë…¸í›„","ì¶”ë½","í˜‘ì°©","ê°ì „","í™”ì¬","ì§ˆì‹","ì¤‘ë…"]
KW_ACC_STEP = ["ë°œìƒ","ê²½ìœ„","ì¡°ì¹˜","ê°œì„ ","êµìœ¡","ì„¤ì¹˜","ë°°ì¹˜","ì ê²€","ê´€ë¦¬"]

GUIDE_STRONG_HINTS = [
    "ë¬¼Â·ê·¸ëŠ˜Â·íœ´ì‹","ë¬¼, ê·¸ëŠ˜, íœ´ì‹","ë¬¼ ê·¸ëŠ˜ íœ´ì‹",
    "ë¬¼Â·ë°”ëŒÂ·íœ´ì‹","ë¬¼, ë°”ëŒ, íœ´ì‹","ë¬¼ ë°”ëŒ íœ´ì‹",
    "ë³´ëƒ‰ì¥êµ¬","ì‘ê¸‰ì¡°ì¹˜","ë¯¼ê°êµ°","ì²´ê°ì˜¨ë„","ì‚¬ì—…ì£¼ëŠ”"
]

def detect_template(text: str) -> str:
    g_hits = sum(text.count(k) for k in (KW_GUIDE_CORE + KW_GUIDE_STEP))
    a_hits = sum(text.count(k) for k in (KW_ACC_CORE + KW_ACC_STEP))
    g_hits += 3 * sum(text.count(k) for k in GUIDE_STRONG_HINTS)
    return "ê°€ì´ë“œí˜•" if g_hits >= a_hits else "ì‚¬ê³ ì‚¬ë¡€í˜•"

# ----------------------------
# ì§ˆë¬¸í˜• ë³€í™˜(ê°€ì´ë“œ/ì‚¬ê³  ê³µí†µ)
# ----------------------------
def to_question(s: str) -> str:
    s = rxx.sub(r"\s{2,}", " ", s).strip(" -â€¢â—â–ªâ–¶â–·").rstrip(" .")
    if has_action_verb(s): return f"ìš°ë¦¬ í˜„ì¥ì— '{s}' í•˜ê³  ìˆë‚˜ìš”?"
    return f"ì´ í•­ëª©ì— ëŒ€í•´ í˜„ì¥ ì ìš©ì´ ë˜ì—ˆë‚˜ìš”? â€” {s}"

# ----------------------------
# TBM ê¸°ë³¸ í…œí”Œë¦¿ ìƒì„± (í˜„í–‰)
# ----------------------------
def make_tbm_guide(text: str, use_ai: bool) -> Tuple[str, Dict[str, List[str]]]:
    sents = split_sentences_ko(text)
    if use_ai: core, core_f = pick_tr(sents, KW_GUIDE_CORE, 3)
    else:      core, core_f = pick_rule(sents, KW_GUIDE_CORE, 3)
    steps, steps_f = pick_rule(sents, KW_GUIDE_STEP, 5)
    qa_src, _ = pick_rule(sents, KW_GUIDE_QA + KW_GUIDE_STEP, 3)
    qa = [to_question(x) for x in qa_src]
    parts = {"í•µì‹¬": core, "ì ˆì°¨": steps, "ì§ˆë¬¸": qa}
    lines = []
    lines.append("ğŸ¦º TBM ëŒ€ë³¸ â€“ ê°€ì´ë“œí˜•"); lines.append("")
    lines.append("â— ì˜¤ëŠ˜ì˜ í•µì‹¬ í¬ì¸íŠ¸");   lines += render_with_marks(core, core_f); lines.append("")
    lines.append("â— ì‘ì—… ì „ ì ˆì°¨/ì ê²€");    lines += render_with_marks(steps, steps_f); lines.append("")
    lines.append("â— í˜„ì¥ í† ì˜ ì§ˆë¬¸");      [lines.append(f"- {q}") for q in qa]; lines.append("")
    lines.append("â— ë§ˆë¬´ë¦¬ ë©˜íŠ¸")
    lines.append("- â€œì˜¤ëŠ˜ ì‘ì—…ì˜ í•µì‹¬ì€ ìœ„ ì„¸ ê°€ì§€ì…ë‹ˆë‹¤. ë‹¤ ê°™ì´ í™•ì¸í•˜ê³  ì‹œì‘í•©ì‹œë‹¤.â€")
    lines.append("- â€œì ê¹ì´ë¼ë„ ì´ìƒí•˜ë©´ ë°”ë¡œ ì¤‘ì§€í•˜ê³ , ê´€ë¦¬ìì—ê²Œ ì•Œë¦½ë‹ˆë‹¤.â€")
    return "\n".join(lines), parts

def make_tbm_accident(text: str, use_ai: bool) -> Tuple[str, Dict[str, List[str]]]:
    sents = split_sentences_ko(text)
    if use_ai: core, core_f = pick_tr(sents, KW_ACC_CORE, 3)
    else:      core, core_f = pick_rule(sents, KW_ACC_CORE, 3)
    steps, steps_f = pick_rule(sents, KW_ACC_STEP, 5)
    qa_src, _ = pick_rule(sents, KW_ACC_STEP, 3)
    qa = [to_question(x) for x in qa_src]
    parts = {"í•µì‹¬": core, "ì‚¬ê³ /ì¡°ì¹˜": steps, "ì§ˆë¬¸": qa}
    lines = []
    lines.append("ğŸ¦º TBM ëŒ€ë³¸ â€“ ì‚¬ê³ ì‚¬ë¡€í˜•"); lines.append("")
    lines.append("â— ì‚¬ê³ /ìœ„í—˜ ìš”ì¸ ìš”ì•½");   lines += render_with_marks(core, core_f); lines.append("")
    lines.append("â— ë°œìƒ ê²½ìœ„/ì¡°ì¹˜/ê°œì„ ");   lines += render_with_marks(steps, steps_f); lines.append("")
    lines.append("â— ì¬ë°œ ë°©ì§€ í† ì˜ ì§ˆë¬¸"); [lines.append(f"- {q}") for q in qa]; lines.append("")
    lines.append("â— ë§ˆë¬´ë¦¬ ë©˜íŠ¸")
    lines.append("- â€œì´ ì‚¬ë¡€ì—ì„œ ë°°ìš´ ì˜ˆë°© í¬ì¸íŠ¸ë¥¼ ì˜¤ëŠ˜ ì‘ì—…ì— ë°”ë¡œ ì ìš©í•©ì‹œë‹¤.â€")
    lines.append("- â€œê°ì ë§¡ì€ ê³µì •ì—ì„œ ë™ì¼ ìœ„í—˜ì´ ì—†ëŠ”ì§€ ë‹¤ì‹œ ì ê²€í•´ ì£¼ì„¸ìš”.â€")
    return "\n".join(lines), parts

# ----------------------------
# âœ… ë¬´ë£Œ â€œìì—°ìŠ¤ëŸ¬ìš´ êµìœ¡ëŒ€ë³¸â€ ìƒì„±ê¸° (ê·œì¹™ ê¸°ë°˜ ë¬¸ì²´ ë³€í™˜/êµ¬ì„±)
# ----------------------------
INTRO_TONES = [
    "ì˜¤ëŠ˜ì€ {topic}ì— ëŒ€í•´ ì´ì•¼ê¸°í•´ë³´ê² ìŠµë‹ˆë‹¤.",
    "í˜„ì¥ì—ì„œ ìì£¼ ë†“ì¹˜ê¸° ì‰¬ìš´ {topic}ì„(ë¥¼) ì‰½ê²Œ ì •ë¦¬í•´ ë“œë¦´ê²Œìš”.",
    "{topic} â€” ì–´ë µì§€ ì•Šê²Œ í•µì‹¬ë§Œ ì§šì–´ë³´ê² ìŠµë‹ˆë‹¤."
]
CONNECTORS = ["ë¨¼ì €", "ê·¸ë¦¬ê³ ", "ë˜í•œ", "ë¬´ì—‡ë³´ë‹¤", "ë§ˆì§€ë§‰ìœ¼ë¡œ", "ë§ë¶™ì´ë©´"]
CLOSERS = [
    "í˜„ì¥ì€ ì‘ì€ ìŠµê´€ì—ì„œ ì•ˆì „ì´ ì‹œì‘ë©ë‹ˆë‹¤.",
    "ì§€ê¸ˆ ë°”ë¡œ ìš°ë¦¬ ì‘ì—…ì— ì ìš©í•´ ë´…ì‹œë‹¤.",
    "ì„œë‘ë¥´ì§€ ë§ê³ , í•œ ë²ˆ ë” í™•ì¸í•©ì‹œë‹¤."
]
SLOGAN = "í•œ ë²ˆ ë” í™•ì¸! í•œ ë²ˆ ë” ì ê²€!"

def guess_topic(text: str) -> str:
    first = text.strip().split("\n", 1)[0][:30]
    if "ì˜¨ì—´" in text or "í­ì—¼" in text: return "ì˜¨ì—´ì§ˆí™˜ ì˜ˆë°©"
    if "ì¶”ë½" in text or "ì§€ë¶•" in text or "ì¬ë¼ì´íŠ¸" in text: return "ì§€ë¶• ì‘ì—… ì¶”ë½ì‚¬ê³  ì˜ˆë°©"
    if "ê°ì •ë…¸ë™" in text: return "ê°ì •ë…¸ë™ì ê±´ê°•ë³´í˜¸"
    if "ì§ˆì‹" in text: return "ì§ˆì‹ ì¬í•´ ì˜ˆë°©"
    if "ê°ì „" in text: return "ê°ì „ ì‚¬ê³  ì˜ˆë°©"
    return first if first else "ì•ˆì „ë³´ê±´ êµìœ¡"

def soften_style(s: str) -> str:
    s = rxx.sub(r"~?í•˜ì—¬ì•¼\s*í•©ë‹ˆë‹¤", "í•´ì•¼ í•©ë‹ˆë‹¤", s)
    s = s.replace("ì‹¤ì‹œí•œë‹¤", "ì‹¤ì‹œí•©ë‹ˆë‹¤").replace("ì‹¤ì‹œí•˜ì—¬ì•¼", "ì‹¤ì‹œí•´ì•¼ í•©ë‹ˆë‹¤")
    s = s.replace("í™•ì¸ ë°”ëŒ", "í™•ì¸í•´ì£¼ì„¸ìš”").replace("í™•ì¸ ë°”ëë‹ˆë‹¤", "í™•ì¸í•´ì£¼ì„¸ìš”")
    s = s.replace("ì¡°ì¹˜í•œë‹¤", "ì¡°ì¹˜í•©ë‹ˆë‹¤").replace("ì°©ìš©í•œë‹¤", "ì°©ìš©í•©ë‹ˆë‹¤")
    s = s.replace("í•„ìš”í•˜ë‹¤", "í•„ìš”í•©ë‹ˆë‹¤").replace("ê¸ˆì§€í•œë‹¤", "ê¸ˆì§€í•©ë‹ˆë‹¤")
    return s

def join_sentences_naturally(lines: List[str]) -> str:
    out = []
    for i, s in enumerate(lines):
        s = s.strip(" -â€¢â—\t")
        s = soften_style(s)
        prefix = (CONNECTORS[min(i, len(CONNECTORS)-1)] + " ") if i < 5 else ""
        out.append(prefix + s)
    return " ".join(out)

def make_natural_script(text: str, detected: str, max_points: int = 6) -> str:
    sents = split_sentences_ko(text)
    if detected == "ê°€ì´ë“œí˜•":
        core, _ = pick_tr(sents, KW_GUIDE_CORE + KW_GUIDE_STEP, max_points)
    else:
        core, _ = pick_tr(sents, KW_ACC_CORE + KW_ACC_STEP, max_points)
    if not core: core = sents[:max_points]

    topic = guess_topic(text)
    intro = f"ğŸ¦º êµìœ¡ëŒ€ë³¸ â€“ {topic}\n\n"
    intro += f"{np.random.choice(INTRO_TONES).format(topic=topic)}\n\n"

    body = join_sentences_naturally(core)

    actions = [c for c in core if has_action_verb(c)]
    if len(actions) < 3:
        extra = [s for s in sents if has_action_verb(s) and s not in actions]
        actions += extra[: 3 - len(actions)]
    actions = actions[:3]
    apply_lines = "\n".join([f"- {soften_style(a)}" for a in actions]) if actions else "- ì˜¤ëŠ˜ ì‘ì—… ê³„íšê³¼ ìœ„í—˜ìš”ì¸ì„ í•¨ê»˜ í™•ì¸í•©ë‹ˆë‹¤."

    closer = np.random.choice(CLOSERS)

    out = []
    out.append(intro)
    out.append("ï¼»ë„ì…ï¼½")
    out.append(f"{np.random.choice(CONNECTORS)} {topic}ì˜ í•µì‹¬ë§Œ ì§šì–´ë³¼ê²Œìš”.\n")
    out.append("ï¼»í•µì‹¬ ì„¤ëª…ï¼½")
    out.append(body + "\n")
    out.append("ï¼»í˜„ì¥ ì ìš©/ì ê²€ï¼½")
    out.append(apply_lines + "\n")
    out.append("ï¼»ë§ˆë¬´ë¦¬ ë‹¹ë¶€ï¼½")
    out.append(closer + "\n")
    out.append("ï¼»êµ¬í˜¸ï¼½")
    out.append(f"â€œ{SLOGAN}â€")
    return "\n".join(out)

# ----------------------------
# âœ… DOCXë¡œ ë‚´ë³´ë‚´ê¸° â€” XML ê¸ˆì§€ë¬¸ì ì œê±°(ì˜¤ë¥˜ ë°©ì§€)
# ----------------------------
_XML_FORBIDDEN = r"[\x00-\x08\x0B\x0C\x0E-\x1F\uD800-\uDFFF\uFFFE\uFFFF]"
def _xml_safe(s: str) -> str:
    if not isinstance(s, str):
        s = str(s) if s is not None else ""
    # python-docx/lxmlì´ í—ˆìš©í•˜ì§€ ì•ŠëŠ” XML 1.0 ê¸ˆì§€ë¬¸ì ì œê±°
    return rxx.sub(_XML_FORBIDDEN, "", s)

def to_docx_bytes(script: str) -> bytes:
    doc = Document()
    # ë³¸ë¬¸ ê¸€ê¼´ ì„¤ì • (ìœˆë„/ë§¥/ë¦¬ëˆ…ìŠ¤ ëª¨ë‘ ë¬´ë‚œí•œ ê¸°ë³¸ì²´ê°€ ì ìš©ë  ìˆ˜ ìˆìŒ)
    try:
        style = doc.styles["Normal"]
        style.font.name = "Malgun Gothic"
        style.font.size = Pt(11)
    except Exception:
        pass

    # ê° ì¤„ì„ ì•ˆì „í•˜ê²Œ ì¶”ê°€
    for raw in script.split("\n"):
        line = _xml_safe(raw)
        p = doc.add_paragraph(line)
        for run in p.runs:
            try:
                run.font.name = "Malgun Gothic"
                run.font.size = Pt(11)
            except Exception:
                pass

    bio = io.BytesIO()
    doc.save(bio)
    bio.seek(0)
    return bio.read()

# ----------------------------
# UI (ì¢Œ/ìš° 2ì—´ + ì‚¬ì´ë“œë°” + ì´ˆê¸°í™”)
# ----------------------------
st.set_page_config(page_title="OPS2TBM", page_icon="ğŸ¦º", layout="wide")

with st.sidebar:
    st.header("â„¹ï¸ ì†Œê°œ / ì‚¬ìš©ë²•")
    st.markdown("""
**AI ê¸°ëŠ¥(ì™„ì „ ë¬´ë£Œ)**  
- ê²½ëŸ‰ TextRank(NumPy) + **MMR ë‹¤ì–‘ì„±**ìœ¼ë¡œ í•µì‹¬ ë¬¸ì¥ ì¶”ì¶œ  
- **ìì—°ìŠ¤ëŸ¬ìš´ êµìœ¡ëŒ€ë³¸(ë¬´ë£Œ ëª¨ë“œ)**: ê·œì¹™ ê¸°ë°˜ ë¬¸ì²´ ë³€í™˜/êµ¬ì„± â†’ ë‹´ë‹¹ìê°€ ì½ê¸° ì¢‹ì€ ë§í•˜ê¸° í†¤ìœ¼ë¡œ ìë™ í¸ì§‘  
- **ì§§ì€ í—¤ë”/ìŠ¬ë¡œê±´ ì œê±°**, **í–‰ë™ ë™ì‚¬ ë¬¸ì¥ ìš°ì„ **

**ì´ˆê¸°í™”**  
- ìš°ìƒë‹¨ **ğŸ§¹ ì´ˆê¸°í™”** ë²„íŠ¼ â†’ ì—…ë¡œë“œ/ì„ íƒ/í…ìŠ¤íŠ¸ ë¦¬ì…‹
""")

st.title("ğŸ¦º OPS2TBM â€” OPS/í¬ìŠ¤í„°ë¥¼ êµìœ¡ ëŒ€ë³¸ìœ¼ë¡œ ìë™ ë³€í™˜ (ì™„ì „ ë¬´ë£Œ)")

def reset_all():
    st.session_state.pop("manual_text", None)
    st.session_state.pop("edited_text", None)
    st.session_state.pop("zip_choice", None)
    st.session_state.uploader_key += 1
    st.rerun()

col_top1, col_top2 = st.columns([4,1])
with col_top2:
    st.button("ğŸ§¹ ì´ˆê¸°í™”", on_click=reset_all, use_container_width=True)

st.markdown("""
**ì•ˆë‚´**  
- í…ìŠ¤íŠ¸ê°€ í¬í•¨ëœ PDF ë˜ëŠ” ë³¸ë¬¸ í…ìŠ¤íŠ¸ë¥¼ ê¶Œì¥í•©ë‹ˆë‹¤.  
- ZIP ì—…ë¡œë“œ ì‹œ ë‚´ë¶€ì˜ PDFë“¤ì„ ìë™ ì¸ì‹í•˜ì—¬ ì„ íƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.  
- ì´ë¯¸ì§€/ìŠ¤ìº”í˜• PDFëŠ” í˜„ì¬ OCR ë¯¸ì§€ì›ì…ë‹ˆë‹¤.
""")

col1, col2 = st.columns([1, 1], gap="large")

# ì¢Œì¸¡: ì…ë ¥
with col1:
    uploaded = st.file_uploader("OPS ì—…ë¡œë“œ (PDF ë˜ëŠ” ZIP) â€¢ í…ìŠ¤íŠ¸ PDF ê¶Œì¥",
                                type=["pdf", "zip"], key=f"uploader_{st.session_state.uploader_key}")
    manual_text = st.text_area("ë˜ëŠ” OPS í…ìŠ¤íŠ¸ ì§ì ‘ ë¶™ì—¬ë„£ê¸°", key="manual_text",
                               height=220, placeholder="ì˜ˆ: í­ì—¼ ì‹œ ì˜¨ì—´ì§ˆí™˜ ì˜ˆë°©ì„ ìœ„í•´â€¦")

    # ZIP ì²˜ë¦¬
    zip_pdfs: Dict[str, bytes] = {}
    selected_zip_pdf = None
    if uploaded and uploaded.name.lower().endswith(".zip"):
        try:
            with zipfile.ZipFile(uploaded, "r") as zf:
                for name in zf.namelist():
                    if name.lower().endswith(".pdf"):
                        zip_pdfs[name] = zf.read(name)
        except Exception:
            st.error("ZIPì„ í•´ì œí•˜ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. íŒŒì¼ì„ í™•ì¸í•´ ì£¼ì„¸ìš”.")
        if zip_pdfs:
            selected_zip_pdf = st.selectbox("ZIP ë‚´ PDF ì„ íƒ", list(zip_pdfs.keys()), key="zip_choice")
        else:
            st.warning("ZIP ì•ˆì—ì„œ PDFë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")

    extracted = ""
    if uploaded and uploaded.name.lower().endswith(".pdf"):
        with st.spinner("PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ ì¤‘..."):
            data = uploaded.read()
            extracted = read_pdf_text(data)
    elif selected_zip_pdf:
        with st.spinner("ZIP ë‚´ë¶€ PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ ì¤‘..."):
            extracted = read_pdf_text(zip_pdfs[selected_zip_pdf])

    base_text = st.session_state.get("manual_text", "").strip() or extracted.strip()
    st.markdown("**ì¶”ì¶œ/ì…ë ¥ í…ìŠ¤íŠ¸ ë¯¸ë¦¬ë³´ê¸°**")
    edited_text = st.text_area("í…ìŠ¤íŠ¸", value=base_text, height=240, key="edited_text")

# ìš°ì¸¡: ì˜µì…˜/ìƒì„±/ë‹¤ìš´ë¡œë“œ
with col2:
    use_ai = st.toggle("ğŸ”¹ AI ìš”ì•½(TextRank+MMR) ì‚¬ìš©", value=True)
    tmpl_choice = st.selectbox("ğŸ§© í…œí”Œë¦¿", ["ìë™ ì„ íƒ", "ì‚¬ê³ ì‚¬ë¡€í˜•", "ê°€ì´ë“œí˜•"])
    gen_mode = st.selectbox("ğŸ§  ìƒì„± ëª¨ë“œ", ["TBM ê¸°ë³¸(í˜„í–‰)", "ìì—°ìŠ¤ëŸ¬ìš´ êµìœ¡ëŒ€ë³¸(ë¬´ë£Œ)"])
    max_points = st.slider("ìš”ì•½ ê°•ë„(í•µì‹¬ë¬¸ì¥ ê°œìˆ˜)", 3, 10, 6)

    if st.button("ğŸ› ï¸ ëŒ€ë³¸ ìƒì„±", type="primary", use_container_width=True):
        if not edited_text.strip():
            st.warning("í…ìŠ¤íŠ¸ê°€ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤. PDF/ZIP ì—…ë¡œë“œ ë˜ëŠ” í…ìŠ¤íŠ¸ ì…ë ¥ í›„ ì‹œë„í•˜ì„¸ìš”.")
        else:
            # í…œí”Œë¦¿ ê²°ì •
            if tmpl_choice == "ìë™ ì„ íƒ":
                detected = detect_template(edited_text)
            else:
                detected = tmpl_choice

            with st.spinner("ëŒ€ë³¸ ìƒì„± ì¤‘..."):
                if gen_mode == "ìì—°ìŠ¤ëŸ¬ìš´ êµìœ¡ëŒ€ë³¸(ë¬´ë£Œ)":
                    script = make_natural_script(edited_text, detected, max_points=max_points)
                    subtitle = f"{detected} Â· ìì—°ìŠ¤ëŸ¬ìš´ êµìœ¡ëŒ€ë³¸(ë¬´ë£Œ)"
                else:
                    if detected == "ì‚¬ê³ ì‚¬ë¡€í˜•":
                        script, _ = make_tbm_accident(edited_text, use_ai=use_ai)
                        subtitle = "ì‚¬ê³ ì‚¬ë¡€í˜• Â· TBM ê¸°ë³¸"
                    else:
                        script, _ = make_tbm_guide(edited_text, use_ai=use_ai)
                        subtitle = "ê°€ì´ë“œí˜• Â· TBM ê¸°ë³¸"

            st.success(f"ëŒ€ë³¸ ìƒì„± ì™„ë£Œ! ({subtitle})")
            st.text_area("ëŒ€ë³¸ ë¯¸ë¦¬ë³´ê¸°", value=script, height=420)

            c3, c4 = st.columns(2)
            with c3:
                st.download_button("â¬‡ï¸ TXT ë‹¤ìš´ë¡œë“œ", data=_xml_safe(script).encode("utf-8"),
                                   file_name="tbm_script.txt", use_container_width=True)
            with c4:
                st.download_button("â¬‡ï¸ DOCX ë‹¤ìš´ë¡œë“œ", data=to_docx_bytes(script),
                                   file_name="tbm_script.docx", use_container_width=True)

st.caption("í˜„ì¬: ì™„ì „ ë¬´ë£Œ. ê·œì¹™ + NumPy TextRank(ê²½ëŸ‰ AI) + MMR. í…œí”Œë¦¿ ìë™/ìˆ˜ë™. ZIP ì§€ì›. OCR ë¯¸ì§€ì›(í…ìŠ¤íŠ¸ PDF ê¶Œì¥). â€˜ìì—°ìŠ¤ëŸ¬ìš´ êµìœ¡ëŒ€ë³¸(ë¬´ë£Œ)â€™ ëª¨ë“œë¡œ ë§í•˜ê¸° í†¤ ìë™ ë³€í™˜. (DOCX ë‚´ë³´ë‚´ê¸° íŠ¹ìˆ˜ë¬¸ì ì˜¤ë¥˜ í•´ê²°)")
