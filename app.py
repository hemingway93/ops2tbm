# app.py
import io
from typing import List, Tuple, Dict
import streamlit as st
from docx import Document
from docx.shared import Pt
import pypdfium2 as pdfium
from pdfminer.high_level import extract_text as pdf_extract_text
import numpy as np
import regex as rxx

# ----------------------------
# ì „ì²˜ë¦¬ ìœ í‹¸
# ----------------------------
def read_pdf_text(file_bytes: bytes) -> str:
    """í…ìŠ¤íŠ¸ê°€ í¬í•¨ëœ ì¼ë°˜ PDFì˜ í…ìŠ¤íŠ¸ ì¶”ì¶œ(pdfminer.six ìš°ì„ ).
    ì´ë¯¸ì§€ ê¸°ë°˜ í˜ì´ì§€ë¼ë„ pypdfium2ë¡œ 1ì°¨ í™•ì¸ í›„, í…ìŠ¤íŠ¸ê°€ ì—†ìœ¼ë©´ ë¹ˆ ë¬¸ìì—´ ìœ ì§€."""
    # pdfminerë¡œ ì‹œë„ (ê°€ì¥ ì•ˆì •ì )
    with io.BytesIO(file_bytes) as bio:
        text = pdf_extract_text(bio) or ""

    # í…ìŠ¤íŠ¸ê°€ ë„ˆë¬´ ì§§ìœ¼ë©´(ì´ë¯¸ì§€ ê¸°ë°˜ ì¶”ì •) í˜ì´ì§€ ìˆ˜ ì •ë„ëŠ” í™•ì¸
    if len(text.strip()) < 10:
        try:
            with io.BytesIO(file_bytes) as bio:
                pdf = pdfium.PdfDocument(bio)
                pages = len(pdf)
            # ì´ë¯¸ì§€ ê¸°ë°˜ì¼ ê°€ëŠ¥ì„± ì•ˆë‚´
            if pages > 0 and not text.strip():
                st.warning("ì´ PDFëŠ” ì´ë¯¸ì§€ ê¸°ë°˜ìœ¼ë¡œ ë³´ì—¬ìš”. í˜„ì¬ ë²„ì „ì€ OCR ì—†ì´ í…ìŠ¤íŠ¸ë§Œ ì²˜ë¦¬í•©ë‹ˆë‹¤.")
        except Exception:
            pass
    return normalize_text(text)

def normalize_text(text: str) -> str:
    # í˜ì´ì§€ ë„˜ë²„/ë¨¸ë¦¬ê¸€ ê¼¬ë¦¬ê¸€ ì œê±°ì— ê°€ê¹Œìš´ ê°„ë‹¨ ì •ë¦¬
    text = text.replace("\x0c", "\n")
    text = rxx.sub(r"[ \t]+\n", "\n", text)
    text = rxx.sub(r"\n{3,}", "\n\n", text)
    text = text.strip()
    return text

def split_sentences_ko(text: str) -> List[str]:
    # ë§ˆì¹¨í‘œ/ë¬¼ìŒí‘œ/ëŠë‚Œí‘œ/ì¢…ê²°ì–´ë¯¸ ë“±ì„ ê¸°ì¤€ìœ¼ë¡œ ëŸ¬í”„í•˜ê²Œ ë¶„ë¦¬
    raw = rxx.split(r"(?<=[\.!\?]|ë‹¤\.)\s+|\n+", text)
    sents = [s.strip() for s in raw if s and len(s.strip()) > 1]
    return sents

# ----------------------------
# ìˆœìˆ˜ NumPy TextRank
# ----------------------------
def sentence_tfidf_vectors(sents: List[str]) -> Tuple[np.ndarray, List[str]]:
    # ì•„ì£¼ ë‹¨ìˆœí•œ BoW ê¸°ë°˜ TF-IDF í‰ë‚´ (ë¹ˆë„/ë¬¸ì¥ìˆ˜ ë³´ì •)
    tokenized = [simple_tokens(s) for s in sents]
    vocab = {}
    for toks in tokenized:
        for t in toks:
            if t not in vocab:
                vocab[t] = len(vocab)
    if not vocab:
        return np.zeros((len(sents), 0)), []
    mat = np.zeros((len(sents), len(vocab)), dtype=np.float32)
    df = np.zeros((len(vocab),), dtype=np.float32)
    for i, toks in enumerate(tokenized):
        for t in toks:
            j = vocab[t]
            mat[i, j] += 1.0
        unique = set(toks)
        for t in unique:
            df[vocab[t]] += 1.0
    # idf
    N = float(len(sents))
    idf = np.log((N + 1.0) / (df + 1.0)) + 1.0
    mat = mat * idf
    # L2 ì •ê·œí™”
    norms = np.linalg.norm(mat, axis=1, keepdims=True) + 1e-8
    mat = mat / norms
    return mat, list(vocab.keys())

def simple_tokens(s: str) -> List[str]:
    # í•œê¸€/ìˆ«ì/ì˜ë¬¸ë§Œ ë‚¨ê¸°ê³  2ê¸€ì ì´ìƒë§Œ í† í°ìœ¼ë¡œ ì‚¬ìš©
    s = s.lower()
    toks = rxx.findall(r"[ê°€-í£a-z0-9]{2,}", s)
    return toks

def cosine_sim_matrix(X: np.ndarray) -> np.ndarray:
    if X.size == 0:
        return np.zeros((len(X), len(X)), dtype=np.float32)
    # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ = X @ X^T (ì´ë¯¸ L2 normalize ë˜ì–´ ìˆìŒ)
    sim = np.clip(X @ X.T, 0.0, 1.0)
    np.fill_diagonal(sim, 0.0)  # ìê¸° ìì‹  ì œì™¸
    return sim

def textrank_scores(sents: List[str], d: float = 0.85, max_iter: int = 50, tol: float = 1e-4) -> List[float]:
    """SciPy/NetworkX ì—†ì´ ìˆœìˆ˜ NumPyë¡œ TextRank ì ìˆ˜ ê³„ì‚°."""
    if len(sents) == 0:
        return []
    if len(sents) == 1:
        return [1.0]

    X, _ = sentence_tfidf_vectors(sents)
    W = cosine_sim_matrix(X)
    # ê° í–‰ì„ í™•ë¥  ë¶„í¬ê°€ ë˜ë„ë¡ ì •ê·œí™”
    row_sums = W.sum(axis=1, keepdims=True)
    P = np.divide(W, row_sums, out=np.zeros_like(W), where=row_sums > 0)

    n = W.shape[0]
    r = np.ones((n, 1), dtype=np.float32) / n
    teleport = np.ones((n, 1), dtype=np.float32) / n

    for _ in range(max_iter):
        r_new = d * (P.T @ r) + (1 - d) * teleport
        if np.linalg.norm(r_new - r, ord=1) < tol:
            r = r_new
            break
        r = r_new
    return [float(v) for v in r.flatten()]

def pick_sentences_tr(sents: List[str], kw: List[str], limit: int, scores: List[float]) -> List[str]:
    """í‚¤ì›Œë“œ ê°€ì¤‘ì¹˜ + TextRank ì ìˆ˜ë¡œ ìƒìœ„ ë¬¸ì¥ ì„ íƒ."""
    if not sents:
        return []
    # í‚¤ì›Œë“œ ê°€ì¤‘
    w = np.array(scores, dtype=np.float32)
    if kw:
        for i, s in enumerate(sents):
            if any(k in s for k in kw):
                w[i] += 0.2
    idx = np.argsort(-w)[:limit]
    return [sents[i] for i in idx]

def pick_sentences_rule(sents: List[str], kw: List[str], limit: int) -> List[str]:
    hits = [s for s in sents if any(k in s for k in kw)]
    if len(hits) >= limit:
        return hits[:limit]
    # ë¶€ì¡±í•˜ë©´ ê¸¸ì´/ìœ„ì¹˜ë¡œ ë³´ì¶©
    remain = [s for s in sents if s not in hits]
    remain = sorted(remain, key=lambda x: (-len(x), sents.index(x)))
    return hits + remain[: max(0, limit - len(hits))]

# ----------------------------
# í…œí”Œë¦¿ í‚¤ì›Œë“œ
# ----------------------------
KW_GUIDE_CORE = ["ëª©í‘œ", "ì¤‘ìš”", "ì¤‘ì ", "í•„ìˆ˜", "ì£¼ì˜", "ì¤€ìˆ˜"]
KW_GUIDE_STEP = ["ì ˆì°¨", "ìˆœì„œ", "ë°©ë²•", "ì ê²€", "í™•ì¸"]
KW_GUIDE_QA   = ["ì§ˆë¬¸", "ì™œ", "ì–´ë–»ê²Œ", "ë¬´ì—‡"]

KW_ACC_CORE = ["ì‚¬ê³ ", "ì¬í•´", "ìœ„í—˜", "ì›ì¸", "ì˜ˆë°©", "ëŒ€ì±…"]
KW_ACC_STEP = ["ë°œìƒ", "ê²½ìœ„", "ì¡°ì¹˜", "ê°œì„ ", "êµìœ¡"]
KW_ACC_QA   = ["ì›ì¸ì€", "ë‹¤ìŒì—ëŠ”", "ì˜ˆë°©í•˜ë ¤ë©´", "í™•ì¸í•  ì "]

# ----------------------------
# ëŒ€ë³¸ ìƒì„± ë¡œì§
# ----------------------------
def make_tbm_script_guide(text: str, use_ai: bool) -> Tuple[str, Dict[str, List[str]]]:
    sents = split_sentences_ko(text)
    # í•µì‹¬ ë©”ì‹œì§€
    if use_ai:
        scores = textrank_scores(sents)
        core = pick_sentences_tr(sents, KW_GUIDE_CORE, 3, scores)
    else:
        core = pick_sentences_rule(sents, KW_GUIDE_CORE, 3)
    # ì ˆì°¨/ì ê²€
    steps = pick_sentences_rule(sents, KW_GUIDE_STEP, 5)
    # ì§ˆë¬¸
    qa = pick_sentences_rule(sents, KW_GUIDE_QA, 3)

    parts = {"í•µì‹¬": core, "ì ˆì°¨": steps, "ì§ˆë¬¸": qa}
    script = render_script_guide(parts)
    return script, parts

def make_tbm_script_accident(text: str, use_ai: bool) -> Tuple[str, Dict[str, List[str]]]:
    sents = split_sentences_ko(text)
    if use_ai:
        scores = textrank_scores(sents)
        core = pick_sentences_tr(sents, KW_ACC_CORE, 3, scores)
    else:
        core = pick_sentences_rule(sents, KW_ACC_CORE, 3)
    steps = pick_sentences_rule(sents, KW_ACC_STEP, 5)
    qa = pick_sentences_rule(sents, KW_ACC_QA, 3)

    parts = {"í•µì‹¬": core, "ì‚¬ê³ /ì¡°ì¹˜": steps, "ì§ˆë¬¸": qa}
    script = render_script_accident(parts)
    return script, parts

def render_script_guide(parts: Dict[str, List[str]]) -> str:
    lines = ["[TBM ëŒ€ë³¸] ê°€ì´ë“œí˜•", ""]
    lines.append("1) ì˜¤ëŠ˜ì˜ í•µì‹¬ í¬ì¸íŠ¸")
    for s in parts["í•µì‹¬"]:
        lines.append(f" - {s}")
    lines.append("")
    lines.append("2) ì‘ì—… ì „ ì ˆì°¨/ì ê²€")
    for s in parts["ì ˆì°¨"]:
        lines.append(f" - {s}")
    lines.append("")
    lines.append("3) í˜„ì¥ í† ì˜ ì§ˆë¬¸")
    for s in parts["ì§ˆë¬¸"]:
        lines.append(f" - {s}")
    lines.append("")
    lines.append("ë³´ë„ˆìŠ¤(ì‹œì—° ë©˜íŠ¸):")
    lines.append(" - â€œì˜¤ëŠ˜ ì‘ì—…ì˜ í•µì‹¬ì€ ìœ„ ì„¸ ê°€ì§€ì…ë‹ˆë‹¤. ë‹¤ ê°™ì´ í™•ì¸í•˜ê³  ì‹œì‘í•©ì‹œë‹¤.â€")
    lines.append(" - â€œì ê¹ì´ë¼ë„ ì´ìƒí•˜ë©´ ë°”ë¡œ ì¤‘ì§€í•˜ê³ , ê´€ë¦¬ìì—ê²Œ ì•Œë¦½ë‹ˆë‹¤.â€")
    return "\n".join(lines)

def render_script_accident(parts: Dict[str, List[str]]) -> str:
    lines = ["[TBM ëŒ€ë³¸] ì‚¬ê³ ì‚¬ë¡€í˜•", ""]
    lines.append("1) ì‚¬ê³ /ìœ„í—˜ ìš”ì¸ ìš”ì•½")
    for s in parts["í•µì‹¬"]:
        lines.append(f" - {s}")
    lines.append("")
    lines.append("2) ë°œìƒ ê²½ìœ„/ì¡°ì¹˜/ê°œì„ ")
    for s in parts["ì‚¬ê³ /ì¡°ì¹˜"]:
        lines.append(f" - {s}")
    lines.append("")
    lines.append("3) ì¬ë°œ ë°©ì§€ í† ì˜ ì§ˆë¬¸")
    for s in parts["ì§ˆë¬¸"]:
        lines.append(f" - {s}")
    lines.append("")
    lines.append("ë³´ë„ˆìŠ¤(ì‹œì—° ë©˜íŠ¸):")
    lines.append(" - â€œì´ ì‚¬ë¡€ì—ì„œ ë°°ìš´ ì˜ˆë°© í¬ì¸íŠ¸ë¥¼ ì˜¤ëŠ˜ ì‘ì—…ì— ë°”ë¡œ ì ìš©í•©ì‹œë‹¤.â€")
    lines.append(" - â€œê°ì ë§¡ì€ ê³µì •ì—ì„œ ë™ì¼ ìœ„í—˜ì´ ì—†ëŠ”ì§€ ë‹¤ì‹œ ì ê²€í•´ ì£¼ì„¸ìš”.â€")
    return "\n".join(lines)

# ----------------------------
# DOCX ë‚´ë³´ë‚´ê¸°
# ----------------------------
def export_docx(script: str, filename: str = "tbm_script.docx") -> bytes:
    doc = Document()
    style = doc.styles["Normal"]
    style.font.name = "Malgun Gothic"
    style.font.size = Pt(11)

    for para in script.split("\n"):
        p = doc.add_paragraph(para)
        for run in p.runs:
            run.font.name = "Malgun Gothic"
            run.font.size = Pt(11)

    bio = io.BytesIO()
    doc.save(bio)
    bio.seek(0)
    return bio.read()

# ----------------------------
# UI
# ----------------------------
st.set_page_config(page_title="OPSâ†’TBM ëŒ€ë³¸ ìƒì„±ê¸°", page_icon="ğŸ› ï¸", layout="wide")

st.title("OPS ë¬¸ì„œì—ì„œ TBM ëŒ€ë³¸ ë½‘ê¸° ğŸ› ï¸")
st.caption("í…ìŠ¤íŠ¸ PDF ê¸°ë°˜. ì´ë¯¸ì§€ ìŠ¤ìº”ë³¸ì€ í˜„ì¬ ë²„ì „ì—ì„œ OCR ì—†ì´ ì•ˆë‚´ë§Œ í•©ë‹ˆë‹¤.")

col1, col2 = st.columns([1, 1])
with col1:
    file = st.file_uploader("OPS PDF ì—…ë¡œë“œ", type=["pdf"])
    use_ai = st.toggle("AI ìš”ì•½/ê°€ì¤‘ì¹˜ ì‚¬ìš©(TextRank)", value=True)
    template = st.selectbox("í…œí”Œë¦¿ ì„ íƒ", ["ê°€ì´ë“œí˜•", "ì‚¬ê³ ì‚¬ë¡€í˜•"])
    run_btn = st.button("ëŒ€ë³¸ ìƒì„±", type="primary")

with col2:
    st.markdown("#### ì‚¬ìš© íŒ")
    st.markdown("- í…ìŠ¤íŠ¸ê°€ ë§ì€ í…œí”Œë¦¿í˜• OPS ë¬¸ì„œì—ì„œ ê°€ì¥ ì„±ëŠ¥ì´ ì¢‹ìŠµë‹ˆë‹¤.")
    st.markdown("- ì´ë¯¸ì§€/ìŠ¤ìº”ë³¸ì€ í…ìŠ¤íŠ¸ê°€ ì—†ì„ ìˆ˜ ìˆì–´ìš” â†’ ë³€í™˜ ì•ˆë‚´ë§Œ í‘œì‹œ.")

result_script = ""
parts = {}

if run_btn and file is not None:
    data = file.read()
    text = read_pdf_text(data)
    if not text.strip():
        st.error("PDFì—ì„œ ì½ì„ í…ìŠ¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤. (ì´ë¯¸ì§€ ê¸°ë°˜ì¼ ìˆ˜ ìˆì–´ìš”)")
    else:
        with st.spinner("ë¬¸ì„œì—ì„œ í•µì‹¬ ë‚´ìš© ì¶”ë¦¬ëŠ” ì¤‘..."):
            if template == "ì‚¬ê³ ì‚¬ë¡€í˜•":
                result_script, parts = make_tbm_script_accident(text, use_ai=use_ai)
                subtitle = "ì‚¬ê³ ì‚¬ë¡€í˜• í…œí”Œë¦¿ ì ìš©"
            else:
                result_script, parts = make_tbm_script_guide(text, use_ai=use_ai)
                subtitle = "ê°€ì´ë“œí˜• í…œí”Œë¦¿ ì ìš©"
        st.success(f"ëŒ€ë³¸ ìƒì„± ì™„ë£Œ! ({subtitle})")

        st.markdown("### ë¯¸ë¦¬ë³´ê¸°")
        st.code(result_script, language="markdown")

        docx_bytes = export_docx(result_script, "tbm_script.docx")
        st.download_button(
            "DOCXë¡œ ë‹¤ìš´ë¡œë“œ",
            data=docx_bytes,
            file_name="tbm_script.docx",
            mime="application/vnd.openxmlformats-officedocument.wordprocessingml.document",
        )

st.divider()
st.markdown("##### ë³´ë„ˆìŠ¤(ì‹œì—° ë©˜íŠ¸)ë§Œ ë”°ë¡œ ë³´ê¸°")
st.markdown("- â€œì˜¤ëŠ˜ ì‘ì—…ì˜ í•µì‹¬ì€ ìœ„ ì„¸ ê°€ì§€ì…ë‹ˆë‹¤. ë‹¤ ê°™ì´ í™•ì¸í•˜ê³  ì‹œì‘í•©ì‹œë‹¤.â€")
st.markdown("- â€œì ê¹ì´ë¼ë„ ì´ìƒí•˜ë©´ ë°”ë¡œ ì¤‘ì§€í•˜ê³ , ê´€ë¦¬ìì—ê²Œ ì•Œë¦½ë‹ˆë‹¤.â€")
st.markdown("- â€œì´ ì‚¬ë¡€ì—ì„œ ë°°ìš´ ì˜ˆë°© í¬ì¸íŠ¸ë¥¼ ì˜¤ëŠ˜ ì‘ì—…ì— ë°”ë¡œ ì ìš©í•©ì‹œë‹¤.â€")
st.markdown("- â€œê°ì ë§¡ì€ ê³µì •ì—ì„œ ë™ì¼ ìœ„í—˜ì´ ì—†ëŠ”ì§€ ë‹¤ì‹œ ì ê²€í•´ ì£¼ì„¸ìš”.â€")
