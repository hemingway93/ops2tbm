# ==========================================================
# OPS2TBM â€” OPS/í¬ìŠ¤í„° â†’ TBM êµìœ¡ ëŒ€ë³¸ ìë™ ìƒì„± (ì™„ì „ ë¬´ë£Œ)
#  * ì…ë ¥: PDF / ZIP(PDF ë¬¶ìŒ) / í…ìŠ¤íŠ¸ ë¶™ì—¬ë„£ê¸°
#  * íŒŒì´í”„ë¼ì¸: ì „ì²˜ë¦¬ â†’ TextRank+MMR ìš”ì•½(ì„¸ì…˜ KB ê°€ì¤‘) â†’ ê·œì¹™ê¸°ë°˜ ë¦¬ë¼ì´íŒ…
#  * ëª¨ë“œ:
#      - í•µì‹¬ìš”ì•½: ë³´ê³ ì„œí˜• ë‹¨ë½ ìš”ì•½(ë¶ˆë¦¿ ìµœì†Œ, ì¡ìŒ ì œê±°, ì—°ê²°ì–´ ì‚½ì…)
#      - ìì—°ìŠ¤ëŸ¬ìš´ êµìœ¡ëŒ€ë³¸(ë¬´ë£Œ): TBM ëŒ€ë³¸(ë„ì…/ì‚¬ë¡€/ìœ„í—˜/ìˆ˜ì¹™/ì§ˆë¬¸/ë§ˆë¬´ë¦¬)
#  * ì‹œë“œ KB: í•™ìŠµíŒŒì¼.zip(ê°œë³„ 24 PDF)ì—ì„œ ì¶”ì¶œí•œ ìœ„í—˜ìœ í˜•/í–‰ë™/ì§ˆë¬¸ ë‚´ì¥
#  * ë„ë©”ì¸ í…œí”Œë¦¿: ê¸°ë³¸ OFF(í† ê¸€ë¡œ ì„ íƒ). íŠ¸ë¦¬ê±°/ìœ ì‚¬ë„/ìš°ì„ ìˆœìœ„ 3ì¤‘ ì•ˆì „ì¥ì¹˜.
#  * OCR ë¯¸ì§€ì›(ì´ë¯¸ì§€/ìŠ¤ìº” PDFëŠ” ê²½ê³ )
#  * UI: ê¸°ì¡´ êµ¬ì¡° ìœ ì§€ (ì¢Œ ì…ë ¥/ë¯¸ë¦¬ë³´ê¸°, ìš° ì˜µì…˜/ìƒì„±/ë‹¤ìš´ë¡œë“œ)
# ==========================================================

import io, zipfile, re
from collections import Counter
from typing import List, Dict, Tuple
import streamlit as st
from docx import Document
from docx.shared import Pt
from pdfminer_high_level import extract_text as pdf_extract_text  # streamlit cloud í˜¸í™˜
import pypdfium2 as pdfium
import numpy as np
import regex as rxx

# ---------- í˜ì´ì§€ ì„¤ì • ----------
st.set_page_config(page_title="OPS2TBM", page_icon="ğŸ¦º", layout="wide")

# ==========================================================
# ì‹œë“œ KB (í•™ìŠµíŒŒì¼.zip 24ê°œ PDFì—ì„œ ìë™ ìˆ˜ì§‘í•œ ë‚´ìš©)
#  - ê³¼ë‹¤í•œ í•˜ë“œì½”ë”© ëŒ€ì‹ , ìµœì†ŒÂ·ëŒ€í‘œ íŒ¨í„´ë§Œ ë‚´ì¥í•˜ì—¬ ì´ˆê¸° í’ˆì§ˆì„ ì•ˆì •í™”
# ==========================================================
SEED_RISK_MAP = {
  "ì¤‘ë…": "ì¤‘ë…", "ë–¨ì–´ì§": "ë–¨ì–´ì§", "ë¼ì„": "ë¼ì„", "ì§ˆì‹": "ì§ˆì‹", "í™”ì¬": "í™”ì¬",
  "ê¹”ë¦¼": "ê¹”ë¦¼", "ë§ìŒ": "ë§ìŒ", "ê°ì „": "ê°ì „", "ì§€ë¶•": "ì§€ë¶•ì‘ì—…", "ì˜ˆì´ˆ": "ì˜ˆì´ˆ",
  "í­ë°œ": "í­ë°œ", "ì²œê³µê¸°": "ì²œê³µ", "ì„ ë°˜": "ì ˆì‚­", "ì»¨ë² ì´ì–´": "í˜‘ì°©", "ë¶€ë”ªí˜": "ì¶©ëŒ",
  "ë¯¸ì„¸ë¨¼ì§€": "ë¯¸ì„¸ë¨¼ì§€", "í¬ë ˆì¸": "ì–‘ì¤‘", "ë¬´ë„ˆì§": "ë¶•ê´´", "ë¹„ê³„": "ë¹„ê³„",
  "ì¶”ë½": "ì¶”ë½", "í­ì—¼": "í­ì—¼", "ë²Œëª©": "ë²Œëª©", "ë‚™í•˜": "ë‚™í•˜", "ë¶•ê´´": "ë¶•ê´´", "ê°±í¼": "ë¹„ê³„", "ë°œíŒ": "ë¹„ê³„"
}

SEED_ACTIONS = [
  "ë°€íê³µê°„ì‘ì—… êµìœ¡ ë° í›ˆë ¨ ì‹¤ì‹œ",
  "ì¶œì… ì „ ì¶©ë¶„í•œ í™˜ê¸° ì‹¤ì‹œ",
  "ì‘ì—… ì „ ê°€ìŠ¤ë†ë„ ì¸¡ì •",
  "ì‘ì—…ìì— í˜¸í¡ìš© ë³´í˜¸êµ¬ ì§€ê¸‰",
  "ì‘ì—… ìƒí™© ê°ì‹œì ë°°ì¹˜",
  "ì‘ì—… ì‹œ í˜¸í¡ìš© ë³´í˜¸êµ¬ ì°©ìš©",
  "ì¶œì…Â·í‡´ì¥ ì¸ì› ì ê²€",
  "ë³´í˜¸ì¥êµ¬ ì—†ì´ êµ¬ì¡° ê¸ˆì§€",
  "MSDS í™•ì¸ ë° ìœ í•´ì„± êµìœ¡ ì‹¤ì‹œ",
  "êµ­ì†Œë°°ê¸°ì¥ì¹˜ ì„¤ì¹˜Â·ê°€ë™",
  "í™˜ê¸°ê°€ ë¶ˆì¶©ë¶„í•œ ê³µê°„ì—ì„œëŠ” ê¸‰ê¸°/ë°°ê¸°íŒ¬ ì‚¬ìš©",
  "ìœ ê¸°í™”í•©ë¬¼ ì·¨ê¸‰ ì‹œ ë°©ë…ë§ˆìŠ¤í¬(ê°ˆìƒ‰ ì •í™”í†µ) ì°©ìš©",
  "ì†¡ê¸°ë§ˆìŠ¤í¬Â·ê³µê¸°í˜¸í¡ê¸° ì ì • ì‚¬ìš©",
  "ì˜ˆì´ˆê¸° ë‚  ì •ì§€ í›„ ì´ë¬¼ì§ˆ ì œê±°Â·ì ê²€",
  "ì˜ˆì´ˆÂ·ë²Œëª© ì‘ì—… ì‹œ ì‘ì—…ì ê°„ ì•ˆì „ê±°ë¦¬ ìœ ì§€",
  "ë²Œëª© ì‹œ ì“°ëŸ¬ì§€ëŠ” ë°©í–¥ ì‚¬ì „ ê²°ì • ë° ëŒ€í”¼ë¡œ í™•ë³´",
  "ì‘ì—…ë°œíŒ ê²¬ê³ íˆ ì„¤ì¹˜ ë° ìƒíƒœ ì ê²€",
  "ê°œêµ¬ë¶€Â·ê°œêµ¬ì°½ ë“± ì¶”ë½ ìœ„í—˜ êµ¬ê°„ì— ì•ˆì „ë‚œê°„ ì„¤ì¹˜",
  "ì•ˆì „ëŒ€ë¥¼ ì•ˆì „í•œ ì§€ì§€ì ì— ì—°ê²°í•˜ê³  ë¼ì´í”„ë¼ì¸ ì‚¬ìš©",
  "ìœ„í—˜êµ¬ì—­ ì„¤ì •Â·ì¶œì…í†µì œÂ·ê°ì‹œì ë°°ì¹˜",
  "ì–‘ì¤‘ ê³„íš ìˆ˜ë¦½ ë° ì‹ í˜¸ìˆ˜ ì§€ì •Â·í†µì‹  ìœ ì§€",
  "íšŒì „ì²´Â·ë¬¼ë¦¼ì  ë°©í˜¸ì¥ì¹˜ ì„¤ì¹˜ ë° ì ê²€",
  "ì‘ì—… ì „ ì‘ì—…ê³„íšì„œ ì‘ì„± ë° ì‘ì—…ì§€íœ˜ì ì§€ì •",
  "ê°œì¸ë³´í˜¸êµ¬(ì•ˆì „ëª¨Â·ë³´í˜¸ì•ˆê²½Â·ì•ˆì „í™” ë“±) ì˜¬ë°”ë¥´ê²Œ ì°©ìš©",
  "í™”ê¸°ì‘ì—… í—ˆê°€ ë° ì•ˆì „ì ê²€ ì² ì €",
  "ì •ë¹„Â·ì²­ì†ŒÂ·ì ê²€ ì‘ì—… ì‹œ ê¸°ê³„ ì „ì› ì°¨ë‹¨",
]

SEED_QUESTIONS = [
  "ì‘ì—… ì „ ì‘ì—…ê³„íšì„œì™€ ìœ„í—˜ì„±í‰ê°€ë¥¼ ê²€í† í–ˆìŠµë‹ˆê¹Œ?",
  "ê°œêµ¬ë¶€Â·ê°œêµ¬ì°½ ë“± ì¶”ë½ ìœ„í—˜ êµ¬ê°„ì— ì•ˆì „ë‚œê°„ì„ ì„¤ì¹˜í–ˆìŠµë‹ˆê¹Œ?",
  "ì‘ì—…ë°œíŒì´ ê²¬ê³ í•˜ê²Œ ì„¤ì¹˜ë˜ê³  ìƒíƒœê°€ ì–‘í˜¸í•©ë‹ˆê¹Œ?",
  "ì•ˆì „ëŒ€ ì—°ê²°ì ê³¼ ë¼ì´í”„ë¼ì¸ì´ í™•ë³´ë˜ì—ˆìŠµë‹ˆê¹Œ?",
  "êµ­ì†Œë°°ê¸°ì¥ì¹˜ë¥¼ ê°€ë™í•˜ê³  í™˜ê¸° ê²½ë¡œê°€ í™•ë³´ë˜ì—ˆìŠµë‹ˆê¹Œ?",
  "í˜¸í¡ë³´í˜¸êµ¬ê°€ ì‘ì—…ì— ì í•©í•˜ë©° ê´€ë¦¬ê°€ ë˜ê³  ìˆìŠµë‹ˆê¹Œ?",
  "ë°€íê³µê°„ ì¶œì…Â·í‡´ì¥ ì¸ì› ì ê²€ì´ ì´ë£¨ì–´ì§€ê³  ìˆìŠµë‹ˆê¹Œ?",
  "ì˜ˆì´ˆÂ·ë²Œëª© ì‘ì—… ì‹œ ì•ˆì „ê±°ë¦¬ì™€ ëŒ€í”¼ë¡œë¥¼ í™•ë³´í–ˆìŠµë‹ˆê¹Œ?",
  "ì–‘ì¤‘ ì‘ì—…ì— ì‹ í˜¸ìˆ˜ ì§€ì • ë° í†µì‹ ì²´ê³„ê°€ ë§ˆë ¨ë˜ì—ˆìŠµë‹ˆê¹Œ?",
  "íšŒì „ì²´Â·ë¬¼ë¦¼ì  ë°©í˜¸ì¥ì¹˜ê°€ ì •ìƒ ë™ì‘í•©ë‹ˆê¹Œ?"
]

# ---------- ì„¸ì…˜ ìƒíƒœ ----------
if "uploader_key" not in st.session_state:
    st.session_state.uploader_key = 0
if "kb_terms" not in st.session_state:
    st.session_state.kb_terms: Counter = Counter()
if "kb_actions" not in st.session_state:
    st.session_state.kb_actions: List[str] = []
if "kb_questions" not in st.session_state:
    st.session_state.kb_questions: List[str] = []
if "domain_toggle" not in st.session_state:
    st.session_state.domain_toggle = False  # ë„ë©”ì¸ í…œí”Œë¦¿ ê¸°ë³¸ Off
if "seed_loaded" not in st.session_state:
    st.session_state.seed_loaded = False

# ==========================================================
# ì „ì²˜ë¦¬ ìœ í‹¸ / íŒ¨í„´
# ==========================================================
NOISE_PATTERNS = [
    r"^ì œ?\s?\d{4}\s?[-.]?\s?\d+\s?í˜¸$",
    r"^(ë™ì ˆê¸°\s*ì£¼ìš”ì‚¬ê³ |ì•ˆì „ì‘ì—…ë°©ë²•|ì½˜í…ì¸ ë§í¬|ì±…ì\s*OPS|ìˆí¼\s*OPS)$",
    r"^(í¬ìŠ¤í„°|ì±…ì|ìŠ¤í‹°ì»¤|ì½˜í…ì¸  ë§í¬)$",
    r"^(ìŠ¤ë§ˆíŠ¸í°\s*APP|ì¤‘ëŒ€ì¬í•´\s*ì‚¬ì´ë Œ|ì‚°ì—…ì•ˆì „í¬í„¸|ê³ ìš©ë…¸ë™ë¶€)$",
    r"^https?://\S+$",
    r"^\(?\s*PowerPoint\s*í”„ë ˆì  í…Œì´ì…˜\s*\)?$",
    r"^ì•ˆì „ë³´ê±´ìë£Œì‹¤.*$",
    r"^ë°°í¬ì²˜\s+.*$",
    r"^í™ˆí˜ì´ì§€\s+.*$",
    r"^ì£¼ì†Œ\s+.*$",
    r"^VR\s+.*$",
    r"^ë¦¬í”Œë¦¿\s+.*$",
    r"^ë™ì˜ìƒ\s+.*$",
    r"^APP\s+.*$",
    r".*ê²€ìƒ‰í•´\s*ë³´ì„¸ìš”.*$",
]
BULLET_PREFIX = r"^[\s\-\â€¢\â—\â–ª\â–¶\â–·\Â·\*\u25CF\u25A0\u25B6\u25C6\u2022\u00B7\u279C\u27A4\u25BA\u25AA\u25AB\u2611\u2713\u2714\u2716\u2794\u27A2\uF0FC\uF0A7]+"
DATE_PAT = r"([â€™']?\d{2,4})\.\s?(\d{1,2})\.\s?(\d{1,2})\.?"
META_PATTERNS = [
    r"<\s*\d+\s*ëª…\s*ì‚¬ë§\s*>",
    r"<\s*\d+\s*ëª…\s*ì‚¬ìƒ\s*>",
    r"<\s*\d+\s*ëª…\s*ì˜ì‹ë¶ˆëª…\s*>",
    r"<\s*ì‚¬ë§\s*\d+\s*ëª…\s*>",
    r"<\s*ì‚¬ìƒ\s*\d+\s*ëª…\s*>",
]
STOP_TERMS = set("""
ë° ë“± ê´€ë ¨ ì‚¬í•­ ë‚´ìš© ì˜ˆë°© ì•ˆì „ ì‘ì—… í˜„ì¥ êµìœ¡ ë°©ë²• ê¸°ì¤€ ì¡°ì¹˜
ì‹¤ì‹œ í™•ì¸ í•„ìš” ê²½ìš° ëŒ€ìƒ ì‚¬ìš© ê´€ë¦¬ ì ê²€ ì ìš© ì •ë„ ì£¼ì˜ ì¤‘ ì „ í›„
ì£¼ìš” ì‚¬ë¡€ ì•ˆì „ì‘ì—…ë°©ë²• í¬ìŠ¤í„° ë™ì˜ìƒ ë¦¬í”Œë¦¿ ê°€ì´ë“œ ìë£Œì‹¤ ê²€ìƒ‰
í‚¤ë©”ì„¸ì§€ êµìœ¡í˜ì‹ ì‹¤ ì•ˆì „ë³´ê±´ê³µë‹¨ ê³µë‹¨ ìë£Œ êµ¬ë… ì•ˆë‚´ ì—°ë½ ì°¸ê³  ì¶œì²˜
ì†Œì¬ ì†Œì¬ì§€ ìœ„ì¹˜ ì¥ì†Œ ì§€ì—­ ì‹œêµ°êµ¬ ì„œìš¸ ì¸ì²œ ë¶€ì‚° ëŒ€êµ¬ ëŒ€ì „ ê´‘ì£¼ ìš¸ì‚° ì„¸ì¢… ê²½ê¸°ë„ ì¶©ì²­ ì „ë¼ ê²½ìƒ ê°•ì› ì œì£¼
ëª… ê±´ í˜¸ í˜¸ì°¨ í˜¸ìˆ˜ í˜ì´ì§€ ìª½ ë¶€ë¡ ì°¸ê³  ê·¸ë¦¼ í‘œ ëª©ì°¨
ì•ˆì „ë³´ê±´ ops í‚¤ ë©”ì„¸ì§€ í‚¤ë©”ì„¸ì§€ ìë£Œ opsêµì•ˆ êµì•ˆ
""".split())
LABEL_DROP_PAT = [
    r"^\d+$", r"^\d{2,4}[-_]\d{1,}$", r"^\d{4}$",
    r"^(ì œ)?\d+í˜¸$", r"^(í˜¸|í˜¸ìˆ˜|í˜¸ì°¨)$",
    r"^(ì‚¬ì—…ì¥|ì—…ì²´|ì†Œì¬|ì†Œì¬ì§€|ì¥ì†Œ|ì§€ì—­)$",
    r"^\d+\s*(ëª…|ê±´)$",
]

# ìœ„í—˜ìœ í˜•(ì´ˆê¸°ê°’) â€” ì‹œë“œ ë§µìœ¼ë¡œ ì´ˆê¸°í™” + ë™ì  ë³´ê°•
RISK_KEYWORDS = dict(SEED_RISK_MAP)

# ---------- ê³µí†µ ìœ í‹¸ ----------
def normalize_text(t: str) -> str:
    t = t.replace("\x0c", "\n")
    t = re.sub(r"[ \t]+\n", "\n", t)
    t = re.sub(r"\n{3,}", "\n\n", t)
    return t.strip()

def strip_noise_line(line: str) -> str:
    s = (line or "").strip()
    if not s: return ""
    s = re.sub(BULLET_PREFIX, "", s).strip()
    for pat in NOISE_PATTERNS:
        if re.match(pat, s, re.IGNORECASE):
            return ""
    s = re.sub(r"https?://\S+", "", s).strip()
    s = s.strip("â€¢â—â–ªâ–¶â–·Â·-â€”â€“")
    return s

def merge_broken_lines(lines: List[str]) -> List[str]:
    out, buf = [], ""
    for raw in lines:
        s = strip_noise_line(raw)
        if not s: continue
        s = s.lstrip("â†³").strip()
        if buf and (buf.endswith((":", "Â·", "â€¢", "â–ª", "â–¶", "â–·")) or re.match(r"^\(.*\)$", buf)):
            buf += " " + s
        else:
            if buf: out.append(buf)
            buf = s
    if buf: out.append(buf)
    return out

def combine_date_with_next(lines: List[str]) -> List[str]:
    out = []; i = 0
    while i < len(lines):
        cur = strip_noise_line(lines[i])
        if re.search(DATE_PAT, cur) and (i + 1) < len(lines):
            nxt = strip_noise_line(lines[i + 1])
            if re.search(r"(ì‚¬ë§|ì‚¬ìƒ|ì‚¬ê³ |ì¤‘ë…|í™”ì¬|ë¶•ê´´|ì§ˆì‹|ì¶”ë½|ê¹”ë¦¼|ë¶€ë”ªí˜|ë¬´ë„ˆì§|ë‚™í•˜)", nxt):
                m = re.search(DATE_PAT, cur); y, mo, d = m.groups()
                y = int(str(y).replace("â€™","").replace("'",""))
                y = 2000 + y if y < 100 else y
                out.append(f"{int(y)}ë…„ {int(mo)}ì›” {int(d)}ì¼, {nxt}")
                i += 2; continue
        out.append(cur); i += 1
    return out

def preprocess_text_to_sentences(text: str) -> List[str]:
    text = normalize_text(text)
    raw_lines = [ln for ln in text.splitlines() if ln.strip()]
    lines = merge_broken_lines(raw_lines)
    lines = combine_date_with_next(lines)
    joined = "\n".join(lines)
    raw = rxx.split(r"(?<=[\.!\?]|ë‹¤\.)\s+|\n+", joined)
    sents = []
    for s in raw:
        s2 = strip_noise_line(s)
        if not s2: continue
        if re.search(r"(ì£¼ìš”ì‚¬ê³ |ì•ˆì „ì‘ì—…ë°©ë²•|ì½˜í…ì¸ ë§í¬|ì£¼ìš” ì‚¬ê³ ê°œìš”)$", s2):
            continue
        if len(s2) < 6: continue
        sents.append(s2)
    # ì¤‘ë³µ ì œê±°
    seen, dedup = set(), []
    for s in sents:
        k = re.sub(r"\s+", "", s)
        if k not in seen:
            seen.add(k); dedup.append(s)
    return dedup

# ==========================================================
# PDF ì²˜ë¦¬
# ==========================================================
def read_pdf_text(b: bytes) -> str:
    try:
        with io.BytesIO(b) as bio:
            t = pdf_extract_text(bio) or ""
    except Exception:
        t = ""
    t = normalize_text(t)
    if len(t.strip()) < 10:
        try:
            with io.BytesIO(b) as bio:
                pdf = pdfium.PdfDocument(bio)
                if len(pdf) > 0 and not t.strip():
                    st.warning("âš ï¸ ì´ë¯¸ì§€/ìŠ¤ìº” PDFë¡œ ë³´ì…ë‹ˆë‹¤. í˜„ì¬ OCR ë¯¸ì§€ì›.")
        except Exception:
            pass
    return t

# ==========================================================
# ìš”ì•½(TextRank+MMR) â€” ì„¸ì…˜ KB ê°€ì¤‘
# ==========================================================
def tokens(s: str) -> List[str]:
    return rxx.findall(r"[ê°€-í£a-z0-9]{2,}", s.lower())

def sentence_tfidf_vectors(sents: List[str], kb_boost: Dict[str, float] = None) -> Tuple[np.ndarray, List[str]]:
    toks = [tokens(s) for s in sents]
    vocab: Dict[str, int] = {}
    for ts in toks:
        for t in ts:
            if t not in vocab:
                vocab[t] = len(vocab)
    if not vocab:
        return np.zeros((len(sents), 0), dtype=np.float32), []
    M = np.zeros((len(sents), len(vocab)), dtype=np.float32)
    df = np.zeros((len(vocab),), dtype=np.float32)
    for i, ts in enumerate(toks):
        for t in ts:
            w = 1.0
            if kb_boost and t in kb_boost:
                w *= kb_boost[t]
            M[i, vocab[t]] += w
        for t in set(ts):
            df[vocab[t]] += 1.0
    N = float(len(sents))
    idf = np.log((N + 1.0) / (df + 1.0)) + 1.0
    M *= idf
    if kb_boost:
        for t, idx in vocab.items():
            if t in kb_boost:
                M[:, idx] *= (1.0 + 0.2 * kb_boost[t])
    M /= (np.linalg.norm(M, axis=1, keepdims=True) + 1e-8)
    return M, list(vocab.keys())

def cosim(X: np.ndarray) -> np.ndarray:
    if X.size == 0:
        return np.zeros((X.shape[0], X.shape[0]), dtype=np.float32)
    S = np.clip(X @ X.T, 0.0, 1.0)
    np.fill_diagonal(S, 0.0)
    return S

def textrank_scores(sents: List[str], X: np.ndarray, d: float = 0.85, max_iter: int = 60, tol: float = 1e-4) -> List[float]:
    n = len(sents)
    if n == 0: return []
    W = cosim(X)
    row = W.sum(axis=1, keepdims=True)
    P = np.divide(W, row, out=np.zeros_like(W), where=row > 0)
    r = np.ones((n, 1), dtype=np.float32) / n
    tel = np.ones((n, 1), dtype=np.float32) / n
    for _ in range(max_iter):
        r2 = d * (P.T @ r) + (1 - d) * tel
        if np.linalg.norm(r2 - r, 1) < tol:
            r = r2; break
        r = r2
    return [float(v) for v in r.flatten()]

def mmr_select(sents: List[str], scores: List[float], X: np.ndarray, k: int, lam: float = 0.7) -> List[int]:
    S = cosim(X); sel: List[int] = []; rem = set(range(len(sents)))
    while rem and len(sel) < k:
        best, val = None, -1e9
        for i in rem:
            rel = scores[i]
            div = max((S[i, j] for j in sel), default=0.0)
            sc = lam * rel - (1 - lam) * div
            if sc > val: val, best = sc, i
        sel.append(best)  # type: ignore
        rem.remove(best)  # type: ignore
    return sel

def ai_extract_summary(text: str, limit: int = 8) -> List[str]:
    sents = preprocess_text_to_sentences(text)
    if not sents: return []
    kb = st.session_state.kb_terms
    total = sum(kb.values()) or 1
    kb_boost = {t: 1.0 + (cnt / total) * 3.0 for t, cnt in kb.items()} if kb else None
    X, _ = sentence_tfidf_vectors(sents, kb_boost=kb_boost)
    scores = textrank_scores(sents, X)
    idx = mmr_select(sents, scores, X, limit, lam=0.7)
    return [sents[i] for i in idx]

# ==========================================================
# ë„ë©”ì¸ í…œí”Œë¦¿(ì•ˆì „ì¥ì¹˜ ì ìš©)
# ==========================================================
DOMAIN_TEMPLATES = [
    ({"ë¹„ê³„","ë°œíŒ","ê°±í¼","ì¶”ë½"}, "ì‘ì—…ë°œíŒì„ ê²¬ê³ í•˜ê²Œ ì„¤ì¹˜í•˜ê³  ì•ˆì „ë‚œê°„ ë° ì¶”ë½ë°©í˜¸ë§ì„ í™•ë³´í•©ë‹ˆë‹¤."),
    ({"ì•ˆì „ë‚œê°„","ë‚œê°„","ê°œêµ¬ë¶€"}, "ê°œêµ¬ë¶€Â·ê°œêµ¬ì°½ ë“± ì¶”ë½ ìœ„í—˜ êµ¬ê°„ì— ì•ˆì „ë‚œê°„ì„ ì„¤ì¹˜í•©ë‹ˆë‹¤."),
    ({"ì•ˆì „ëŒ€","ë¼ì´í”„ë¼ì¸","ë²¨íŠ¸"}, "ì•ˆì „ëŒ€ë¥¼ ì•ˆì „í•œ ì§€ì§€ì ì— ì—°ê²°í•˜ê³  ë¼ì´í”„ë¼ì¸ì„ ì‚¬ìš©í•©ë‹ˆë‹¤."),
    ({"MSDS","êµ­ì†Œë°°ê¸°","í™˜ê¸°"}, "ì·¨ê¸‰ ë¬¼ì§ˆì˜ MSDSë¥¼ í™•ì¸í•˜ê³  êµ­ì†Œë°°ê¸°ì¥ì¹˜ë¥¼ ê°€ë™í•˜ì—¬ ì¶©ë¶„íˆ í™˜ê¸°í•©ë‹ˆë‹¤."),
    ({"ì˜ˆì´ˆ","ë²Œëª©","ì˜ˆì´ˆê¸°"}, "ì˜ˆì´ˆÂ·ë²Œëª© ì‘ì—… ì‹œ ì‘ì—…ì ê°„ ì•ˆì „ê±°ë¦¬ë¥¼ ìœ ì§€í•˜ê³  ëŒ€í”¼ë¡œë¥¼ í™•ë³´í•©ë‹ˆë‹¤."),
    ({"í¬ë ˆì¸","ì–‘ì¤‘"}, "ì–‘ì¤‘ ê³„íšì„ ìˆ˜ë¦½í•˜ê³  ì‹ í˜¸ìˆ˜ë¥¼ ì§€ì •í•˜ì—¬ í†µì‹ ì„ ìœ ì§€í•©ë‹ˆë‹¤."),
    ({"ì»¨ë² ì´ì–´","í˜‘ì°©","íšŒì „ì²´"}, "íšŒì „ì²´Â·ë¬¼ë¦¼ì  ì ‘ì´‰ì„ ë°©ì§€í•˜ë„ë¡ ë°©í˜¸ì¥ì¹˜ë¥¼ ì„¤ì¹˜í•˜ê³  ì ê²€í•©ë‹ˆë‹¤."),
]

def jaccard(a: set, b: set) -> float:
    return len(a & b) / (len(a | b) + 1e-8)

# ==========================================================
# ë¼ë²¨/ë¶„ë¥˜/ë¦¬ë¼ì´íŒ…
# ==========================================================
ACTION_VERBS = [
    "ì„¤ì¹˜","ë°°ì¹˜","ì°©ìš©","ì ê²€","í™•ì¸","ì¸¡ì •","ê¸°ë¡","í‘œì‹œ","ì œê³µ","ë¹„ì¹˜",
    "ë³´ê³ ","ì‹ ê³ ","êµìœ¡","ì£¼ì§€","ì¤‘ì§€","í†µì œ","íœ´ì‹","í™˜ê¸°","ì°¨ë‹¨","êµëŒ€","ë°°ì œ","ë°°ë ¤",
    "ê°€ë™","ì¤€ìˆ˜","ìš´ì˜","ìœ ì§€","êµì²´","ì •ë¹„","ì²­ì†Œ","ê³ ì •","ê²©ë¦¬","ë³´í˜¸","ë³´ìˆ˜","ì‘ì„±","ì§€ì •"
]
ACTION_PAT = (
    r"(?P<obj>[ê°€-í£a-zA-Z0-9Â·\(\)\[\]\/\-\s]{2,}?)\s*"
    r"(?P<verb>" + "|".join(ACTION_VERBS) + r"|ì‹¤ì‹œ|ìš´ì˜|ê´€ë¦¬)\b"
    r"|(?P<obj2>[ê°€-í£a-zA-Z0-9Â·\(\)\[\]\/\-\s]{2,}?)\s*(ì„|ë¥¼)\s*"
    r"(?P<verb2>" + "|".join(ACTION_VERBS) + r"|ì‹¤ì‹œ|ìš´ì˜|ê´€ë¦¬)\b"
)

def drop_label_token(t: str) -> bool:
    if t in STOP_TERMS: return True
    for pat in LABEL_DROP_PAT:
        if re.match(pat, t): return True
    if t in {"ì†Œì¬","ì†Œì¬ì§€","ì§€ì—­","ì¥ì†Œ","ë²„ìŠ¤","ì˜ì—…ì†Œ","ì—…ì²´","ìë£Œ","í‚¤","ë©”ì„¸ì§€","ëª…","ì•ˆì „ë³´ê±´"}:
        return True
    return False

def top_terms_for_label(text: str, k: int = 3) -> List[str]:
    doc_cnt = Counter([t for t in tokens(text) if not drop_label_token(t)])
    bonus = Counter()
    for t in list(doc_cnt.keys()):
        if t in RISK_KEYWORDS:
            bonus[RISK_KEYWORDS[t]] += doc_cnt[t]
    doc_cnt += bonus
    kb = st.session_state.kb_terms
    if kb:
        for t, c in kb.items():
            if not drop_label_token(t):
                doc_cnt[t] += 0.2 * c
    if not doc_cnt: return ["ì•ˆì „ë³´ê±´", "êµìœ¡"]
    commons = {"ì•ˆì „","êµìœ¡","ì‘ì—…","í˜„ì¥","ì˜ˆë°©","ì¡°ì¹˜","í™•ì¸","ê´€ë¦¬","ì ê²€","ê°€ì´ë“œ","ì§€ì¹¨"}
    cand = [(t, doc_cnt[t]) for t in doc_cnt if t not in commons and len(t) >= 2]
    if not cand: cand = list(doc_cnt.items())
    cand.sort(key=lambda x: x[1], reverse=True)
    return [t for t, _ in cand[:k]]

def dynamic_topic_label(text: str) -> str:
    terms = top_terms_for_label(text, k=3)
    risks = [RISK_KEYWORDS.get(t, t) for t in terms if t in RISK_KEYWORDS or t in RISK_KEYWORDS.values()]
    extra = [t for t in terms if t not in risks]
    label_core = " ".join(sorted(set(risks), key=risks.index)) or "ì•ˆì „ë³´ê±´"
    tail = " ".join(extra[:1])
    label = (label_core + (" " + tail if tail else "")).strip()
    if "ì˜ˆë°©" not in label:
        label += " ì¬í•´ì˜ˆë°©"
    return label

def soften(s: str) -> str:
    s = s.replace("í•˜ì—¬ì•¼", "í•´ì•¼ í•©ë‹ˆë‹¤").replace("í•œë‹¤", "í•©ë‹ˆë‹¤").replace("í•œë‹¤.", "í•©ë‹ˆë‹¤.")
    s = s.replace("ë°”ëë‹ˆë‹¤", "í•´ì£¼ì„¸ìš”").replace("í™•ì¸ ë°”ëŒ", "í™•ì¸í•´ì£¼ì„¸ìš”")
    s = s.replace("ê¸ˆì§€í•œë‹¤", "ê¸ˆì§€í•©ë‹ˆë‹¤").replace("í•„ìš”í•˜ë‹¤", "í•„ìš”í•©ë‹ˆë‹¤")
    s = re.sub(r"^\(([^)]+)\)\s*", "", s)
    for pat in META_PATTERNS:
        s = re.sub(pat, "", s).strip()
    s = re.sub(BULLET_PREFIX, "", s).strip(" -â€¢â—\t")
    return s

def is_accident_sentence(s: str) -> bool:
    if any(w in s for w in ["ì˜ˆë°©", "ëŒ€ì±…", "ì§€ì¹¨", "ìˆ˜ì¹™"]):
        return False
    return bool(re.search(DATE_PAT, s) or re.search(r"(ì‚¬ë§|ì‚¬ìƒ|ì‚¬ê³ |ì¤‘ë…|í™”ì¬|ë¶•ê´´|ì§ˆì‹|ì¶”ë½|ê¹”ë¦¼|ë¶€ë”ªí˜|ë¬´ë„ˆì§|ë‚™í•˜)", s))

def is_prevention_sentence(s: str) -> bool:
    return any(w in s for w in ["ì˜ˆë°©", "ëŒ€ì±…", "ì§€ì¹¨", "ìˆ˜ì¹™", "ì•ˆì „ì¡°ì¹˜"])

def is_risk_sentence(s: str) -> bool:
    return any(w in s for w in ["ìœ„í—˜", "ìš”ì¸", "ì›ì¸", "ì¦ìƒ", "ê²°ë¹™", "ê°•í’", "í­ì—¼", "ë¯¸ì„¸ë¨¼ì§€", "íšŒì „ì²´", "ë¹„ì‚°", "ë§ë¦¼", "ì¶”ë½", "ë‚™í•˜", "í˜‘ì°©"])

def naturalize_case_sentence(s: str) -> str:
    s = soften(s)
    death = re.search(r"ì‚¬ë§\s*(\d+)\s*ëª…", s)
    inj = re.search(r"ì‚¬ìƒ\s*(\d+)\s*ëª…", s)
    unconscious = re.search(r"ì˜ì‹ë¶ˆëª…", s)
    info = []
    if death: info.append(f"ê·¼ë¡œì {death.group(1)}ëª… ì‚¬ë§")
    if inj and not death: info.append(f"{inj.group(1)}ëª… ì‚¬ìƒ")
    if unconscious: info.append("ì˜ì‹ë¶ˆëª… ë°œìƒ")
    m = re.search(DATE_PAT, s)
    date_txt = ""
    if m:
        y, mo, d = m.groups()
        y = int(str(y).replace("â€™","").replace("'",""))
        y = 2000 + y if y < 100 else y
        date_txt = f"{int(y)}ë…„ {int(mo)}ì›” {int(d)}ì¼, "
        s = s.replace(m.group(0), "").strip()
    s = s.strip(" ,.-")
    if not re.search(r"(ì‚¬ë§|ì‚¬ìƒ|ì‚¬ê³ |ì¤‘ë…|ë¶•ê´´|ì§ˆì‹|ì¶”ë½|ê¹”ë¦¼|ë¶€ë”ªí˜|ë¬´ë„ˆì§|ë‚™í•˜)", s):
        if not re.search(r"(ë‹¤\.|ì…ë‹ˆë‹¤\.|í–ˆìŠµë‹ˆë‹¤\.)$", s):
            s = s.rstrip(" .") + " ì‚¬ê³ ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
    tail = ""
    if info and not s.endswith("ìŠµë‹ˆë‹¤."):
        tail = " " + (", ".join(info)) + "í–ˆìŠµë‹ˆë‹¤."
    return (date_txt + s + tail).strip()

# ë„ë©”ì¸ í…œí”Œë¦¿(ì‹ ì¤‘ ì ìš©)
def _domain_template_apply(s: str, base_text: str) -> str:
    if not st.session_state.get("domain_toggle"):
        return s
    sent_toks = set(tokens(s))
    base_toks = set(tokens(base_text))
    # ìœ ì‚¬ë„ ê¸°ì¤€(ê³¼ì ìš© ë°©ì§€)
    if jaccard(sent_toks, base_toks) < 0.05:
        return s
    best = None; best_hits = 0
    for triggers, render in DOMAIN_TEMPLATES:
        if (sent_toks & triggers) and (base_toks & triggers):
            hits = len((sent_toks | base_toks) & triggers)
            if hits > best_hits:
                best_hits = hits; best = render
    return best if best else s

def to_action_sentence(s: str, base_text: str) -> str:
    s2 = soften(s)
    # í™ë³´ì„±/ì˜ë¯¸ ë¶ˆëª… êµ¬ì ˆ ì œê±°
    s2 = re.sub(r"(ìœ„ê¸°íƒˆì¶œ\s*ì•ˆì „ë³´ê±´)", "", s2).strip()
    # 'ì— ë”°ë¥¸/ë”°ë¼' êµì •
    s2 = re.sub(r"\s*ì—\s*ë”°ë¥¸\s*", " ì‹œ ", s2)
    s2 = re.sub(r"\s*ì—\s*ë”°ë¼\s*", " ì‹œ ", s2)
    # ë„ë©”ì¸ í…œí”Œë¦¿(ì•ˆì „ì¥ì¹˜) ì ìš©
    s2_tpl = _domain_template_apply(s2, base_text)
    if s2_tpl != s2:
        return s2_tpl if s2_tpl.endswith(("ë‹¤.", "ìŠµë‹ˆë‹¤.", "í•©ë‹ˆë‹¤.")) else (s2_tpl.rstrip(" .") + " í•©ë‹ˆë‹¤.")
    # ì¼ë°˜ íŒ¨í„´
    m = re.search(ACTION_PAT, s2)
    if not m:
        return s2 if s2.endswith(("ë‹ˆë‹¤.", "í•©ë‹ˆë‹¤.", "ë‹¤.")) else (s2.rstrip(" .") + " í•©ë‹ˆë‹¤.")
    obj = (m.group("obj") or m.group("obj2") or "").strip()
    verb = (m.group("verb") or m.group("verb2") or "ì‹¤ì‹œ").strip()
    prefix = "ë°˜ë“œì‹œ " if "ì„¤ì¹˜" in verb else ("ì‘ì—… ì „ " if verb in ("í™•ì¸","ì ê²€","ì¸¡ì •","ê¸°ë¡","ì‘ì„±","ì§€ì •") else "")
    if obj and not re.search(r"(ì„|ë¥¼|ì—|ì—ì„œ|ê³¼|ì™€|ì˜)$", obj):
        obj += "ë¥¼"
    core = f"{prefix}{obj} {verb}".strip()
    core = re.sub(r"\s+", " ", core)
    return (core + "í•©ë‹ˆë‹¤.").replace("  ", " ")

def classify_sentence(s: str) -> str:
    if is_accident_sentence(s): return "case"
    if re.search(ACTION_PAT, s) or is_prevention_sentence(s): return "action"
    if is_risk_sentence(s): return "risk"
    if "?" in s or "í™•ì¸" in s or "ì ê²€" in s: return "question"
    return "other"

# ==========================================================
# ì„¸ì…˜ KB ëˆ„ì /í™œìš© (+ ì‹œë“œ KB ë¡œë“œ)
# ==========================================================
def seed_kb_once():
    if not st.session_state.seed_loaded:
        # ìœ„í—˜ìœ í˜• ì‹œë“œ
        for t, k in SEED_RISK_MAP.items():
            if t not in RISK_KEYWORDS:
                RISK_KEYWORDS[t] = k
        # í–‰ë™/ì§ˆë¬¸ ì‹œë“œ
        for a in SEED_ACTIONS:
            if 2 <= len(a) <= 160:
                st.session_state.kb_actions.append(a if a.endswith(("ë‹¤","ë‹¤.","í•©ë‹ˆë‹¤","í•©ë‹ˆë‹¤.")) else a + " í•©ë‹ˆë‹¤.")
        for q in SEED_QUESTIONS:
            st.session_state.kb_questions.append(q if q.endswith("?") else q + "?")
        # ìš©ì–´ ì‹œë“œ(ìœ„í—˜ìœ í˜• ê°€ì¤‘)
        for t in SEED_RISK_MAP.keys():
            st.session_state.kb_terms[t] += 5
        st.session_state.seed_loaded = True

def kb_ingest_text(text: str) -> None:
    if not (text or "").strip(): return
    sents = preprocess_text_to_sentences(text)
    for s in sents:
        for t in tokens(s):
            if len(t) >= 2:
                st.session_state.kb_terms[t] += 1
                if re.search(r"(ì¶”ë½|ë‚™í•˜|ê¹”ë¦¼|ë¼ì„|ì¤‘ë…|ì§ˆì‹|í™”ì¬|í­ë°œ|ê°ì „|í­ì—¼|ë¶•ê´´|ë¹„ê³„|ê°±í¼|ì˜ˆì´ˆ|ë²Œëª©|ì»¨ë² ì´ì–´|í¬ë ˆì¸|ì§€ë¶•|ì„ ë°˜|ì²œê³µ)", t):
                    if t not in RISK_KEYWORDS:
                        RISK_KEYWORDS[t] = t
    for s in sents:
        if re.search(ACTION_PAT, s) or is_prevention_sentence(s):
            cand = to_action_sentence(s, text)
            if 2 <= len(cand) <= 160:
                st.session_state.kb_actions.append(cand)
    for s in sents:
        if "?" in s or "í™•ì¸" in s or "ì ê²€" in s:
            q = soften(s if s.endswith("?") else s + " ë§ìŠµë‹ˆê¹Œ?")
            if 2 <= len(q) <= 140:
                st.session_state.kb_questions.append(q)

def kb_prune() -> None:
    def dedup_keep_order(lst: List[str]) -> List[str]:
        seen, out = set(), []
        for x in lst:
            k = re.sub(r"\s+", "", x)
            if k not in seen:
                seen.add(k); out.append(x)
        return out
    st.session_state.kb_actions = dedup_keep_order(st.session_state.kb_actions)[:900]
    st.session_state.kb_questions = dedup_keep_order(st.session_state.kb_questions)[:500]
    st.session_state.kb_terms = Counter(dict(st.session_state.kb_terms.most_common(1800)))

def kb_match_candidates(cands: List[str], base_text: str, limit: int) -> List[str]:
    bt = set(tokens(base_text))
    scored: List[Tuple[float, str]] = []
    for c in cands:
        ct = set(tokens(c))
        j = len(bt & ct) / (len(bt | ct) + 1e-8)
        if j > 0:
            scored.append((j, c))
    scored.sort(key=lambda x: x[0], reverse=True)
    return [c for _, c in scored[:limit]]

# ==========================================================
# ìƒì„± ëª¨ë“œ 1: ìì—°ìŠ¤ëŸ¬ìš´ êµìœ¡ëŒ€ë³¸(ë¬´ë£Œ)
# ==========================================================
def make_structured_script(text: str, max_points: int = 6) -> str:
    topic_label = dynamic_topic_label(text)
    core = [soften(s) for s in ai_extract_summary(text, max_points)]
    if not core:
        return "ë³¸ë¬¸ì´ ì¶©ë¶„í•˜ì§€ ì•Šì•„ ëŒ€ë³¸ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

    case, risk, act, ask = [], [], [], []
    for s in core:
        c = classify_sentence(s)
        if c == "case":
            case.append(naturalize_case_sentence(s))
        elif c == "action":
            act.append(to_action_sentence(s, text))
        elif c == "risk":
            risk.append(soften(s))
        elif c == "question":
            ask.append(soften(s if s.endswith("?") else s + " ë§ìŠµë‹ˆê¹Œ?"))

    if len(act) < 5 and st.session_state.kb_actions:
        act += kb_match_candidates(st.session_state.kb_actions, text, 5 - len(act))
    act = act[:5]

    if not ask and st.session_state.kb_questions:
        ask = kb_match_candidates(st.session_state.kb_questions, text, 3)
    if not ask:
        ask = ["í•„ìš”í•œ ì•ˆì „ì¡°ì¹˜ê°€ ì˜¤ëŠ˜ ì‘ì—… ë²”ìœ„ì— ë§ê²Œ ì¤€ë¹„ë˜ì–´ ìˆìŠµë‹ˆê¹Œ?"]

    lines = []
    lines.append(f"ğŸ¦º TBM êµìœ¡ëŒ€ë³¸ â€“ {topic_label}\n")
    lines.append("â— ë„ì…")
    lines.append(f"ì˜¤ëŠ˜ì€ ìµœê·¼ ë°œìƒí•œ '{topic_label.replace(' ì¬í•´ì˜ˆë°©','')}' ì‚¬ê³  ì‚¬ë¡€ë¥¼ ì¤‘ì‹¬ìœ¼ë¡œ, ìš°ë¦¬ í˜„ì¥ì—ì„œ ê°™ì€ ì‚¬ê³ ë¥¼ ì˜ˆë°©í•˜ê¸° ìœ„í•œ ì•ˆì „ì¡°ì¹˜ë¥¼ í•¨ê»˜ ì‚´í´ë³´ê² ìŠµë‹ˆë‹¤.\n")

    if case:
        lines.append("â— ì‚¬ê³  ì‚¬ë¡€")
        for c in case: lines.append(f"- {c}")
        lines.append("")

    if risk:
        lines.append("â— ì£¼ìš” ìœ„í—˜ìš”ì¸")
        for r in risk: lines.append(f"- {r}")
        lines.append("")

    if act:
        lines.append("â— ì˜ˆë°©ì¡°ì¹˜ / ì‹¤ì²œ ìˆ˜ì¹™")
        for i, a in enumerate(act, 1): lines.append(f"{i}ï¸âƒ£ {a}")
        lines.append("")

    if ask:
        lines.append("â— í˜„ì¥ ì ê²€ ì§ˆë¬¸")
        for q in ask: lines.append(f"- {q}")
        lines.append("")

    lines.append("â— ë§ˆë¬´ë¦¬ ë‹¹ë¶€")
    lines.append("ì˜ˆë°©ì¡°ì¹˜ëŠ” 'ì„ ì¡°ì¹˜ í›„ì‘ì—…'ì´ ì›ì¹™ì…ë‹ˆë‹¤. ì˜¤ëŠ˜ ì‘ì—… ì „, ê° ê³µì •ë³„ ìœ„í—˜ìš”ì¸ì„ ë‹¤ì‹œ í•œ ë²ˆ ì ê²€í•˜ê³  í•„ìš”í•œ ë³´í˜¸êµ¬ì™€ ì•ˆì „ì¡°ì¹˜ë¥¼ ë°˜ë“œì‹œ ì¤€ë¹„í•©ì‹œë‹¤.")
    lines.append("â— êµ¬í˜¸")
    lines.append("â€œí•œ ë²ˆ ë” í™•ì¸! í•œ ë²ˆ ë” ì ê²€!â€")

    return "\n".join(lines)

# ==========================================================
# ìƒì„± ëª¨ë“œ 2: í•µì‹¬ìš”ì•½ (ë³´ê³ ì„œí˜•)
# ==========================================================
def make_concise_report(text: str, max_points: int = 6) -> str:
    sents = ai_extract_summary(text, max_points)
    sents = [soften(s) for s in sents if not re.match(r"(ë°°í¬ì²˜|ì£¼ì†Œ|í™ˆí˜ì´ì§€|VR|ë¦¬í”Œë¦¿)", s)]
    if not sents:
        return "í…ìŠ¤íŠ¸ì—ì„œ í•µì‹¬ì„ ìš”ì•½í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

    cases = [naturalize_case_sentence(s) for s in sents if is_accident_sentence(s)]
    risks  = [soften(s) for s in sents if (not is_accident_sentence(s)) and is_risk_sentence(s)]
    acts   = [to_action_sentence(s, text) for s in sents if (not is_accident_sentence(s)) and (is_prevention_sentence(s) or re.search(ACTION_PAT, s))]

    def uniq_keep(seq: List[str]) -> List[str]:
        seen, out = set(), []
        for x in seq:
            k = re.sub(r"\s+", "", x)
            if k not in seen:
                seen.add(k); out.append(x)
        return out

    cases = uniq_keep(cases)[:3]
    risks  = uniq_keep(risks)[:3]
    acts   = uniq_keep(acts)[:4]

    topic = dynamic_topic_label(text)

    lines = [f"ğŸ“„ í•µì‹¬ìš”ì•½ â€” {topic}\n"]
    if cases:
        lines.append("ã€ì‚¬ê³  ê°œìš”ã€‘")
        lines.append("ìµœê·¼ ìë£Œì—ì„œ ë‹¤ìŒê³¼ ê°™ì€ ì‚¬ê³ ê°€ í™•ì¸ë˜ì—ˆìŠµë‹ˆë‹¤.")
        for c in cases:
            lines.append(f"- {c}")
        lines.append("")
    if risks:
        lines.append("ã€ì£¼ìš” ìœ„í—˜ìš”ì¸ã€‘")
        lines.append("ìë£Œ ì „ë°˜ì—ì„œ ë‹¤ìŒ ìš”ì¸ì´ ë°˜ë³µì ìœ¼ë¡œ ë‚˜íƒ€ë‚¬ìŠµë‹ˆë‹¤.")
        for r in risks:
            lines.append(f"- {r}")
        lines.append("")
    if acts:
        lines.append("ã€ì˜ˆë°©/ì‹¤ì²œ ìš”ì•½ã€‘")
        lines.append("í˜„ì¥ì—ì„œ ì¦‰ì‹œ ì ìš© ê°€ëŠ¥í•œ í•µì‹¬ ìˆ˜ì¹™ì…ë‹ˆë‹¤.")
        for a in acts:
            lines.append(f"- {a}")
        lines.append("")
    if not (cases or risks or acts):
        lines.append("ìë£Œì˜ í•µì‹¬ì„ ê°„ë‹¨íˆ ì •ë¦¬í•˜ë©´ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.")
        for s in sents:
            lines.append(f"- {s}")
    return "\n".join(lines)

# ==========================================================
# DOCX ë‚´ë³´ë‚´ê¸° (XML ì•ˆì „í•„í„°)
# ==========================================================
_XML_FORBIDDEN = r"[\x00-\x08\x0B\x0C\x0E-\x1F\uD800-\uDFFF\uFFFE\uFFFF]"
def _xml_safe(s: str) -> str:
    if not isinstance(s, str): s = "" if s is None else str(s)
    return rxx.sub(_XML_FORBIDDEN, "", s)

def to_docx_bytes(script: str) -> bytes:
    doc = Document()
    try:
        style = doc.styles["Normal"]; style.font.name = "Malgun Gothic"; style.font.size = Pt(11)
    except Exception:
        pass
    for raw in script.split("\n"):
        line = _xml_safe(raw)
        p = doc.add_paragraph(line)
        for run in p.runs:
            try:
                run.font.name = "Malgun Gothic"; run.font.size = Pt(11)
            except Exception:
                pass
    bio = io.BytesIO(); doc.save(bio); bio.seek(0); return bio.read()

# ==========================================================
# UI (ê¸°ì¡´ í‹€ ìœ ì§€ / ëª¨ë“œëª…ë§Œ ë³€ê²½ + ì‹œë“œ ë¡œë”©)
# ==========================================================
with st.sidebar:
    st.header("â„¹ï¸ ì†Œê°œ / ì‚¬ìš©ë²•")
    st.markdown("""
**AI íŒŒì´í”„ë¼ì¸(ì™„ì „ ë¬´ë£Œ)**  
- ì „ì²˜ë¦¬ â†’ TextRank+MMR ìš”ì•½(ì„¸ì…˜ KB ê°€ì¤‘) â†’ ë°ì´í„° ê¸°ë°˜ ë¦¬ë¼ì´íŒ…  
- PDF/ZIP/í…ìŠ¤íŠ¸ ì—…ë¡œë“œ ì‹œ ì¦‰ì‹œ ëˆ„ì  í•™ìŠµ(ìš©ì–´/í–‰ë™/ì§ˆë¬¸).
- ì´ë¯¸ì§€/ìŠ¤ìº” PDFëŠ” í…ìŠ¤íŠ¸ ì¶”ì¶œì´ ì–´ë ¤ìš¸ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
""")
    # ğŸ”§ ë„ë©”ì¸ í…œí”Œë¦¿ ì˜µì…˜ (ê¸°ë³¸ Off)
    st.session_state.domain_toggle = st.toggle("ğŸ”§ ë„ë©”ì¸ í…œí”Œë¦¿ ê°•í™”(ì‹ ì¤‘ ì ìš©)", value=False,
                                              help="ë¬¸ì¥Â·ë³¸ë¬¸ íŠ¸ë¦¬ê±° ì¼ì¹˜ + ìœ ì‚¬ë„ ê¸°ì¤€ ì¶©ì¡± ì‹œì—ë§Œ í…œí”Œë¦¿ì„ ì ìš©í•©ë‹ˆë‹¤. ìƒˆë¡œìš´ íŒŒì¼ì—” ë³´ìˆ˜ì ìœ¼ë¡œ ë™ì‘í•©ë‹ˆë‹¤.")

# ì‹œë“œ KB ìµœì´ˆ 1íšŒ ë¡œë”©
seed_kb_once()

st.title("ğŸ¦º OPS/í¬ìŠ¤í„°ë¥¼ êµìœ¡ ëŒ€ë³¸ìœ¼ë¡œ ìë™ ë³€í™˜ (ì™„ì „ ë¬´ë£Œ)")

def reset_all():
    st.session_state.pop("manual_text", None)
    st.session_state.pop("edited_text", None)
    st.session_state.pop("zip_choice", None)
    st.session_state.kb_terms = Counter()
    st.session_state.kb_actions = []
    st.session_state.kb_questions = []
    st.session_state.uploader_key += 1
    st.session_state.seed_loaded = False
    st.rerun()

col_top1, col_top2 = st.columns([4, 1])
with col_top2:
    st.button("ğŸ§¹ ì´ˆê¸°í™”", on_click=reset_all, use_container_width=True)

st.markdown("""
**ì•ˆë‚´**  
- í…ìŠ¤íŠ¸ê°€ í¬í•¨ëœ PDF ë˜ëŠ” ë³¸ë¬¸ í…ìŠ¤íŠ¸ë¥¼ ê¶Œì¥í•©ë‹ˆë‹¤.  
- ì´ë¯¸ì§€/ìŠ¤ìº” PDFëŠ” í˜„ì¬ OCR ë¯¸ì§€ì›ì…ë‹ˆë‹¤.
""")

col1, col2 = st.columns([1, 1], gap="large")

# ---------- ì¢Œì¸¡ ì…ë ¥/ë¯¸ë¦¬ë³´ê¸° ----------
with col1:
    uploaded = st.file_uploader(
        "OPS ì—…ë¡œë“œ (PDF ë˜ëŠ” ZIP) â€¢ í…ìŠ¤íŠ¸ PDF ê¶Œì¥",
        type=["pdf", "zip"],
        key=f"uploader_{st.session_state.uploader_key}"
    )
    manual_text = st.text_area(
        "ë˜ëŠ” OPS í…ìŠ¤íŠ¸ ì§ì ‘ ë¶™ì—¬ë„£ê¸°",
        key="manual_text",
        height=220,
        placeholder="ì˜ˆ: í˜„ì¥ ì•ˆë‚´ë¬¸ ë˜ëŠ” OPS ë³¸ë¬¸ í…ìŠ¤íŠ¸â€¦"
    )

    extracted: str = ""
    zip_pdfs: Dict[str, bytes] = {}
    selected_zip_pdf: str = ""

    if uploaded is not None:
        fname = (uploaded.name or "").lower()
        if fname.endswith(".zip"):
            try:
                with zipfile.ZipFile(uploaded, "r") as zf:
                    for name in zf.namelist():
                        if name.lower().endswith(".pdf"):
                            data = zf.read(name)
                            zip_pdfs[name] = data
                            txt = read_pdf_text(data)
                            if txt.strip():
                                kb_ingest_text(txt)  # ZIP ì „ì²´ í•™ìŠµ
                kb_prune()
            except Exception as e:
                st.error(f"ZIP í•´ì œ ì˜¤ë¥˜: {e}")
            if zip_pdfs:
                selected_zip_pdf = st.selectbox("ZIP ë‚´ PDF ì„ íƒ", list(zip_pdfs.keys()), key="zip_choice")
                if selected_zip_pdf:
                    with st.spinner("ZIP ë‚´ë¶€ PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ ì¤‘..."):
                        extracted = read_pdf_text(zip_pdfs[selected_zip_pdf])
        elif fname.endswith(".pdf"):
            with st.spinner("PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ ì¤‘..."):
                data = uploaded.read()
                extracted = read_pdf_text(data)
                if extracted.strip():
                    kb_ingest_text(extracted); kb_prune()
                else:
                    st.warning("âš ï¸ PDFì—ì„œ ìœ íš¨í•œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        else:
            st.warning("ì§€ì›í•˜ì§€ ì•ŠëŠ” í˜•ì‹ì…ë‹ˆë‹¤. PDF ë˜ëŠ” ZIPì„ ì—…ë¡œë“œí•˜ì„¸ìš”.")

    pasted = (manual_text or "").strip()
    if pasted:
        kb_ingest_text(pasted); kb_prune()

    base_text = pasted or extracted.strip()
    st.markdown("**ì¶”ì¶œ/ì…ë ¥ í…ìŠ¤íŠ¸ ë¯¸ë¦¬ë³´ê¸°**")
    edited_text = st.text_area("í…ìŠ¤íŠ¸", value=base_text, height=240, key="edited_text")

# ---------- ìš°ì¸¡ ì˜µì…˜/ìƒì„±/ë‹¤ìš´ë¡œë“œ ----------
with col2:
    gen_mode = st.selectbox("ğŸ§  ìƒì„± ëª¨ë“œ", ["í•µì‹¬ìš”ì•½", "ìì—°ìŠ¤ëŸ¬ìš´ êµìœ¡ëŒ€ë³¸(ë¬´ë£Œ)"])
    max_points = st.slider("ìš”ì•½ ê°•ë„(í•µì‹¬ë¬¸ì¥ ê°œìˆ˜)", 3, 10, 6)

    if st.button("ğŸ› ï¸ ëŒ€ë³¸ ìƒì„±", type="primary", use_container_width=True):
        text_for_gen = (st.session_state.get("edited_text") or "").strip()
        if not text_for_gen:
            st.warning("í…ìŠ¤íŠ¸ê°€ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤. PDF/ZIP ì—…ë¡œë“œ ë˜ëŠ” í…ìŠ¤íŠ¸ ì…ë ¥ í›„ ì‹œë„í•˜ì„¸ìš”.")
        else:
            with st.spinner("ìƒì„± ì¤‘..."):
                if gen_mode == "ìì—°ìŠ¤ëŸ¬ìš´ êµìœ¡ëŒ€ë³¸(ë¬´ë£Œ)":
                    script = make_structured_script(text_for_gen, max_points=max_points)
                    subtitle = "ìì—°ìŠ¤ëŸ¬ìš´ êµìœ¡ëŒ€ë³¸(ë¬´ë£Œ)"
                else:
                    script = make_concise_report(text_for_gen, max_points=max_points)
                    subtitle = "í•µì‹¬ìš”ì•½"
            st.success(f"ìƒì„± ì™„ë£Œ! ({subtitle})")
            st.text_area("ê²°ê³¼ ë¯¸ë¦¬ë³´ê¸°", value=script, height=420)
            c3, c4 = st.columns(2)
            with c3:
                st.download_button("â¬‡ï¸ TXT ë‹¤ìš´ë¡œë“œ", data=_xml_safe(script).encode("utf-8"),
                                   file_name="tbm_output.txt", use_container_width=True)
            with c4:
                st.download_button("â¬‡ï¸ DOCX ë‹¤ìš´ë¡œë“œ", data=to_docx_bytes(script),
                                   file_name="tbm_output.docx", use_container_width=True)

st.caption("ì™„ì „ ë¬´ë£Œ. ì‹œë“œ KB(í•™ìŠµíŒŒì¼.zip 24ê°œ) + ì—…ë¡œë“œ/ë¶™ì—¬ë„£ê¸° ëˆ„ì  í•™ìŠµ â†’ ìš”ì•½ ê°€ì¤‘/í–‰ë™/ì§ˆë¬¸ ë³´ê°•. ë™ì  ì£¼ì œ ë¼ë²¨. ë„ë©”ì¸ í…œí”Œë¦¿ì€ ê¸°ë³¸ OFFë¡œ ì•ˆì „ ì ìš©.")
