# ==========================================================
# OPS2TBM â€” OPS/í¬ìŠ¤í„° â†’ TBM êµìœ¡ ëŒ€ë³¸ ìë™ ìƒì„± (ì™„ì „ ë¬´ë£Œ)
#  * ê¸°ì¡´ UI ìœ ì§€ (ì¢Œ: ì—…ë¡œë“œ/ë¯¸ë¦¬ë³´ê¸°, ìš°: ì˜µì…˜/ìƒì„±/ë‹¤ìš´ë¡œë“œ)
#  * ì „ì²˜ë¦¬ â†’ TextRank+MMR ìš”ì•½(ì„¸ì…˜ KB ê°€ì¤‘) â†’ ì‚¬ëŒë§ ê°™ì€ ë¦¬ë¼ì´íŒ…
#  * PDF/ZIP/ë¶™ì—¬ë„£ê¸° ì…ë ¥ì€ ì„¸ì…˜ KB(ìš©ì–´/í–‰ë™/ì§ˆë¬¸)ì— ëˆ„ì 
#  * ì´ë¯¸ì§€/ìŠ¤ìº” PDFëŠ” OCR ë¯¸ì§€ì›(ê²½ê³ )
# ==========================================================

import io, zipfile, re
from collections import Counter
from typing import List, Dict, Tuple
import streamlit as st
from docx import Document
from docx.shared import Pt
from pdfminer.high_level import extract_text as pdf_extract_text
import pypdfium2 as pdfium
import numpy as np
import regex as rxx

st.set_page_config(page_title="OPS2TBM", page_icon="ğŸ¦º", layout="wide")

# ----------------------------
# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
# ----------------------------
if "uploader_key" not in st.session_state:
    st.session_state.uploader_key = 0
if "kb_terms" not in st.session_state:
    st.session_state.kb_terms: Counter = Counter()
if "kb_actions" not in st.session_state:
    st.session_state.kb_actions: List[str] = []
if "kb_questions" not in st.session_state:
    st.session_state.kb_questions: List[str] = []

# ==========================================================
# ì „ì²˜ë¦¬ ì„¤ì •/ìœ í‹¸
# ==========================================================
NOISE_PATTERNS = [
    r"^ì œ?\s?\d{4}\s?[-.]?\s?\d+\s?í˜¸$",
    r"^(ë™ì ˆê¸°\s*ì£¼ìš”ì‚¬ê³ |ì•ˆì „ì‘ì—…ë°©ë²•|ì½˜í…ì¸ ë§í¬|ì±…ì\s*OPS|ìˆí¼\s*OPS)$",
    r"^(í¬ìŠ¤í„°|ì±…ì|ìŠ¤í‹°ì»¤|ì½˜í…ì¸  ë§í¬)$",
    r"^(ìŠ¤ë§ˆíŠ¸í°\s*APP|ì¤‘ëŒ€ì¬í•´\s*ì‚¬ì´ë Œ|ì‚°ì—…ì•ˆì „í¬í„¸|ê³ ìš©ë…¸ë™ë¶€)$",
    r"^https?://\S+$",
    r"^\(?\s*PowerPoint\s*í”„ë ˆì  í…Œì´ì…˜\s*\)?$",
    r"^ì•ˆì „ë³´ê±´ìë£Œì‹¤.*$",
    r"^ë°°í¬ì²˜\s+.*$",
    r"^í™ˆí˜ì´ì§€\s+.*$",
    r"^ì£¼ì†Œ\s+.*$",
    r"^VR\s+.*$",
    r"^ë¦¬í”Œë¦¿\s+.*$",
    r"^ë™ì˜ìƒ\s+.*$",
    r"^APP\s+.*$",
    r".*ê²€ìƒ‰í•´\s*ë³´ì„¸ìš”.*$",
]

BULLET_PREFIX = r"^[\s\-\â€¢\â—\â–ª\â–¶\â–·\Â·\*\u25CF\u25A0\u25B6\u25C6\u2022\u00B7\u279C\u27A4\u25BA\u25AA\u25AB\u2611\u2713\u2714\u2716\u2794\u27A2\uF0FC\uF0A7]+"

DATE_PAT = r"([â€™']?\d{2,4})\.\s?(\d{1,2})\.\s?(\d{1,2})\.?"
#   â€˜25. 02. 24.  /  2025.2.24.  ë“± ëŒ€ì‘

META_PATTERNS = [
    r"<\s*\d+\s*ëª…\s*ì‚¬ë§\s*>",
    r"<\s*\d+\s*ëª…\s*ì‚¬ìƒ\s*>",
    r"<\s*\d+\s*ëª…\s*ì˜ì‹ë¶ˆëª…\s*>",
    r"<\s*ì‚¬ë§\s*\d+\s*ëª…\s*>",
    r"<\s*ì‚¬ìƒ\s*\d+\s*ëª…\s*>",
]

STOP_TERMS = set("""
ë° ë“± ê´€ë ¨ ì‚¬í•­ ë‚´ìš© ì˜ˆë°© ì•ˆì „ ì‘ì—… í˜„ì¥ êµìœ¡ ë°©ë²• ê¸°ì¤€ ì¡°ì¹˜
ì‹¤ì‹œ í™•ì¸ í•„ìš” ê²½ìš° ëŒ€ìƒ ì‚¬ìš© ê´€ë¦¬ ì ê²€ ì ìš© ì •ë„ ì£¼ì˜ ì¤‘ ì „ í›„
ì£¼ìš” ì‚¬ë¡€ ì•ˆì „ì‘ì—…ë°©ë²• í¬ìŠ¤í„° ë™ì˜ìƒ ë¦¬í”Œë¦¿ ê°€ì´ë“œ ìë£Œì‹¤ ê²€ìƒ‰
í‚¤ë©”ì„¸ì§€ êµìœ¡í˜ì‹ ì‹¤ ì•ˆì „ë³´ê±´ê³µë‹¨ ê³µë‹¨ ìë£Œ êµ¬ë… ì•ˆë‚´ ì—°ë½ ì°¸ê³  ì¶œì²˜
ì†Œì¬ ì†Œì¬ì§€ ìœ„ì¹˜ ì¥ì†Œ ì§€ì—­ ì‹œêµ°êµ¬ ì„œìš¸ ì¸ì²œ ë¶€ì‚° ëŒ€êµ¬ ëŒ€ì „ ê´‘ì£¼ ìš¸ì‚° ì„¸ì¢… ê²½ê¸°ë„ ì¶©ì²­ ì „ë¼ ê²½ìƒ ê°•ì› ì œì£¼
ëª… ê±´ í˜¸ í˜¸ì°¨ í˜¸ìˆ˜ í˜ì´ì§€ ìª½ ë¶€ë¡ ì°¸ê³  ê·¸ë¦¼ í‘œ ëª©ì°¨
""".split())

LABEL_DROP_PAT = [
    r"^\d+$", r"^\d{2,4}[-_]\d{1,}$", r"^\d{4}$",
    r"^(ì œ)?\d+í˜¸$", r"^(í˜¸|í˜¸ìˆ˜|í˜¸ì°¨)$",
    r"^(ì‚¬ì—…ì¥|ì—…ì²´|ì†Œì¬|ì†Œì¬ì§€|ì¥ì†Œ|ì§€ì—­)$",
    r"^\d+\s*(ëª…|ê±´)$",
]

RISK_KEYWORDS = {
    "ë–¨ì–´ì§":"ì¶”ë½","ì¶”ë½":"ì¶”ë½","ë‚™í•˜":"ë‚™í•˜","ê¹”ë¦¼":"ê¹”ë¦¼","ë¼ì„":"ë¼ì„",
    "ë§ìŒ":"ì¶©ëŒ","ë¶€ë”ªí˜":"ì¶©ëŒ","ë¬´ë„ˆì§":"ë¶•ê´´","ë¶•ê´´":"ë¶•ê´´",
    "ì§ˆì‹":"ì§ˆì‹","ì¤‘ë…":"ì¤‘ë…","í­ë°œ":"í­ë°œ","í™”ì¬":"í™”ì¬","ê°ì „":"ê°ì „",
    "í­ì—¼":"í­ì—¼","í•œì—´":"í­ì—¼","ì—´ì‚¬ë³‘":"í­ì—¼","ë¯¸ì„¸ë¨¼ì§€":"ë¯¸ì„¸ë¨¼ì§€",
    "ì»¨ë² ì´ì–´":"í˜‘ì°©","ì„ ë°˜":"ì ˆì‚­","í¬ë ˆì¸":"ì–‘ì¤‘","ì²œê³µê¸°":"ì²œê³µ",
    "ì§€ë¶•":"ì§€ë¶•ì‘ì—…","ë¹„ê³„":"ë¹„ê³„","ê°±í¼":"ë¹„ê³„","ë°œíŒ":"ë¹„ê³„"
}

def normalize_text(t: str) -> str:
    t = t.replace("\x0c", "\n")
    t = re.sub(r"[ \t]+\n", "\n", t)
    t = re.sub(r"\n{3,}", "\n\n", t)
    return t.strip()

def strip_noise_line(line: str) -> str:
    s = (line or "").strip()
    if not s: return ""
    s = re.sub(BULLET_PREFIX, "", s).strip()
    for pat in NOISE_PATTERNS:
        if re.match(pat, s, re.IGNORECASE):
            return ""
    s = re.sub(r"https?://\S+", "", s).strip()
    s = s.strip("â€¢â—â–ªâ–¶â–·Â·-â€”â€“")
    return s

def merge_broken_lines(lines: List[str]) -> List[str]:
    out, buf = [], ""
    for raw in lines:
        s = strip_noise_line(raw)
        if not s: continue
        s = s.lstrip("â†³").strip()
        if buf and (buf.endswith((":", "Â·", "â€¢", "â–ª", "â–¶", "â–·")) or re.match(r"^\(.*\)$", buf)):
            buf += " " + s
        else:
            if buf: out.append(buf)
            buf = s
    if buf: out.append(buf)
    return out

def combine_date_with_next(lines: List[str]) -> List[str]:
    out = []; i = 0
    while i < len(lines):
        cur = strip_noise_line(lines[i])
        if re.search(DATE_PAT, cur) and (i + 1) < len(lines):
            nxt = strip_noise_line(lines[i + 1])
            if re.search(r"(ì‚¬ë§|ì‚¬ìƒ|ì‚¬ê³ |ì¤‘ë…|í™”ì¬|ë¶•ê´´|ì§ˆì‹|ì¶”ë½|ê¹”ë¦¼|ë¶€ë”ªí˜|ë¬´ë„ˆì§|ë‚™í•˜)", nxt):
                m = re.search(DATE_PAT, cur); y, mo, d = m.groups()
                # '25 â†’ 2025 ë³´ì •
                y = int(y.replace("â€™","").replace("'",""))
                y = 2000 + y if y < 100 else y
                out.append(f"{int(y)}ë…„ {int(mo)}ì›” {int(d)}ì¼, {nxt}")
                i += 2; continue
        out.append(cur); i += 1
    return out

def preprocess_text_to_sentences(text: str) -> List[str]:
    text = normalize_text(text)
    raw_lines = [ln for ln in text.splitlines() if ln.strip()]
    lines = merge_broken_lines(raw_lines)
    lines = combine_date_with_next(lines)
    joined = "\n".join(lines)
    raw = rxx.split(r"(?<=[\.!\?]|ë‹¤\.)\s+|\n+", joined)
    sents = []
    for s in raw:
        s2 = strip_noise_line(s)
        if not s2: continue
        if re.search(r"(ì£¼ìš”ì‚¬ê³ |ì•ˆì „ì‘ì—…ë°©ë²•|ì½˜í…ì¸ ë§í¬|ì£¼ìš” ì‚¬ê³ ê°œìš”)$", s2):  # ì¡ì œëª© ì œê±°
            continue
        if len(s2) < 6: continue
        sents.append(s2)
    # ì¤‘ë³µ ì œê±°
    seen, dedup = set(), []
    for s in sents:
        k = re.sub(r"\s+", "", s)
        if k not in seen:
            seen.add(k); dedup.append(s)
    return dedup

# ==========================================================
# PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ
# ==========================================================
def read_pdf_text(b: bytes) -> str:
    try:
        with io.BytesIO(b) as bio:
            t = pdf_extract_text(bio) or ""
    except Exception:
        t = ""
    t = normalize_text(t)
    if len(t.strip()) < 10:
        try:
            with io.BytesIO(b) as bio:
                pdf = pdfium.PdfDocument(bio)
                if len(pdf) > 0 and not t.strip():
                    st.warning("âš ï¸ ì´ë¯¸ì§€/ìŠ¤ìº” PDFë¡œ ë³´ì…ë‹ˆë‹¤. í˜„ì¬ OCR ë¯¸ì§€ì›.")
        except Exception:
            pass
    return t

# ==========================================================
# ìš”ì•½(TextRank+MMR) â€” ì„¸ì…˜ KB ê°€ì¤‘
# ==========================================================
def tokens(s: str) -> List[str]:
    return rxx.findall(r"[ê°€-í£a-z0-9]{2,}", s.lower())

def sentence_tfidf_vectors(sents: List[str], kb_boost: Dict[str, float] = None) -> Tuple[np.ndarray, List[str]]:
    toks = [tokens(s) for s in sents]
    vocab: Dict[str, int] = {}
    for ts in toks:
        for t in ts:
            if t not in vocab:
                vocab[t] = len(vocab)
    if not vocab:
        return np.zeros((len(sents), 0), dtype=np.float32), []
    M = np.zeros((len(sents), len(vocab)), dtype=np.float32)
    df = np.zeros((len(vocab),), dtype=np.float32)
    for i, ts in enumerate(toks):
        for t in ts:
            w = 1.0
            if kb_boost and t in kb_boost:
                w *= kb_boost[t]
            M[i, vocab[t]] += w
        for t in set(ts):
            df[vocab[t]] += 1.0
    N = float(len(sents))
    idf = np.log((N + 1.0) / (df + 1.0)) + 1.0
    M *= idf
    if kb_boost:
        for t, idx in vocab.items():
            if t in kb_boost:
                M[:, idx] *= (1.0 + 0.2 * kb_boost[t])
    M /= (np.linalg.norm(M, axis=1, keepdims=True) + 1e-8)
    return M, list(vocab.keys())

def cosim(X: np.ndarray) -> np.ndarray:
    if X.size == 0:
        return np.zeros((X.shape[0], X.shape[0]), dtype=np.float32)
    S = np.clip(X @ X.T, 0.0, 1.0)
    np.fill_diagonal(S, 0.0)
    return S

def textrank_scores(sents: List[str], X: np.ndarray, d: float = 0.85, max_iter: int = 60, tol: float = 1e-4) -> List[float]:
    n = len(sents)
    if n == 0: return []
    W = cosim(X)
    row = W.sum(axis=1, keepdims=True)
    P = np.divide(W, row, out=np.zeros_like(W), where=row > 0)
    r = np.ones((n, 1), dtype=np.float32) / n
    tel = np.ones((n, 1), dtype=np.float32) / n
    for _ in range(max_iter):
        r2 = d * (P.T @ r) + (1 - d) * tel
        if np.linalg.norm(r2 - r, 1) < tol:
            r = r2; break
        r = r2
    return [float(v) for v in r.flatten()]

def mmr_select(sents: List[str], scores: List[float], X: np.ndarray, k: int, lam: float = 0.7) -> List[int]:
    S = cosim(X); sel: List[int] = []; rem = set(range(len(sents)))
    while rem and len(sel) < k:
        best, val = None, -1e9
        for i in rem:
            rel = scores[i]
            div = max((S[i, j] for j in sel), default=0.0)
            sc = lam * rel - (1 - lam) * div
            if sc > val: val, best = sc, i
        sel.append(best)  # type: ignore
        rem.remove(best)  # type: ignore
    return sel

def ai_extract_summary(text: str, limit: int = 8) -> List[str]:
    sents = preprocess_text_to_sentences(text)
    if not sents: return []
    kb = st.session_state.kb_terms
    total = sum(kb.values()) or 1
    kb_boost = {t: 1.0 + (cnt / total) * 3.0 for t, cnt in kb.items()} if kb else None
    X, _ = sentence_tfidf_vectors(sents, kb_boost=kb_boost)
    scores = textrank_scores(sents, X)
    idx = mmr_select(sents, scores, X, limit, lam=0.7)
    return [sents[i] for i in idx]

# ==========================================================
# ë¼ë²¨/ë¬¸ì¥ ë¶„ë¥˜/ë¦¬ë¼ì´íŒ…
# ==========================================================
ACTION_VERBS = [
    "ì„¤ì¹˜","ë°°ì¹˜","ì°©ìš©","ì ê²€","í™•ì¸","ì¸¡ì •","ê¸°ë¡","í‘œì‹œ","ì œê³µ","ë¹„ì¹˜",
    "ë³´ê³ ","ì‹ ê³ ","êµìœ¡","ì£¼ì§€","ì¤‘ì§€","í†µì œ","íœ´ì‹","í™˜ê¸°","ì°¨ë‹¨","êµëŒ€","ë°°ì œ","ë°°ë ¤",
    "ê°€ë™","ì¤€ìˆ˜","ìš´ì˜","ìœ ì§€","êµì²´","ì •ë¹„","ì²­ì†Œ","ê³ ì •","ê²©ë¦¬","ë³´í˜¸","ë³´ìˆ˜","ì‘ì„±","ì§€ì •"
]
ACTION_PAT = (
    r"(?P<obj>[ê°€-í£a-zA-Z0-9Â·\(\)\[\]\/\-\s]{2,}?)\s*"
    r"(?P<verb>" + "|".join(ACTION_VERBS) + r"|ì‹¤ì‹œ|ìš´ì˜|ê´€ë¦¬)\b"
    r"|(?P<obj2>[ê°€-í£a-zA-Z0-9Â·\(\)\[\]\/\-\s]{2,}?)\s*(ì„|ë¥¼)\s*"
    r"(?P<verb2>" + "|".join(ACTION_VERBS) + r"|ì‹¤ì‹œ|ìš´ì˜|ê´€ë¦¬)\b"
)

def drop_label_token(t: str) -> bool:
    if t in STOP_TERMS: return True
    for pat in LABEL_DROP_PAT:
        if re.match(pat, t): return True
    if t in {"ì†Œì¬","ì†Œì¬ì§€","ì§€ì—­","ì¥ì†Œ","ë²„ìŠ¤","ì˜ì—…ì†Œ","ì—…ì²´","ìë£Œ","í‚¤","ë©”ì„¸ì§€","ëª…"}:
        return True
    return False

def top_terms_for_label(text: str, k: int = 3) -> List[str]:
    doc_cnt = Counter([t for t in tokens(text) if not drop_label_token(t)])
    # ìœ„í—˜ìœ í˜•ì„ ìš°ì„  ë°˜ì˜
    bonus = Counter()
    for t in list(doc_cnt.keys()):
        if t in RISK_KEYWORDS:
            bonus[RISK_KEYWORDS[t]] += doc_cnt[t]
    doc_cnt += bonus
    kb = st.session_state.kb_terms
    if kb:
        for t, c in kb.items():
            if not drop_label_token(t):
                doc_cnt[t] += 0.2 * c
    if not doc_cnt: return ["ì•ˆì „ë³´ê±´", "êµìœ¡"]
    commons = {"ì•ˆì „","êµìœ¡","ì‘ì—…","í˜„ì¥","ì˜ˆë°©","ì¡°ì¹˜","í™•ì¸","ê´€ë¦¬","ì ê²€","ê°€ì´ë“œ","ì§€ì¹¨"}
    cand = [(t, doc_cnt[t]) for t in doc_cnt if t not in commons and len(t) >= 2]
    if not cand: cand = list(doc_cnt.items())
    cand.sort(key=lambda x: x[1], reverse=True)
    return [t for t, _ in cand[:k]]

def dynamic_topic_label(text: str) -> str:
    terms = top_terms_for_label(text, k=3)
    # 'ë¹„ê³„'ì™€ 'ì¶”ë½' ê°™ì´ ë‚˜ì˜¤ë©´ 'ë¹„ê³„ ì¶”ë½ ì¬í•´ì˜ˆë°©' ì‹ìœ¼ë¡œ ë³´ê°•
    risks = [RISK_KEYWORDS.get(t, t) for t in terms if t in RISK_KEYWORDS or t in RISK_KEYWORDS.values()]
    extra = [t for t in terms if t not in risks]
    label_core = " ".join(sorted(set(risks), key=risks.index)) or "ì•ˆì „ë³´ê±´"
    tail = " ".join(extra[:1])  # ë„ˆë¬´ ê¸¸ì–´ì§€ì§€ ì•Šê²Œ 1ê°œë§Œ
    label = (label_core + (" " + tail if tail else "")).strip()
    if "ì˜ˆë°©" not in label:
        label += " ì¬í•´ì˜ˆë°©"
    return label

def soften(s: str) -> str:
    s = s.replace("í•˜ì—¬ì•¼", "í•´ì•¼ í•©ë‹ˆë‹¤").replace("í•œë‹¤", "í•©ë‹ˆë‹¤").replace("í•œë‹¤.", "í•©ë‹ˆë‹¤.")
    s = s.replace("ë°”ëë‹ˆë‹¤", "í•´ì£¼ì„¸ìš”").replace("í™•ì¸ ë°”ëŒ", "í™•ì¸í•´ì£¼ì„¸ìš”")
    s = s.replace("ê¸ˆì§€í•œë‹¤", "ê¸ˆì§€í•©ë‹ˆë‹¤").replace("í•„ìš”í•˜ë‹¤", "í•„ìš”í•©ë‹ˆë‹¤")
    s = re.sub(r"^\(([^)]+)\)\s*", "", s)
    for pat in META_PATTERNS:
        s = re.sub(pat, "", s).strip()
    s = re.sub(BULLET_PREFIX, "", s).strip(" -â€¢â—\t")
    return s

def is_accident_sentence(s: str) -> bool:
    if any(w in s for w in ["ì˜ˆë°©", "ëŒ€ì±…", "ì§€ì¹¨", "ìˆ˜ì¹™"]):
        return False
    return bool(re.search(DATE_PAT, s) or re.search(r"(ì‚¬ë§|ì‚¬ìƒ|ì‚¬ê³ |ì¤‘ë…|í™”ì¬|ë¶•ê´´|ì§ˆì‹|ì¶”ë½|ê¹”ë¦¼|ë¶€ë”ªí˜|ë¬´ë„ˆì§|ë‚™í•˜)", s))

def is_prevention_sentence(s: str) -> bool:
    return any(w in s for w in ["ì˜ˆë°©", "ëŒ€ì±…", "ì§€ì¹¨", "ìˆ˜ì¹™", "ì•ˆì „ì¡°ì¹˜"])

def is_risk_sentence(s: str) -> bool:
    return any(w in s for w in ["ìœ„í—˜", "ìš”ì¸", "ì›ì¸", "ì¦ìƒ", "ê²°ë¹™", "ê°•í’", "í­ì—¼", "ë¯¸ì„¸ë¨¼ì§€", "íšŒì „ì²´", "ë¹„ì‚°", "ë§ë¦¼", "ì¶”ë½", "ë‚™í•˜", "í˜‘ì°©"])

def naturalize_case_sentence(s: str) -> str:
    s = soften(s)
    # <ì‚¬ë§ 1ëª…> ë“± ìˆ˜ì¹˜ í‘œí˜„ì„ ìì—° ë¬¸ì¥ìœ¼ë¡œ
    death = re.search(r"ì‚¬ë§\s*(\d+)\s*ëª…", s)
    inj = re.search(r"ì‚¬ìƒ\s*(\d+)\s*ëª…", s)
    unconscious = re.search(r"ì˜ì‹ë¶ˆëª…", s)
    info = []
    if death: info.append(f"ê·¼ë¡œì {death.group(1)}ëª… ì‚¬ë§")
    if inj and not death: info.append(f"{inj.group(1)}ëª… ì‚¬ìƒ")
    if unconscious: info.append("ì˜ì‹ë¶ˆëª… ë°œìƒ")
    # ë‚ ì§œ ì¶”ì¶œ/ë³´ì •
    m = re.search(DATE_PAT, s)
    date_txt = ""
    if m:
        y, mo, d = m.groups()
        y = int(str(y).replace("â€™","").replace("'",""))
        y = 2000 + y if y < 100 else y
        date_txt = f"{int(y)}ë…„ {int(mo)}ì›” {int(d)}ì¼, "
        s = s.replace(m.group(0), "").strip()
    # ì¥ì†Œ/ì‘ì—… ë‚´ìš© ë‹¨ì„œë“¤ ì •ë¦¬
    s = s.strip(" ,.-")
    # ì´ë¯¸ ì‚¬ê³ /ì‚¬ë§ ë‹¨ì–´ê°€ ìˆìœ¼ë©´ ê³¼ë„í•œ ìë™ë¶™ì„ ê¸ˆì§€
    if not re.search(r"(ì‚¬ë§|ì‚¬ìƒ|ì‚¬ê³ |ì¤‘ë…|ë¶•ê´´|ì§ˆì‹|ì¶”ë½|ê¹”ë¦¼|ë¶€ë”ªí˜|ë¬´ë„ˆì§|ë‚™í•˜)", s):
        if not re.search(r"(ë‹¤\.|ì…ë‹ˆë‹¤\.|í–ˆìŠµë‹ˆë‹¤\.)$", s):
            s = s.rstrip(" .") + " ì‚¬ê³ ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
    tail = ""
    if info:
        tail = " " + (", ".join(info)) + "í–ˆìŠµë‹ˆë‹¤." if not s.endswith("ìŠµë‹ˆë‹¤.") else ""
    return (date_txt + s + tail).strip()

def to_action_sentence(s: str) -> str:
    s2 = soften(s)
    # 'ì— ë”°ë¥¸' êµ¬ì¡° êµì •
    s2 = re.sub(r"\s*ì—\s*ë”°ë¥¸\s*", " ì‹œ ", s2)
    s2 = re.sub(r"\s*ì—\s*ë”°ë¼\s*", " ì‹œ ", s2)
    # ê³„íšì„œ/ì§€íœ˜ì/ë°œíŒ/ë‚œê°„/ë°©í˜¸ë§ ë“± í…œí”Œë¦¿ ê°•í™”
    if re.search(r"(ì‘ì—…ê³„íšì„œ|ê³„íšì„œ)\s*(ì‘ì„±|ìˆ˜ë¦½)?", s2):
        return "ì‘ì—… ì „ ì‘ì—…ê³„íšì„œë¥¼ ì‘ì„±í•˜ê³  ì‘ì—…ì§€íœ˜ìë¥¼ ì§€ì •í•©ë‹ˆë‹¤."
    if re.search(r"(ë°œíŒ|ì‘ì—…ë°œíŒ)", s2) and re.search(r"(ì„¤ì¹˜|í™•ì¸|ì ê²€)", s2):
        return "ì‘ì—…ë°œíŒì„ ê²¬ê³ í•˜ê²Œ ì„¤ì¹˜í•˜ê³  ìƒíƒœë¥¼ ì ê²€í•©ë‹ˆë‹¤."
    if re.search(r"(ë‚œê°„|ì•ˆì „ë‚œê°„)", s2):
        return "ì¶”ë½ ìœ„í—˜ êµ¬ê°„ì— ì•ˆì „ë‚œê°„ì„ ì„¤ì¹˜í•©ë‹ˆë‹¤."
    if re.search(r"(ë°©í˜¸ë§|ì¶”ë½ë°©í˜¸ë§|ì•ˆì „ë§)", s2):
        return "ì‘ì—… í•˜ë¶€ì— ì¶”ë½ë°©í˜¸ë§ì„ ì„¤ì¹˜í•©ë‹ˆë‹¤."
    if re.search(r"(ì•ˆì „ëŒ€|ë¼ì´í”„ë¼ì¸|ë²¨íŠ¸)", s2):
        return "ì•ˆì „ëŒ€ë¥¼ ì•ˆì „í•œ ì§€ì§€ì ì— ì—°ê²°í•˜ê³  ë¼ì´í”„ë¼ì¸ì„ ì‚¬ìš©í•©ë‹ˆë‹¤."
    if re.search(r"(ê°œì¸ë³´í˜¸êµ¬|PPE|ì•ˆì „ëª¨|ë³´í˜¸ì•ˆê²½|ë³´í˜¸ì¥ê°‘|ì•ˆì „í™”)", s2):
        return "ì•ˆì „ëª¨Â·ë³´í˜¸ì•ˆê²½Â·ì•ˆì „í™” ë“± ê°œì¸ë³´í˜¸êµ¬ë¥¼ ì˜¬ë°”ë¥´ê²Œ ì°©ìš©í•©ë‹ˆë‹¤."
    if re.search(r"(ì¶œì…í†µì œ|ìœ„í—˜êµ¬ì—­|ê°ì‹œì|ìœ ë„ì›)", s2):
        return "ìœ„í—˜êµ¬ì—­ì„ ì„¤ì •í•˜ê³  ì¶œì…ì„ í†µì œí•˜ë©° ê°ì‹œìë¥¼ ë°°ì¹˜í•©ë‹ˆë‹¤."

    m = re.search(ACTION_PAT, s2)
    if not m:
        return s2 if s2.endswith(("ë‹ˆë‹¤.", "í•©ë‹ˆë‹¤.", "ë‹¤.")) else (s2.rstrip(" .") + " í•©ë‹ˆë‹¤.")
    obj = (m.group("obj") or m.group("obj2") or "").strip()
    verb = (m.group("verb") or m.group("verb2") or "ì‹¤ì‹œ").strip()
    prefix = "ë°˜ë“œì‹œ " if "ì„¤ì¹˜" in verb else ("ì‘ì—… ì „ " if verb in ("í™•ì¸","ì ê²€","ì¸¡ì •","ê¸°ë¡","ì‘ì„±","ì§€ì •") else "")
    if obj and not re.search(r"(ì„|ë¥¼|ì—|ì—ì„œ|ê³¼|ì™€|ì˜)$", obj):
        obj += "ë¥¼"
    core = f"{prefix}{obj} {verb}".strip()
    core = re.sub(r"\s+", " ", core)
    return (core + "í•©ë‹ˆë‹¤.").replace("  ", " ")

def classify_sentence(s: str) -> str:
    if is_accident_sentence(s): return "case"
    if re.search(ACTION_PAT, s) or is_prevention_sentence(s): return "action"
    if is_risk_sentence(s): return "risk"
    if "?" in s or "í™•ì¸" in s or "ì ê²€" in s: return "question"
    return "other"

# ==========================================================
# ì„¸ì…˜ KB ëˆ„ì /í™œìš©
# ==========================================================
def kb_ingest_text(text: str) -> None:
    if not (text or "").strip(): return
    sents = preprocess_text_to_sentences(text)
    for s in sents:
        for t in tokens(s):
            if len(t) >= 2:
                st.session_state.kb_terms[t] += 1
    for s in sents:
        if re.search(ACTION_PAT, s) or is_prevention_sentence(s):
            cand = to_action_sentence(s)
            if 2 <= len(cand) <= 160:
                st.session_state.kb_actions.append(cand)
    for s in sents:
        if "?" in s or "í™•ì¸" in s or "ì ê²€" in s:
            q = soften(s if s.endswith("?") else s + " ë§ìŠµë‹ˆê¹Œ?")
            if 2 <= len(q) <= 140:
                st.session_state.kb_questions.append(q)

def kb_prune() -> None:
    def dedup_keep_order(lst: List[str]) -> List[str]:
        seen, out = set(), []
        for x in lst:
            k = re.sub(r"\s+", "", x)
            if k not in seen:
                seen.add(k); out.append(x)
        return out
    st.session_state.kb_actions = dedup_keep_order(st.session_state.kb_actions)[:700]
    st.session_state.kb_questions = dedup_keep_order(st.session_state.kb_questions)[:400]
    st.session_state.kb_terms = Counter(dict(st.session_state.kb_terms.most_common(1500)))

def kb_match_candidates(cands: List[str], base_text: str, limit: int) -> List[str]:
    bt = set(tokens(base_text))
    scored: List[Tuple[float, str]] = []
    for c in cands:
        ct = set(tokens(c))
        j = len(bt & ct) / (len(bt | ct) + 1e-8)
        if j > 0:
            scored.append((j, c))
    scored.sort(key=lambda x: x[0], reverse=True)
    return [c for _, c in scored[:limit]]

# ==========================================================
# ëŒ€ë³¸ ìƒì„±(ìì—°ìŠ¤ëŸ¬ìš´ êµìœ¡ëŒ€ë³¸)
# ==========================================================
def make_structured_script(text: str, max_points: int = 6) -> str:
    topic_label = dynamic_topic_label(text)
    core = [soften(s) for s in ai_extract_summary(text, max_points)]
    if not core:
        return "ë³¸ë¬¸ì´ ì¶©ë¶„í•˜ì§€ ì•Šì•„ ëŒ€ë³¸ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

    case, risk, act, ask = [], [], [], []
    for s in core:
        c = classify_sentence(s)
        if c == "case":
            case.append(naturalize_case_sentence(s))
        elif c == "action":
            act.append(to_action_sentence(s))
        elif c == "risk":
            risk.append(soften(s))
        elif c == "question":
            ask.append(soften(s if s.endswith("?") else s + " ë§ìŠµë‹ˆê¹Œ?"))

    # ë¶€ì¡±ë¶„ ë³´ê°•(KB)
    if len(act) < 5 and st.session_state.kb_actions:
        act += kb_match_candidates(st.session_state.kb_actions, text, 5 - len(act))
    act = act[:5]

    if not ask and st.session_state.kb_questions:
        ask = kb_match_candidates(st.session_state.kb_questions, text, 3)
    if not ask:
        ask = ["í•„ìš”í•œ ì•ˆì „ì¡°ì¹˜ê°€ ì˜¤ëŠ˜ ì‘ì—… ë²”ìœ„ì— ë§ê²Œ ì¤€ë¹„ë˜ì–´ ìˆìŠµë‹ˆê¹Œ?"]

    lines = []
    lines.append(f"ğŸ¦º TBM êµìœ¡ëŒ€ë³¸ â€“ {topic_label}\n")
    lines.append("â— ë„ì…")
    lines.append(f"ì˜¤ëŠ˜ì€ ìµœê·¼ ë°œìƒí•œ '{topic_label.replace(' ì¬í•´ì˜ˆë°©','')}' ì‚¬ë¡€ë¥¼ í†µí•´, ìš°ë¦¬ í˜„ì¥ì—ì„œ ê°™ì€ ì‚¬ê³ ë¥¼ ì˜ˆë°©í•˜ê¸° ìœ„í•œ ì•ˆì „ì¡°ì¹˜ë¥¼ í•¨ê»˜ ì‚´í´ë³´ê² ìŠµë‹ˆë‹¤.\n")

    if case:
        lines.append("â— ì‚¬ê³  ì‚¬ë¡€")
        for c in case: lines.append(f"- {c}")
        lines.append("")

    if risk:
        lines.append("â— ì£¼ìš” ìœ„í—˜ìš”ì¸")
        for r in risk: lines.append(f"- {r}")
        lines.append("")

    if act:
        lines.append("â— ì˜ˆë°©ì¡°ì¹˜ / ì‹¤ì²œ ìˆ˜ì¹™")
        for i, a in enumerate(act, 1): lines.append(f"{i}ï¸âƒ£ {a}")
        lines.append("")

    if ask:
        lines.append("â— í˜„ì¥ ì ê²€ ì§ˆë¬¸")
        for q in ask: lines.append(f"- {q}")
        lines.append("")

    lines.append("â— ë§ˆë¬´ë¦¬ ë‹¹ë¶€")
    lines.append("ì˜¤ëŠ˜ ì‘ì—… ì „, ê° ê³µì •ë³„ ìœ„í—˜ìš”ì¸ì„ ë‹¤ì‹œ í•œ ë²ˆ ì ê²€í•˜ê³  í•„ìš”í•œ ë³´í˜¸êµ¬ì™€ ì•ˆì „ì¡°ì¹˜ë¥¼ ë°˜ë“œì‹œ ì¤€ë¹„í•©ì‹œë‹¤.")
    lines.append("â— êµ¬í˜¸")
    lines.append("â€œí•œ ë²ˆ ë” í™•ì¸! í•œ ë²ˆ ë” ì ê²€!â€")

    return "\n".join(lines)

# ==========================================================
# DOCX ë‚´ë³´ë‚´ê¸° (XML ì•ˆì „í•„í„°)
# ==========================================================
_XML_FORBIDDEN = r"[\x00-\x08\x0B\x0C\x0E-\x1F\uD800-\uDFFF\uFFFE\uFFFF]"
def _xml_safe(s: str) -> str:
    if not isinstance(s, str): s = "" if s is None else str(s)
    return rxx.sub(_XML_FORBIDDEN, "", s)

def to_docx_bytes(script: str) -> bytes:
    doc = Document()
    try:
        style = doc.styles["Normal"]; style.font.name = "Malgun Gothic"; style.font.size = Pt(11)
    except Exception: pass
    for raw in script.split("\n"):
        line = _xml_safe(raw)
        p = doc.add_paragraph(line)
        for run in p.runs:
            try:
                run.font.name = "Malgun Gothic"; run.font.size = Pt(11)
            except Exception: pass
    bio = io.BytesIO(); doc.save(bio); bio.seek(0); return bio.read()

# ==========================================================
# UI (ê¸°ì¡´ ìœ ì§€)
# ==========================================================
with st.sidebar:
    st.header("â„¹ï¸ ì†Œê°œ / ì‚¬ìš©ë²•")
    st.markdown("""
**AI íŒŒì´í”„ë¼ì¸(ì™„ì „ ë¬´ë£Œ)**  
- ì „ì²˜ë¦¬ â†’ TextRank+MMR ìš”ì•½(ì„¸ì…˜ KB ê°€ì¤‘) â†’ ë°ì´í„° ê¸°ë°˜ ë¦¬ë¼ì´íŒ…  
- PDF/ZIP/í…ìŠ¤íŠ¸ ì—…ë¡œë“œ ì‹œ ì¦‰ì‹œ ëˆ„ì  í•™ìŠµ(ìš©ì–´/í–‰ë™/ì§ˆë¬¸).
""")

st.title("ğŸ¦º OPS/í¬ìŠ¤í„°ë¥¼ êµìœ¡ ëŒ€ë³¸ìœ¼ë¡œ ìë™ ë³€í™˜ (ì™„ì „ ë¬´ë£Œ)")

def reset_all():
    st.session_state.pop("manual_text", None)
    st.session_state.pop("edited_text", None)
    st.session_state.pop("zip_choice", None)
    st.session_state.kb_terms = Counter()
    st.session_state.kb_actions = []
    st.session_state.kb_questions = []
    st.session_state.uploader_key += 1
    st.rerun()

col_top1, col_top2 = st.columns([4, 1])
with col_top2:
    st.button("ğŸ§¹ ì´ˆê¸°í™”", on_click=reset_all, use_container_width=True)

st.markdown("""
**ì•ˆë‚´**  
- í…ìŠ¤íŠ¸ê°€ í¬í•¨ëœ PDF ë˜ëŠ” ë³¸ë¬¸ í…ìŠ¤íŠ¸ë¥¼ ê¶Œì¥í•©ë‹ˆë‹¤.  
- ì´ë¯¸ì§€/ìŠ¤ìº” PDFëŠ” í˜„ì¬ OCR ë¯¸ì§€ì›ì…ë‹ˆë‹¤.
""")

col1, col2 = st.columns([1, 1], gap="large")

# ---------- ì¢Œì¸¡ ì…ë ¥/ë¯¸ë¦¬ë³´ê¸° ----------
with col1:
    uploaded = st.file_uploader(
        "OPS ì—…ë¡œë“œ (PDF ë˜ëŠ” ZIP) â€¢ í…ìŠ¤íŠ¸ PDF ê¶Œì¥",
        type=["pdf", "zip"],
        key=f"uploader_{st.session_state.uploader_key}"
    )
    manual_text = st.text_area(
        "ë˜ëŠ” OPS í…ìŠ¤íŠ¸ ì§ì ‘ ë¶™ì—¬ë„£ê¸°",
        key="manual_text",
        height=220,
        placeholder="ì˜ˆ: í˜„ì¥ ì•ˆë‚´ë¬¸ ë˜ëŠ” OPS ë³¸ë¬¸ í…ìŠ¤íŠ¸â€¦"
    )

    extracted: str = ""
    zip_pdfs: Dict[str, bytes] = {}
    selected_zip_pdf: str = ""

    if uploaded is not None:
        fname = (uploaded.name or "").lower()
        if fname.endswith(".zip"):
            try:
                with zipfile.ZipFile(uploaded, "r") as zf:
                    for name in zf.namelist():
                        if name.lower().endswith(".pdf"):
                            data = zf.read(name)
                            zip_pdfs[name] = data
                            txt = read_pdf_text(data)
                            if txt.strip():
                                kb_ingest_text(txt)  # ZIP ì „ì²´ í•™ìŠµ
                kb_prune()
            except Exception as e:
                st.error(f"ZIP í•´ì œ ì˜¤ë¥˜: {e}")
            if zip_pdfs:
                selected_zip_pdf = st.selectbox("ZIP ë‚´ PDF ì„ íƒ", list(zip_pdfs.keys()), key="zip_choice")
                if selected_zip_pdf:
                    with st.spinner("ZIP ë‚´ë¶€ PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ ì¤‘..."):
                        extracted = read_pdf_text(zip_pdfs[selected_zip_pdf])
        elif fname.endswith(".pdf"):
            with st.spinner("PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ ì¤‘..."):
                data = uploaded.read()
                extracted = read_pdf_text(data)
                if extracted.strip():
                    kb_ingest_text(extracted); kb_prune()
                else:
                    st.warning("âš ï¸ PDFì—ì„œ ìœ íš¨í•œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        else:
            st.warning("ì§€ì›í•˜ì§€ ì•ŠëŠ” í˜•ì‹ì…ë‹ˆë‹¤. PDF ë˜ëŠ” ZIPì„ ì—…ë¡œë“œí•˜ì„¸ìš”.")

    pasted = (manual_text or "").strip()
    if pasted:
        kb_ingest_text(pasted); kb_prune()

    base_text = pasted or extracted.strip()
    st.markdown("**ì¶”ì¶œ/ì…ë ¥ í…ìŠ¤íŠ¸ ë¯¸ë¦¬ë³´ê¸°**")
    edited_text = st.text_area("í…ìŠ¤íŠ¸", value=base_text, height=240, key="edited_text")

# ---------- ìš°ì¸¡ ì˜µì…˜/ìƒì„±/ë‹¤ìš´ë¡œë“œ ----------
with col2:
    use_ai = st.toggle("ğŸ”¹ AI ìš”ì•½(TextRank+MMR) ì‚¬ìš©", value=True)
    tmpl_choice = st.selectbox("ğŸ§© í…œí”Œë¦¿", ["ìë™ ì„ íƒ", "ì‚¬ê³ ì‚¬ë¡€í˜•", "ê°€ì´ë“œí˜•"])  # í‘œì‹œë§Œ ìœ ì§€
    gen_mode = st.selectbox("ğŸ§  ìƒì„± ëª¨ë“œ", ["TBM ê¸°ë³¸(í˜„í–‰)", "ìì—°ìŠ¤ëŸ¬ìš´ êµìœ¡ëŒ€ë³¸(ë¬´ë£Œ)"])
    max_points = st.slider("ìš”ì•½ ê°•ë„(í•µì‹¬ë¬¸ì¥ ê°œìˆ˜)", 3, 10, 6)

    if st.button("ğŸ› ï¸ ëŒ€ë³¸ ìƒì„±", type="primary", use_container_width=True):
        text_for_gen = (st.session_state.get("edited_text") or "").strip()
        if not text_for_gen:
            st.warning("í…ìŠ¤íŠ¸ê°€ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤. PDF/ZIP ì—…ë¡œë“œ ë˜ëŠ” í…ìŠ¤íŠ¸ ì…ë ¥ í›„ ì‹œë„í•˜ì„¸ìš”.")
        else:
            with st.spinner("ëŒ€ë³¸ ìƒì„± ì¤‘..."):
                if gen_mode == "ìì—°ìŠ¤ëŸ¬ìš´ êµìœ¡ëŒ€ë³¸(ë¬´ë£Œ)"):
                    script = make_structured_script(text_for_gen, max_points=max_points)
                    subtitle = "ìì—°ìŠ¤ëŸ¬ìš´ êµìœ¡ëŒ€ë³¸(ë¬´ë£Œ)"
                else:
                    sents = ai_extract_summary(text_for_gen, max_points if use_ai else 6)
                    # TBM ê¸°ë³¸ ëª¨ë“œë„ ë…¸ì´ì¦ˆ ì •ë¦¬/í†¤ ì™„í™”
                    sents = [soften(s) for s in sents if not re.match(r"(ë°°í¬ì²˜|ì£¼ì†Œ|í™ˆí˜ì´ì§€)", s)]
                    script = "\n".join([f"- {s}" for s in sents]) if sents else "í…ìŠ¤íŠ¸ì—ì„œ í•µì‹¬ ë¬¸ì¥ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."
                    subtitle = "TBM ê¸°ë³¸(í˜„í–‰)"

            st.success(f"ëŒ€ë³¸ ìƒì„± ì™„ë£Œ! ({subtitle})")
            st.text_area("ëŒ€ë³¸ ë¯¸ë¦¬ë³´ê¸°", value=script, height=420)
            c3, c4 = st.columns(2)
            with c3:
                st.download_button("â¬‡ï¸ TXT ë‹¤ìš´ë¡œë“œ", data=_xml_safe(script).encode("utf-8"),
                                   file_name="tbm_script.txt", use_container_width=True)
            with c4:
                st.download_button("â¬‡ï¸ DOCX ë‹¤ìš´ë¡œë“œ", data=to_docx_bytes(script),
                                   file_name="tbm_script.docx", use_container_width=True)

st.caption("ì™„ì „ ë¬´ë£Œ. ì—…ë¡œë“œ/ë¶™ì—¬ë„£ê¸°ë§Œ í•´ë„ ëˆ„ì  í•™ìŠµ â†’ ìš”ì•½ ê°€ì¤‘/í–‰ë™/ì§ˆë¬¸ ë³´ê°•. ë™ì  ì£¼ì œ ë¼ë²¨. UI ë³€ê²½ ì—†ìŒ.")
