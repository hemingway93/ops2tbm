# ==========================================================
# OPS2TBM â€” OPS/í¬ìŠ¤í„° â†’ TBM êµìœ¡ ëŒ€ë³¸ ìë™ ë³€í™˜ (ì™„ì „ ë¬´ë£Œ)
# v2025-11-04:
#  - [â‘ ] ì‚¬ê³  ë¬¸ì¥ ì¢…ê²° ë³´ê°•: "ì‚¬ë§/ì§ˆì‹/ì¶”ë½..."ìœ¼ë¡œ ëë‚˜ë„ ìì—°ìŠ¤ëŸ¬ìš´ ì¢…ê²°("í–ˆìŠµë‹ˆë‹¤.") ì¶”ê°€
#  - [â‘¡] ì§ˆë¬¸/í–‰ë™ í›„ë³´ ë§¤ì¹­ ì •ë°€í™”: ìœ ì‚¬ë„ ì„ê³„ê°’â†‘ + í˜„ì¬ í…ìŠ¤íŠ¸ ë„ë©”ì¸ í‚¤ì›Œë“œ ê¸°ë°˜ í•„í„°
#  - [â‘¢][â‘£] ì˜ë¯¸ ì—†ëŠ” ì§§ì€ ë¬¸ì¥(ì„œìˆ ì–´ë§Œ) ì œê±°: 10ê¸€ì ë¯¸ë§Œ/ë™ì‚¬ ë‹¨ë…í˜• í•„í„°
#  - UI/ë ˆì´ì•„ì›ƒ ë³€ê²½ ì—†ìŒ
# ==========================================================

import io
import zipfile
import re
from collections import Counter
from typing import List, Dict, Tuple

import numpy as np
import regex as rxx
import streamlit as st
from docx import Document
from docx.shared import Pt
from pdfminer.high_level import extract_text as pdf_extract_text
import pypdfium2 as pdfium

st.set_page_config(page_title="OPS2TBM", page_icon="ğŸ¦º", layout="wide")

# -------------------- ì‹œë“œ KB --------------------
SEED_RISK_MAP = {
    "ì¤‘ë…": "ì¤‘ë…", "ë–¨ì–´ì§": "ë–¨ì–´ì§", "ë¼ì„": "ë¼ì„", "ì§ˆì‹": "ì§ˆì‹", "í™”ì¬": "í™”ì¬", "ê¹”ë¦¼": "ê¹”ë¦¼",
    "ë§ìŒ": "ë§ìŒ", "ê°ì „": "ê°ì „", "ì§€ë¶•": "ì§€ë¶•ì‘ì—…", "ì˜ˆì´ˆ": "ì˜ˆì´ˆ", "í­ë°œ": "í­ë°œ", "ì²œê³µê¸°": "ì²œê³µ",
    "ì„ ë°˜": "ì ˆì‚­", "ì»¨ë² ì´ì–´": "í˜‘ì°©", "ë¶€ë”ªí˜": "ì¶©ëŒ", "ë¯¸ì„¸ë¨¼ì§€": "ë¯¸ì„¸ë¨¼ì§€", "í¬ë ˆì¸": "ì–‘ì¤‘",
    "ë¬´ë„ˆì§": "ë¶•ê´´", "ë¹„ê³„": "ë¹„ê³„", "ì¶”ë½": "ì¶”ë½", "í­ì—¼": "í­ì—¼", "ë²Œëª©": "ë²Œëª©",
    "ë‚™í•˜": "ë‚™í•˜", "ë¶•ê´´": "ë¶•ê´´", "ê°±í¼": "ë¹„ê³„", "ë°œíŒ": "ë¹„ê³„"
}
SEED_ACTIONS = [
    "ë°€íê³µê°„ì‘ì—… êµìœ¡ ë° í›ˆë ¨ ì‹¤ì‹œ", "ì¶œì… ì „ ì¶©ë¶„í•œ í™˜ê¸° ì‹¤ì‹œ", "ì‘ì—… ì „ ê°€ìŠ¤ë†ë„ ì¸¡ì • ë° ê¸°ë¡",
    "ì‘ì—… ìƒí™© ê°ì‹œì ë°°ì¹˜", "ì¶œì…Â·í‡´ì¥ ì¸ì› ì ê²€", "ë³´í˜¸ì¥êµ¬ ì—†ì´ êµ¬ì¡° ê¸ˆì§€",
    "MSDS í™•ì¸ ë° ìœ í•´ì„± êµìœ¡ ì‹¤ì‹œ", "êµ­ì†Œë°°ê¸°ì¥ì¹˜ ì„¤ì¹˜Â·ê°€ë™", "í™˜ê¸°ê°€ ë¶ˆì¶©ë¶„í•œ ê³µê°„ì—ì„œëŠ” ê¸‰ê¸°/ë°°ê¸°íŒ¬ ì‚¬ìš©",
    "ìœ ê¸°í™”í•©ë¬¼ ì·¨ê¸‰ ì‹œ ë°©ë…ë§ˆìŠ¤í¬(ê°ˆìƒ‰ ì •í™”í†µ) ì°©ìš©", "ì†¡ê¸°ë§ˆìŠ¤í¬Â·ê³µê¸°í˜¸í¡ê¸° ì ì • ì‚¬ìš©",
    "ì˜ˆì´ˆê¸° ì •ì§€ í›„ ì´ë¬¼ì§ˆ ì œê±°Â·ì ê²€", "ì˜ˆì´ˆÂ·ë²Œëª© ì‘ì—… ì•ˆì „ê±°ë¦¬ ìœ ì§€ ë° ëŒ€í”¼ë¡œ í™•ë³´",
    "ì‘ì—…ë°œíŒ ê²¬ê³ íˆ ì„¤ì¹˜ ë° ìƒíƒœ ì ê²€", "ê°œêµ¬ë¶€Â·ê°œêµ¬ì°½ ì¶”ë½ ìœ„í—˜ êµ¬ê°„ ì•ˆì „ë‚œê°„ ì„¤ì¹˜",
    "ì•ˆì „ëŒ€ ì§€ì§€ì  ì—°ê²° ë° ë¼ì´í”„ë¼ì¸ ì‚¬ìš©", "ìœ„í—˜êµ¬ì—­ ì„¤ì •Â·ì¶œì…í†µì œÂ·ê°ì‹œì ë°°ì¹˜",
    "ì–‘ì¤‘ ê³„íš ìˆ˜ë¦½ ë° ì‹ í˜¸ìˆ˜ ì§€ì •Â·í†µì‹  ìœ ì§€", "íšŒì „ì²´Â·ë¬¼ë¦¼ì  ë°©í˜¸ì¥ì¹˜ ì„¤ì¹˜ ë° ì ê²€",
    "ì‘ì—… ì „ ì‘ì—…ê³„íšì„œ ì‘ì„± ë° ì‘ì—…ì§€íœ˜ì ì§€ì •", "ê°œì¸ë³´í˜¸êµ¬ ì°©ìš©(ì•ˆì „ëª¨Â·ë³´í˜¸ì•ˆê²½Â·ì•ˆì „í™” ë“±)",
    "í™”ê¸°ì‘ì—… í—ˆê°€ ë° ì•ˆì „ì ê²€", "ì •ë¹„Â·ì²­ì†ŒÂ·ì ê²€ ì‹œ ê¸°ê³„ ì „ì› ì°¨ë‹¨"
]
SEED_QUESTIONS = [
    "ì‘ì—… ì „ ì‘ì—…ê³„íšì„œì™€ ìœ„í—˜ì„±í‰ê°€ë¥¼ ê²€í† í–ˆìŠµë‹ˆê¹Œ?",
    "ê°œêµ¬ë¶€Â·ê°œêµ¬ì°½ ë“± ì¶”ë½ ìœ„í—˜ êµ¬ê°„ì— ì•ˆì „ë‚œê°„ì„ ì„¤ì¹˜í–ˆìŠµë‹ˆê¹Œ?",
    "ì‘ì—…ë°œíŒì´ ê²¬ê³ í•˜ê²Œ ì„¤ì¹˜ë˜ê³  ìƒíƒœê°€ ì–‘í˜¸í•©ë‹ˆê¹Œ?",
    "ì•ˆì „ëŒ€ ì—°ê²°ì ê³¼ ë¼ì´í”„ë¼ì¸ì´ í™•ë³´ë˜ì—ˆìŠµë‹ˆê¹Œ?",
    "êµ­ì†Œë°°ê¸°ì¥ì¹˜ë¥¼ ê°€ë™í•˜ê³  í™˜ê¸° ê²½ë¡œê°€ í™•ë³´ë˜ì—ˆìŠµë‹ˆê¹Œ?",
    "í˜¸í¡ë³´í˜¸êµ¬ê°€ ì‘ì—…ì— ì í•©í•˜ë©° ê´€ë¦¬ê°€ ë˜ê³  ìˆìŠµë‹ˆê¹Œ?",
    "ë°€íê³µê°„ ì¶œì…Â·í‡´ì¥ ì¸ì› ì ê²€ì´ ì´ë£¨ì–´ì§€ê³  ìˆìŠµë‹ˆê¹Œ?",
    "ì˜ˆì´ˆÂ·ë²Œëª© ì‘ì—… ì‹œ ì•ˆì „ê±°ë¦¬ì™€ ëŒ€í”¼ë¡œë¥¼ í™•ë³´í–ˆìŠµë‹ˆê¹Œ?",
    "ì–‘ì¤‘ ì‘ì—…ì— ì‹ í˜¸ìˆ˜ ì§€ì • ë° í†µì‹ ì²´ê³„ê°€ ë§ˆë ¨ë˜ì—ˆìŠµë‹ˆê¹Œ?",
    "íšŒì „ì²´Â·ë¬¼ë¦¼ì  ë°©í˜¸ì¥ì¹˜ê°€ ì •ìƒ ë™ì‘í•©ë‹ˆê¹Œ?"
]

# -------------------- ì„¸ì…˜ ìƒíƒœ --------------------
def _init_once():
    ss = st.session_state
    ss.setdefault("uploader_key", 0)
    ss.setdefault("kb_terms", Counter())
    ss.setdefault("kb_actions", [])
    ss.setdefault("kb_questions", [])
    ss.setdefault("domain_toggle", False)
    ss.setdefault("seed_loaded", False)
    ss.setdefault("last_file_diag", {})
    ss.setdefault("last_extracted_cache", "")
_init_once()

# -------------------- ìœ í‹¸: í•œê¸€ ì¡°ì‚¬/ë„ì–´ì“°ê¸° --------------------
def _has_final_consonant(k: str) -> bool:
    if not k:
        return False
    ch = k[-1]
    base = ord('ê°€')
    code = ord(ch) - base
    if code < 0 or code > 11171:
        return False
    jong = code % 28
    return jong != 0

def add_obj_particle(noun: str) -> str:
    noun = noun.strip()
    if not noun:
        return noun
    particle = "ì„" if _has_final_consonant(noun[-1]) else "ë¥¼"
    return f"{noun}{particle}"

def tidy_korean_spaces(s: str) -> str:
    # ê°„ë‹¨í•œ ë„ì–´ì“°ê¸°/ë¬¸ì¥ë¶€í˜¸ ë³´ì •
    s = re.sub(r"\s+", " ", s)
    s = s.replace("ì „ì¶©ë¶„í•œ", "ì „ ì¶©ë¶„í•œ").replace("ì „ì¶©ë¶„íˆ", "ì „ ì¶©ë¶„íˆ")
    s = re.sub(r"\s([,.])", r"\1", s)
    return s.strip()

# -------------------- ì „ì²˜ë¦¬/íŒ¨í„´ --------------------
NOISE_PATTERNS = [
    r"^ì œ?\s?\d{4}\s?[-.]?\s?\d+\s?í˜¸$",
    r"^(ë™ì ˆê¸°\s*ì£¼ìš”ì‚¬ê³ |ì•ˆì „ì‘ì—…ë°©ë²•|ì½˜í…ì¸ \s*ë§í¬|ì±…ì\s*OPS|ìˆí¼\s*OPS)$",
    r"^(í¬ìŠ¤í„°|ì±…ì|ìŠ¤í‹°ì»¤|ì½˜í…ì¸  ë§í¬)$",
    r"^(ìŠ¤ë§ˆíŠ¸í°\s*APP|ì¤‘ëŒ€ì¬í•´\s*ì‚¬ì´ë Œ|ì‚°ì—…ì•ˆì „í¬í„¸|ê³ ìš©ë…¸ë™ë¶€)$",
    r"^https?://\S+$",
    r"^\(?\s*PowerPoint\s*í”„ë ˆì  í…Œì´ì…˜\s*\)?$",
    r"^ì•ˆì „ë³´ê±´ìë£Œì‹¤.*$",
    r"^ë°°í¬ì²˜\s+.*$",
    r"^í™ˆí˜ì´ì§€\s+.*$",
    r"^VR\s+.*$",
    r"^ë¦¬í”Œë¦¿\s+.*$",
    r"^ë™ì˜ìƒ\s+.*$",
    r"^APP\s+.*$",
    r".*ê²€ìƒ‰í•´\s*ë³´ì„¸ìš”.*$",
]
BULLET_PREFIX = r"^[\s\-\â€¢\â—\â–ª\â–¶\â–·\Â·\*\u25CF\u25A0\u25B6\u25C6\u2022\u00B7\u279C\u27A4\u25BA\u25AA\u25AB\u2611\u2713\u2714\u2716\u2794\u27A2]+"
DATE_PAT = r"([â€™']?\d{2,4})\.\s?(\d{1,2})\.\s?(\d{1,2})\.?"
META_PATTERNS = [
    r"<\s*\d+\s*ëª…\s*ì‚¬ë§\s*>", r"<\s*\d+\s*ëª…\s*ì‚¬ìƒ\s*>", r"<\s*\d+\s*ëª…\s*ì˜ì‹ë¶ˆëª…\s*>",
    r"<\s*ì‚¬ë§\s*\d+\s*ëª…\s*>", r"<\s*ì‚¬ìƒ\s*\d+\s*ëª…\s*>"
]
STOP_TERMS = set("""
ë° ë“± ê´€ë ¨ ì‚¬í•­ ë‚´ìš© ì˜ˆë°© ì•ˆì „ ì‘ì—… í˜„ì¥ êµìœ¡ ë°©ë²• ê¸°ì¤€ ì¡°ì¹˜
ì‹¤ì‹œ í™•ì¸ í•„ìš” ê²½ìš° ëŒ€ìƒ ì‚¬ìš© ê´€ë¦¬ ì ê²€ ì ìš© ì •ë„ ì£¼ì˜ ì¤‘ ì „ í›„
ì£¼ìš” ì‚¬ë¡€ ì•ˆì „ì‘ì—…ë°©ë²• í¬ìŠ¤í„° ë™ì˜ìƒ ë¦¬í”Œë¦¿ ê°€ì´ë“œ ìë£Œì‹¤ ê²€ìƒ‰
í‚¤ë©”ì„¸ì§€ êµìœ¡í˜ì‹ ì‹¤ ì•ˆì „ë³´ê±´ê³µë‹¨ ê³µë‹¨ ìë£Œ êµ¬ë… ì•ˆë‚´ ì—°ë½ ì°¸ê³  ì¶œì²˜
ì†Œì¬ ì†Œì¬ì§€ ìœ„ì¹˜ ì¥ì†Œ ì§€ì—­ ì‹œêµ°êµ¬ ì„œìš¸ ì¸ì²œ ë¶€ì‚° ëŒ€êµ¬ ëŒ€ì „ ê´‘ì£¼ ìš¸ì‚° ì„¸ì¢… ê²½ê¸°ë„ ì¶©ì²­ ì „ë¼ ê²½ìƒ ê°•ì› ì œì£¼
ëª… ê±´ í˜¸ í˜¸ì°¨ í˜¸ìˆ˜ í˜ì´ì§€ ìª½ ë¶€ë¡ ì°¸ê³  ê·¸ë¦¼ í‘œ ëª©ì°¨
ì•ˆì „ë³´ê±´ ops í‚¤ ë©”ì„¸ì§€ í‚¤ë©”ì„¸ì§€ ìë£Œ opsêµì•ˆ êµì•ˆ
""".split())
LABEL_DROP_PAT = [
    r"^\d+$", r"^\d{2,4}[-_]\d{1,}$", r"^\d{4}$", r"^(ì œ)?\d+í˜¸$", r"^(í˜¸|í˜¸ìˆ˜|í˜¸ì°¨)$",
    r"^(ì‚¬ì—…ì¥|ì—…ì²´|ì†Œì¬|ì†Œì¬ì§€|ì¥ì†Œ|ì§€ì—­)$", r"^\d+\s*(ëª…|ê±´)$"
]
RISK_KEYWORDS = dict(SEED_RISK_MAP)

def tokens(s: str) -> List[str]:
    return rxx.findall(r"[ê°€-í£a-z0-9]{2,}", s.lower())

def normalize_text(t: str) -> str:
    t = t.replace("\x0c", "\n")
    t = re.sub(r"[ \t]+\n", "\n", t)
    t = re.sub(r"\n{3,}", "\n\n", t)
    return t.strip()

def strip_noise_line(line: str) -> str:
    s = (line or "").strip()
    if not s:
        return ""
    s = re.sub(BULLET_PREFIX, "", s).strip()
    for pat in NOISE_PATTERNS:
        if re.match(pat, s, re.IGNORECASE):
            return ""
    s = re.sub(r"https?://\S+", "", s).strip()
    s = s.strip("â€¢â—â–ªâ–¶â–·Â·-â€”â€“")
    return s

def merge_broken_lines(lines: List[str]) -> List[str]:
    """ë¶ˆë¦¿/ê°œí–‰ íŒŒí¸ì„ ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë³‘í•©"""
    out, buf = [], ""
    for raw in lines:
        s = strip_noise_line(raw)
        if not s:
            continue
        s = s.lstrip("â†³").strip()
        if buf and (buf.endswith((":", "Â·", "â€¢", "â–ª", "â–¶", "â–·")) or re.match(r"^\(.*\)$", buf)):
            buf += " " + s
        else:
            if buf:
                out.append(buf)
            buf = s
    if buf:
        out.append(buf)
    return out

def combine_date_with_next(lines: List[str]) -> List[str]:
    """ë‚ ì§œ ì¤„ê³¼ ì‚¬ê³  ë‚´ìš© ì¤„ì„ í•©ì³ 'YYYYë…„ Mì›” Dì¼, ë‚´ìš©'ìœ¼ë¡œ ë§Œë“¦"""
    out = []
    i = 0
    while i < len(lines):
        cur = strip_noise_line(lines[i])
        if re.search(DATE_PAT, cur) and (i + 1) < len(lines):
            nxt = strip_noise_line(lines[i + 1])
            if re.search(r"(ì‚¬ë§|ì‚¬ìƒ|ì‚¬ê³ |ì¤‘ë…|í™”ì¬|ë¶•ê´´|ì§ˆì‹|ì¶”ë½|ê¹”ë¦¼|ë¶€ë”ªí˜|ë¬´ë„ˆì§|ë‚™í•˜)", nxt):
                m = re.search(DATE_PAT, cur)
                y, mo, d = m.groups()
                y = int(str(y).replace("â€™", "").replace("'", ""))
                y = 2000 + y if y < 100 else y
                out.append(f"{int(y)}ë…„ {int(mo)}ì›” {int(d)}ì¼, {nxt}")
                i += 2
                continue
        out.append(cur)
        i += 1
    return out

# -------- ì‚¬ê³  ë¸”ë¡ ë³‘í•© --------
CASE_JOIN_TRIG = ("ì“°ëŸ¬ì§€ì", "êµ¬ì¡°í•˜ë˜ ì¤‘", "ì°¨ë¡€ë¡œ", "ì´ì–´", "ì´í›„", "ë™ì‹œì—", "ê²°êµ­", "ê·¸ ê³¼ì •ì—ì„œ", "ì™¸ë¶€ì— ìˆë˜", "í˜„ì¥ì— ìˆë˜")
CASE_KEYWORDS = ("ì‚¬ë§", "ì‚¬ìƒ", "ì¤‘ë…", "ì¶”ë½", "ë¶•ê´´", "ë‚™í•˜", "ì§ˆì‹", "ë¼ì„", "ê¹”ë¦¼", "ë¶€ë”ªí˜", "ê°ì „", "í­ë°œ")

def stitch_case_blocks(sents: List[str]) -> List[str]:
    """ì—°ì†ëœ ì‚¬ê³  ì„œìˆ ì„ ì‹ í˜¸/í‚¤ì›Œë“œë¡œ ë³‘í•©í•´ ìì—°ìŠ¤ëŸ¬ìš´ í•œ ì¤„ë¡œ ë§Œë“ ë‹¤."""
    if not sents:
        return sents
    out = []
    i = 0
    while i < len(sents):
        cur = sents[i].strip()
        merged = cur
        j = i + 1
        merged_any = False
        while j < len(sents):
            nxt = sents[j].strip()
            cond_keyword = (any(k in cur for k in CASE_KEYWORDS) and any(k in nxt for k in CASE_KEYWORDS))
            cond_trigger = (any(t in nxt for t in CASE_JOIN_TRIG) or any(t in cur for t in CASE_JOIN_TRIG))
            if cond_keyword or cond_trigger:
                sep = ", " if not merged.endswith(("ë‹¤.", "ìŠµë‹ˆë‹¤.", "í–ˆë‹¤.", ".")) else " "
                merged = tidy_korean_spaces(merged.rstrip(" .") + sep + nxt.lstrip(" ,"))
                cur = merged
                j += 1
                merged_any = True
            else:
                break
        out.append(merged)
        i = j if merged_any else i + 1
    # ì¤‘ë³µ ì œê±°
    seen, dedup = set(), []
    for s in out:
        k = re.sub(r"\s+", "", s)
        if k not in seen:
            seen.add(k)
            dedup.append(s)
    return dedup

def preprocess_text_to_sentences(text: str) -> List[str]:
    text = normalize_text(text)
    raw_lines = [ln for ln in text.splitlines() if ln.strip()]
    lines = merge_broken_lines(raw_lines)
    lines = combine_date_with_next(lines)
    joined = "\n".join(lines)
    raw = rxx.split(r"(?<=[\.!\?]|ë‹¤\.)\s+|\n+", joined)
    sents = []
    for s in raw:
        s2 = strip_noise_line(s)
        if not s2:
            continue
        if re.search(r"(ì£¼ìš”ì‚¬ê³ |ì•ˆì „ì‘ì—…ë°©ë²•|ì½˜í…ì¸ ë§í¬|ì£¼ìš” ì‚¬ê³ ê°œìš”)$", s2):
            continue
        if len(s2) < 6:
            continue
        sents.append(s2)
    sents = stitch_case_blocks(sents)
    return sents

# -------------------- PDF ì²˜ë¦¬/ì§„ë‹¨ --------------------
def read_pdf_text_from_bytes(b: bytes, fname: str = "") -> str:
    """í…ìŠ¤íŠ¸í˜• PDF ìš°ì„  ì¶”ì¶œ, ì‹¤íŒ¨ ì‹œ ìŠ¤ìº” ê²½ê³ ë§Œ í‘œì‹œ(OCR ë¯¸ì§€ì›)."""
    try:
        with io.BytesIO(b) as bio:
            t = pdf_extract_text(bio) or ""
    except Exception:
        t = ""
    t = normalize_text(t)
    if len(t.strip()) < 10:
        try:
            with io.BytesIO(b) as bio:
                pdf = pdfium.PdfDocument(bio)
                if len(pdf) > 0 and not t.strip():
                    st.warning("âš ï¸ ì´ë¯¸ì§€/ìŠ¤ìº” PDFë¡œ ë³´ì…ë‹ˆë‹¤. í˜„ì¬ OCR ë¯¸ì§€ì›.")
        except Exception:
            pass
    st.session_state["last_file_diag"] = {
        "name": fname, "size_bytes": len(b), "extracted_chars": len(t),
        "note": "empty_or_scanned" if (len(t.strip()) < 10) else "ok"
    }
    return t

# -------------------- ìš”ì•½/ê°€ì¤‘ --------------------
def sentence_tfidf_vectors(sents: List[str], kb_boost: Dict[str, float] = None) -> Tuple[np.ndarray, List[str]]:
    toks = [tokens(s) for s in sents]
    vocab: Dict[str, int] = {}
    for ts in toks:
        for t in ts:
            if t not in vocab:
                vocab[t] = len(vocab)
    if not vocab:
        return np.zeros((len(sents), 0), dtype=np.float32), []
    M = np.zeros((len(sents), len(vocab)), dtype=np.float32)
    df = np.zeros((len(vocab),), dtype=np.float32)
    for i, ts in enumerate(toks):
        for t in ts:
            w = 1.0
            if kb_boost and t in kb_boost:
                w *= kb_boost[t]
            M[i, vocab[t]] += w
        for t in set(ts):
            df[vocab[t]] += 1.0
    N = float(len(sents))
    idf = np.log((N + 1.0) / (df + 1.0)) + 1.0
    M *= idf
    if kb_boost:
        for t, idx in vocab.items():
            if t in kb_boost:
                M[:, idx] *= (1.0 + 0.2 * kb_boost[t])
    M /= (np.linalg.norm(M, axis=1, keepdims=True) + 1e-8)
    return M, list(vocab.keys())

def cosim(X: np.ndarray) -> np.ndarray:
    if X.size == 0:
        return np.zeros((X.shape[0], X.shape[0]), dtype=np.float32)
    S = np.clip(X @ X.T, 0.0, 1.0)
    np.fill_diagonal(S, 0.0)
    return S

def textrank_scores(sents: List[str], X: np.ndarray, d: float = 0.85, max_iter: int = 60, tol: float = 1e-4) -> List[float]:
    n = len(sents)
    if n == 0:
        return []
    W = cosim(X)
    row = W.sum(axis=1, keepdims=True)
    P = np.divide(W, row, out=np.zeros_like(W), where=row > 0)
    r = np.ones((n, 1), dtype=np.float32) / n
    tel = np.ones((n, 1), dtype=np.float32) / n
    for _ in range(max_iter):
        r2 = d * (P.T @ r) + (1 - d) * tel
        if np.linalg.norm(r2 - r, 1) < tol:
            r = r2
            break
        r = r2
    return [float(v) for v in r.flatten()]

def mmr_select(sents: List[str], scores: List[float], X: np.ndarray, k: int, lam: float = 0.7) -> List[int]:
    S = cosim(X)
    sel: List[int] = []
    rem = set(range(len(sents)))
    while rem and len(sel) < k:
        best, val = None, -1e9
        for i in rem:
            rel = scores[i]
            div = max((S[i, j] for j in sel), default=0.0)
            sc = lam * rel - (1 - lam) * div
            if sc > val:
                val, best = sc, i
        sel.append(best)
        rem.remove(best)
    return sel

def ai_extract_summary(text: str, limit: int = 8) -> List[str]:
    """TextRank+MMR ìš”ì•½(ì„¸ì…˜ KB ê°€ì¤‘ì¹˜ ë°˜ì˜)."""
    sents = preprocess_text_to_sentences(text)
    if not sents:
        return []
    kb = st.session_state["kb_terms"]
    total = sum(kb.values()) or 1
    kb_boost = {t: 1.0 + (cnt / total) * 3.0 for t, cnt in kb.items()} if kb else None
    X, _ = sentence_tfidf_vectors(sents, kb_boost=kb_boost)
    scores = textrank_scores(sents, X)
    idx = mmr_select(sents, scores, X, limit, lam=0.7)
    return [sents[i] for i in idx]

# -------------------- í…œí”Œë¦¿/ë¶„ë¥˜/NLG --------------------
DOMAIN_TEMPLATES = [
    ({"ë¹„ê³„", "ë°œíŒ", "ê°±í¼", "ì¶”ë½"}, "ì‘ì—…ë°œíŒì„ ê²¬ê³ í•˜ê²Œ ì„¤ì¹˜í•˜ê³  ì•ˆì „ë‚œê°„ ë° ì¶”ë½ë°©í˜¸ë§ì„ í™•ë³´í•©ë‹ˆë‹¤."),
    ({"ì•ˆì „ë‚œê°„", "ë‚œê°„", "ê°œêµ¬ë¶€"}, "ê°œêµ¬ë¶€Â·ê°œêµ¬ì°½ ë“± ì¶”ë½ ìœ„í—˜ êµ¬ê°„ì— ì•ˆì „ë‚œê°„ì„ ì„¤ì¹˜í•©ë‹ˆë‹¤."),
    ({"MSDS", "êµ­ì†Œë°°ê¸°", "í™˜ê¸°"}, "ì·¨ê¸‰ ë¬¼ì§ˆì˜ MSDSë¥¼ í™•ì¸í•˜ê³  êµ­ì†Œë°°ê¸°ì¥ì¹˜ë¥¼ ê°€ë™í•˜ì—¬ ì¶©ë¶„íˆ í™˜ê¸°í•©ë‹ˆë‹¤."),
    ({"ì˜ˆì´ˆ", "ë²Œëª©", "ì˜ˆì´ˆê¸°"}, "ì˜ˆì´ˆÂ·ë²Œëª© ì‘ì—… ì‹œ ì‘ì—…ì ê°„ ì•ˆì „ê±°ë¦¬ë¥¼ ìœ ì§€í•˜ê³  ëŒ€í”¼ë¡œë¥¼ í™•ë³´í•©ë‹ˆë‹¤."),
    ({"í¬ë ˆì¸", "ì–‘ì¤‘"}, "ì–‘ì¤‘ ê³„íšì„ ìˆ˜ë¦½í•˜ê³  ì‹ í˜¸ìˆ˜ë¥¼ ì§€ì •í•˜ì—¬ í†µì‹ ì„ ìœ ì§€í•©ë‹ˆë‹¤."),
    ({"ì»¨ë² ì´ì–´", "í˜‘ì°©", "íšŒì „ì²´"}, "íšŒì „ì²´Â·ë¬¼ë¦¼ì  ì ‘ì´‰ì„ ë°©ì§€í•˜ë„ë¡ ë°©í˜¸ì¥ì¹˜ë¥¼ ì„¤ì¹˜í•˜ê³  ì ê²€í•©ë‹ˆë‹¤."),
]

def jaccard(a: set, b: set) -> float:
    return len(a & b) / (len(a | b) + 1e-8)

ACTION_VERBS = [
    "ì„¤ì¹˜", "ë°°ì¹˜", "ì°©ìš©", "ì ê²€", "í™•ì¸", "ì¸¡ì •", "ê¸°ë¡", "í‘œì‹œ", "ì œê³µ", "ë¹„ì¹˜", "ë³´ê³ ", "ì‹ ê³ ",
    "êµìœ¡", "ì£¼ì§€", "ì¤‘ì§€", "í†µì œ", "íœ´ì‹", "í™˜ê¸°", "ì°¨ë‹¨", "êµëŒ€", "ë°°ì œ", "ë°°ë ¤", "ê°€ë™", "ì¤€ìˆ˜",
    "ìš´ì˜", "ìœ ì§€", "êµì²´", "ì •ë¹„", "ì²­ì†Œ", "ê³ ì •", "ê²©ë¦¬", "ë³´í˜¸", "ë³´ìˆ˜", "ì‘ì„±", "ì§€ì •"
]
ACTION_PAT = (
    r"(?P<obj>[ê°€-í£a-zA-Z0-9Â·\(\)\[\]\/\-\s]{2,}?)\s*(?P<verb>" + "|".join(ACTION_VERBS) + r"|ì‹¤ì‹œ|ìš´ì˜|ê´€ë¦¬)\b"
    r"|(?P<obj2>[ê°€-í£a-zA-Z0-9Â·\(\)\[\]\/\-\s]{2,}?)\s*(ì„|ë¥¼)\s*(?P<verb2>" + "|".join(ACTION_VERBS) + r"|ì‹¤ì‹œ|ìš´ì˜|ê´€ë¦¬)\b"
)

# ---------- (â‘¢â‘£) ì˜ë¯¸ ìˆëŠ” ë¬¸ì¥ ì—¬ë¶€ íŒì • ----------
def is_meaningful_sentence(s: str) -> bool:
    """10ì ë¯¸ë§Œ ë˜ëŠ” ë™ì‚¬ ë‹¨ë…í˜•(ì˜ë¯¸ ë¹ˆì•½)ì€ ì œì™¸"""
    raw = re.sub(r"\s+", "", s)
    if len(raw) < 10:
        return False
    # 'í•©ë‹ˆë‹¤/í•œë‹¤'ë§Œ ìˆëŠ” ë‹¨ë…í˜• ë°°ì œ(ì¡°ì‚¬ ì—†ëŠ” ì¼€ì´ìŠ¤ ë“±)
    if re.fullmatch(r"[ê°€-í£\s]+(í•©ë‹ˆë‹¤|í•œë‹¤)\.", s.strip()):
        return False
    return True

def soften(s: str) -> str:
    s = s.replace("í•˜ì—¬ì•¼", "í•´ì•¼ í•©ë‹ˆë‹¤").replace("í•œë‹¤", "í•©ë‹ˆë‹¤").replace("í•œë‹¤.", "í•©ë‹ˆë‹¤.")
    s = s.replace("ë°”ëë‹ˆë‹¤", "í•´ì£¼ì„¸ìš”").replace("í™•ì¸ ë°”ëŒ", "í™•ì¸í•´ì£¼ì„¸ìš”")
    s = s.replace("ê¸ˆì§€í•œë‹¤", "ê¸ˆì§€í•©ë‹ˆë‹¤").replace("í•„ìš”í•˜ë‹¤", "í•„ìš”í•©ë‹ˆë‹¤")
    s = re.sub(r"^\(([^)]+)\)\s*", "", s)
    for pat in META_PATTERNS:
        s = re.sub(pat, "", s).strip()
    s = re.sub(BULLET_PREFIX, "", s).strip(" -â€¢â—\t")
    return tidy_korean_spaces(s)

def is_accident_sentence(s: str) -> bool:
    if any(w in s for w in ["ì˜ˆë°©", "ëŒ€ì±…", "ì§€ì¹¨", "ìˆ˜ì¹™"]):
        return False
    return bool(re.search(DATE_PAT, s) or re.search(r"(ì‚¬ë§|ì‚¬ìƒ|ì‚¬ê³ |ì¤‘ë…|í™”ì¬|ë¶•ê´´|ì§ˆì‹|ì¶”ë½|ê¹”ë¦¼|ë¶€ë”ªí˜|ë¬´ë„ˆì§|ë‚™í•˜)", s))

def is_prevention_sentence(s: str) -> bool:
    return any(w in s for w in ["ì˜ˆë°©", "ëŒ€ì±…", "ì§€ì¹¨", "ìˆ˜ì¹™", "ì•ˆì „ì¡°ì¹˜"])

def is_risk_sentence(s: str) -> bool:
    return any(w in s for w in ["ìœ„í—˜", "ìš”ì¸", "ì›ì¸", "ì¦ìƒ", "ê²°ë¹™", "ê°•í’", "í­ì—¼", "ë¯¸ì„¸ë¨¼ì§€", "íšŒì „ì²´", "ë¹„ì‚°", "ë§ë¦¼", "ì¶”ë½", "ë‚™í•˜", "í˜‘ì°©"])

def _domain_template_apply(s: str, base_text: str) -> str:
    if not st.session_state.get("domain_toggle"):
        return s
    sent_toks = set(tokens(s))
    base_toks = set(tokens(base_text))
    if jaccard(sent_toks, base_toks) < 0.05:
        return s
    best = None
    best_hits = 0
    for triggers, render in DOMAIN_TEMPLATES:
        if (sent_toks & triggers) and (base_toks & triggers):
            hits = len((sent_toks | base_toks) & triggers)
            if hits > best_hits:
                best_hits = hits
                best = render
    return best if best else s

def to_action_sentence(s: str, base_text: str) -> str:
    """í–‰ë™/ìˆ˜ì¹™ ë¬¸ì¥ì„ ìì—°ìŠ¤ëŸ½ê²Œ ë¦¬ë¼ì´íŒ… + ì¡°ì‚¬ ë³´ì •"""
    s2 = soften(s)
    s2 = re.sub(r"(ìœ„ê¸°íƒˆì¶œ\s*ì•ˆì „ë³´ê±´)", "", s2).strip()
    s2 = re.sub(r"\s*ì—\s*ë”°ë¥¸\s*", " ì‹œ ", s2)
    s2 = re.sub(r"\s*ì—\s*ë”°ë¼\s*", " ì‹œ ", s2)

    # ë„ë©”ì¸ í…œí”Œë¦¿(ì„ íƒì ) ì ìš©
    s2_tpl = _domain_template_apply(s2, base_text)
    if s2_tpl != s2:
        txt = s2_tpl
        if not txt.endswith(("ë‹¤.", "ìŠµë‹ˆë‹¤.", "í•©ë‹ˆë‹¤.")):
            txt = txt.rstrip(" .") + " í•©ë‹ˆë‹¤."
        return tidy_korean_spaces(txt)

    m = re.search(ACTION_PAT, s2)
    if not m:
        # íŒ¨í„´ ê°ì§€ê°€ ì•ˆ ë˜ë©´ ì¢…ê²°ë§Œ ì •ë¦¬
        txt = s2 if s2.endswith(("ë‹ˆë‹¤.", "í•©ë‹ˆë‹¤.", "ë‹¤.")) else (s2.rstrip(" .") + " í•©ë‹ˆë‹¤.")
        return tidy_korean_spaces(txt)

    obj = (m.group("obj") or m.group("obj2") or "").strip()
    verb = (m.group("verb") or m.group("verb2") or "ì‹¤ì‹œ").strip()

    # ëª©ì ì–´ ì¡°ì‚¬ ë³´ì •
    if obj and not re.search(r"(ì„|ë¥¼|ì—|ì—ì„œ|ê³¼|ì™€|ì˜)$", obj):
        obj = add_obj_particle(obj)

    # ì ‘ë‘ì–´(ë°˜ë“œì‹œ/ì‘ì—… ì „)
    prefix = "ë°˜ë“œì‹œ " if "ì„¤ì¹˜" in verb else ("ì‘ì—… ì „ " if verb in ("í™•ì¸", "ì ê²€", "ì¸¡ì •", "ê¸°ë¡", "ì‘ì„±", "ì§€ì •") else "")
    core = tidy_korean_spaces(f"{prefix}{obj} {verb}")
    return core.rstrip(" .") + " í•©ë‹ˆë‹¤."

def classify_sentence(s: str) -> str:
    if is_accident_sentence(s):
        return "case"
    if re.search(ACTION_PAT, s) or is_prevention_sentence(s):
        return "action"
    if is_risk_sentence(s):
        return "risk"
    if "?" in s or "í™•ì¸" in s or "ì ê²€" in s:
        return "question"
    return "other"

# -------------------- KB ëˆ„ì /ì‹œë“œ --------------------
def seed_kb_once():
    if not st.session_state["seed_loaded"]:
        for t, k in SEED_RISK_MAP.items():
            if t not in RISK_KEYWORDS:
                RISK_KEYWORDS[t] = k
        for a in SEED_ACTIONS:
            if 2 <= len(a) <= 160:
                st.session_state["kb_actions"].append(a if a.endswith(("ë‹¤", "ë‹¤.", "í•©ë‹ˆë‹¤", "í•©ë‹ˆë‹¤.")) else a + " í•©ë‹ˆë‹¤.")
        for q in SEED_QUESTIONS:
            st.session_state["kb_questions"].append(q if q.endswith("?") else q + "?")
        for t in SEED_RISK_MAP.keys():
            st.session_state["kb_terms"][t] += 5
        st.session_state["seed_loaded"] = True

def kb_ingest_text(text: str) -> None:
    if not (text or "").strip():
        return
    sents = preprocess_text_to_sentences(text)
    for s in sents:
        for t in tokens(s):
            if len(t) >= 2:
                st.session_state["kb_terms"][t] += 1
                if re.search(r"(ì¶”ë½|ë‚™í•˜|ê¹”ë¦¼|ë¼ì„|ì¤‘ë…|ì§ˆì‹|í™”ì¬|í­ë°œ|ê°ì „|í­ì—¼|ë¶•ê´´|ë¹„ê³„|ê°±í¼|ì˜ˆì´ˆ|ë²Œëª©|ì»¨ë² ì´ì–´|í¬ë ˆì¸|ì§€ë¶•|ì„ ë°˜|ì²œê³µ)", t):
                    if t not in RISK_KEYWORDS:
                        RISK_KEYWORDS[t] = t
    for s in sents:
        if re.search(ACTION_PAT, s) or is_prevention_sentence(s):
            cand = to_action_sentence(s, text)
            if 2 <= len(cand) <= 160:
                st.session_state["kb_actions"].append(cand)
    for s in sents:
        if "?" in s or "í™•ì¸" in s or "ì ê²€" in s:
            q = soften(s if s.endswith("?") else s + " ë§ìŠµë‹ˆê¹Œ?")
            if 2 <= len(q) <= 140:
                st.session_state["kb_questions"].append(q)

def kb_prune() -> None:
    def dedup_keep_order(lst: List[str]) -> List[str]:
        seen, out = set(), []
        for x in lst:
            k = re.sub(r"\s+", "", x)
            if k not in seen:
                seen.add(k)
                out.append(x)
        return out
    st.session_state["kb_actions"] = dedup_keep_order(st.session_state["kb_actions"])[:900]
    st.session_state["kb_questions"] = dedup_keep_order(st.session_state["kb_questions"])[:500]
    st.session_state["kb_terms"] = Counter(dict(st.session_state["kb_terms"].most_common(1800)))

# ---------- (â‘¡) ë„ë©”ì¸ í‚¤ì›Œë“œ ê¸°ë°˜ + ìœ ì‚¬ë„ ì„ê³„ ë³´ê°• ----------
def kb_match_candidates(cands: List[str], base_text: str, limit: int, min_sim: float = 0.15) -> List[str]:
    """í˜„ì¬ í…ìŠ¤íŠ¸ì— ì—†ëŠ” ìœ„í—˜ í‚¤ì›Œë“œê°€ ë“¤ì–´ê°„ í›„ë³´ëŠ” ë°°ì œ + ìœ ì‚¬ë„ ì„ê³„ì¹˜ ì ìš©"""
    bt = set(tokens(base_text))
    # í˜„ì¬ ë³¸ë¬¸ì— ë“±ì¥í•œ ìœ„í—˜ í‚¤ì›Œë“œ ì§‘í•©(ì›í˜•/ë§¤í•‘ ëª¨ë‘ ê³ ë ¤)
    present_risks = {t for t in bt if (t in RISK_KEYWORDS or t in RISK_KEYWORDS.values())}
    scored: List[Tuple[float, str]] = []
    for c in cands:
        ct = set(tokens(c))
        # ë„ë©”ì¸ í‚¤ì›Œë“œ í•„í„°: í›„ë³´ì— ìˆëŠ” ìœ„í—˜ í‚¤ì›Œë“œê°€ í˜„ì¬ í…ìŠ¤íŠ¸ì— ì—†ìœ¼ë©´ ì œì™¸
        cand_risks = {RISK_KEYWORDS.get(t, t) for t in ct if (t in RISK_KEYWORDS or t in RISK_KEYWORDS.values())}
        if cand_risks and not (cand_risks & present_risks):
            continue
        j = len(bt & ct) / (len(bt | ct) + 1e-8)
        if j >= min_sim:
            scored.append((j, c))
    scored.sort(key=lambda x: x[0], reverse=True)
    return [c for _, c in scored[:limit]]

# -------------------- ìƒì„±ê¸° --------------------
def naturalize_case_sentence(s: str) -> str:
    """ì‚¬ê³  ì„œìˆ ì„ ìì—°ì–´ ì¢…ê²°ë¡œ ë³´ì •(â‘  ì¢…ê²° ê°•í™”)."""
    s = soften(s)
    # ì‚¬ìƒ/ì‚¬ë§ ë“± ìˆ˜ì¹˜
    death = re.search(r"ì‚¬ë§\s*(\d+)\s*ëª…", s)
    inj = re.search(r"ì‚¬ìƒ\s*(\d+)\s*ëª…", s)
    unconscious = re.search(r"ì˜ì‹ë¶ˆëª…", s)
    info = []
    if death:
        info.append(f"ê·¼ë¡œì {death.group(1)}ëª… ì‚¬ë§")
    if inj and not death:
        info.append(f"{inj.group(1)}ëª… ì‚¬ìƒ")
    if unconscious:
        info.append("ì˜ì‹ë¶ˆëª… ë°œìƒ")
    # ë‚ ì§œ
    m = re.search(DATE_PAT, s)
    date_txt = ""
    if m:
        y, mo, d = m.groups()
        y = int(str(y).replace("â€™", "").replace("'", ""))
        y = 2000 + y if y < 100 else y
        date_txt = f"{int(y)}ë…„ {int(mo)}ì›” {int(d)}ì¼, "
        s = s.replace(m.group(0), "").strip()
    s = s.strip(" ,.-")

    # ì¢…ê²° ì²˜ë¦¬: ì‚¬ê³  í‚¤ì›Œë“œë¡œ ëë‚˜ëŠ” ê²½ìš° "í–ˆìŠµë‹ˆë‹¤." ë¶€ì—¬
    if not re.search(r"(ë‹¤\.|ì…ë‹ˆë‹¤\.|í–ˆìŠµë‹ˆë‹¤\.)$", s):
        if re.search(r"(ì‚¬ë§|ì‚¬ìƒ|ì¤‘ë…|ì¶”ë½|ë‚™í•˜|ë¶•ê´´|ì§ˆì‹|ë¼ì„|ê¹”ë¦¼|ë¶€ë”ªí˜|ê°ì „|í­ë°œ)\s*$", s):
            s = s.rstrip(" .") + "í–ˆìŠµë‹ˆë‹¤."
        elif re.search(r"(ì‚¬ê±´|ì‚¬ê³ )\s*$", s):
            s = s.rstrip(" .") + "ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
        else:
            s = s.rstrip(" .") + " ì‚¬ê³ ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."

    # ì¶”ê°€ ì •ë³´ ê¼¬ë¦¬(ì´ë¯¸ 'í–ˆìŠµë‹ˆë‹¤.'ë¡œ ëë‚œ ê²½ìš°ëŠ” ì¤‘ë³µ íšŒí”¼)
    if info and not s.endswith("í–ˆìŠµë‹ˆë‹¤."):
        s = tidy_korean_spaces(s.rstrip(" .") + " " + (", ".join(info)) + "í–ˆìŠµë‹ˆë‹¤.")
    return tidy_korean_spaces((date_txt + s).strip())

def make_structured_script(text: str, max_points: int = 6) -> str:
    topic_label = dynamic_topic_label(text)
    core = [soften(s) for s in ai_extract_summary(text, max_points)]
    if not core:
        return "ë³¸ë¬¸ì´ ì¶©ë¶„í•˜ì§€ ì•Šì•„ ëŒ€ë³¸ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
    case, risk, act, ask = [], [], [], []
    for s in core:
        c = classify_sentence(s)
        if c == "case":
            case.append(naturalize_case_sentence(s))
        elif c == "action":
            act.append(to_action_sentence(s, text))
        elif c == "risk":
            risk.append(soften(s))
        elif c == "question":
            ask.append(soften(s if s.endswith("?") else s + " ë§ìŠµë‹ˆê¹Œ?"))

    # (â‘¡) ë„ë©”ì¸/ìœ ì‚¬ë„ í•„í„°ë¡œ KB ë³´ê°•
    if len(act) < 5 and st.session_state["kb_actions"]:
        act += kb_match_candidates(st.session_state["kb_actions"], text, 5 - len(act), min_sim=0.15)
    if not ask and st.session_state["kb_questions"]:
        ask = kb_match_candidates(st.session_state["kb_questions"], text, 3, min_sim=0.15)

    # (â‘¢â‘£) ì˜ë¯¸ ì—†ëŠ” ì§§ì€ ë¬¸ì¥ ì œê±°
    act = [a for a in act if is_meaningful_sentence(a)][:5]
    ask = [q if q.endswith("?") else q + "?" for q in ask if is_meaningful_sentence(q)][:3]

    # ë³¸ë¬¸ êµ¬ì„±
    lines = []
    lines.append(f"ğŸ¦º TBM êµìœ¡ëŒ€ë³¸ â€“ {topic_label}\n")
    lines.append("â— ë„ì…")
    lines.append(f"ì˜¤ëŠ˜ì€ ìµœê·¼ ë°œìƒí•œ '{topic_label.replace(' ì¬í•´ì˜ˆë°©','')}' ì‚¬ê³  ì‚¬ë¡€ë¥¼ ì¤‘ì‹¬ìœ¼ë¡œ, ìš°ë¦¬ í˜„ì¥ì—ì„œ ê°™ì€ ì‚¬ê³ ë¥¼ ì˜ˆë°©í•˜ê¸° ìœ„í•œ ì•ˆì „ì¡°ì¹˜ë¥¼ í•¨ê»˜ ì‚´í´ë³´ê² ìŠµë‹ˆë‹¤.\n")
    if case:
        lines.append("â— ì‚¬ê³  ì‚¬ë¡€")
        for c in case:
            lines.append(f"- {c}")
        lines.append("")
    if risk:
        lines.append("â— ì£¼ìš” ìœ„í—˜ìš”ì¸")
        for r in risk:
            lines.append(f"- {r}")
        lines.append("")
    if act:
        lines.append("â— ì˜ˆë°©ì¡°ì¹˜ / ì‹¤ì²œ ìˆ˜ì¹™")
        for i, a in enumerate(act, 1):
            lines.append(f"{i}ï¸âƒ£ {a}")
        lines.append("")
    if ask:
        lines.append("â— í˜„ì¥ ì ê²€ ì§ˆë¬¸")
        for q in ask:
            lines.append(f"- {q}")
        lines.append("")
    lines.append("â— ë§ˆë¬´ë¦¬ ë‹¹ë¶€")
    lines.append("ì˜ˆë°©ì¡°ì¹˜ëŠ” 'ì„ ì¡°ì¹˜ í›„ì‘ì—…'ì´ ì›ì¹™ì…ë‹ˆë‹¤. ì˜¤ëŠ˜ ì‘ì—… ì „, ê° ê³µì •ë³„ ìœ„í—˜ìš”ì¸ì„ ë‹¤ì‹œ í•œ ë²ˆ ì ê²€í•˜ê³  í•„ìš”í•œ ë³´í˜¸êµ¬ì™€ ì•ˆì „ì¡°ì¹˜ë¥¼ ë°˜ë“œì‹œ ì¤€ë¹„í•©ì‹œë‹¤.")
    lines.append("â— êµ¬í˜¸")
    lines.append("â€œí•œ ë²ˆ ë” í™•ì¸! í•œ ë²ˆ ë” ì ê²€!â€")
    return "\n".join(lines)

def make_concise_report(text: str, max_points: int = 6) -> str:
    sents = ai_extract_summary(text, max_points)
    sents = [soften(s) for s in sents if not re.match(r"(ë°°í¬ì²˜|ì£¼ì†Œ|í™ˆí˜ì´ì§€|VR|ë¦¬í”Œë¦¿)", s)]
    if not sents:
        return "í…ìŠ¤íŠ¸ì—ì„œ í•µì‹¬ì„ ìš”ì•½í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
    cases = [naturalize_case_sentence(s) for s in sents if is_accident_sentence(s)]
    risks  = [soften(s) for s in sents if (not is_accident_sentence(s)) and is_risk_sentence(s)]
    acts   = [to_action_sentence(s, text) for s in sents if (not is_accident_sentence(s)) and (is_prevention_sentence(s) or re.search(ACTION_PAT, s))]

    def uniq_keep(seq: List[str]) -> List[str]:
        seen, out = set(), []
        for x in seq:
            k = re.sub(r"\s+", "", x)
            if k not in seen:
                seen.add(k)
                out.append(x)
        return out

    # (â‘¢â‘£) ì˜ë¯¸ ì—†ëŠ” ë¬¸ì¥ ì œê±°
    cases = uniq_keep([c for c in cases if is_meaningful_sentence(c)])[:3]
    risks = uniq_keep([r for r in risks if is_meaningful_sentence(r)])[:3]
    acts  = uniq_keep([a for a in acts if is_meaningful_sentence(a)])[:4]

    topic = dynamic_topic_label(text)
    lines = [f"ğŸ“„ í•µì‹¬ìš”ì•½ â€” {topic}\n"]
    if cases:
        lines.append("ã€ì‚¬ê³  ê°œìš”ã€‘")
        lines.append("ìµœê·¼ ìë£Œì—ì„œ ë‹¤ìŒê³¼ ê°™ì€ ì‚¬ê³ ê°€ í™•ì¸ë˜ì—ˆìŠµë‹ˆë‹¤.")
        for c in cases:
            lines.append(f"- {c}")
        lines.append("")
    if risks:
        lines.append("ã€ì£¼ìš” ìœ„í—˜ìš”ì¸ã€‘")
        lines.append("ìë£Œ ì „ë°˜ì—ì„œ ë‹¤ìŒ ìš”ì¸ì´ ë°˜ë³µì ìœ¼ë¡œ ë‚˜íƒ€ë‚¬ìŠµë‹ˆë‹¤.")
        for r in risks:
            lines.append(f"- {r}")
        lines.append("")
    if acts:
        lines.append("ã€ì˜ˆë°©/ì‹¤ì²œ ìš”ì•½ã€‘")
        lines.append("í˜„ì¥ì—ì„œ ì¦‰ì‹œ ì ìš© ê°€ëŠ¥í•œ í•µì‹¬ ìˆ˜ì¹™ì…ë‹ˆë‹¤.")
        for a in acts:
            lines.append(f"- {a}")
        lines.append("")
    if not (cases or risks or acts):
        lines.append("ìë£Œì˜ í•µì‹¬ì„ ê°„ë‹¨íˆ ì •ë¦¬í•˜ë©´ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.")
        for s in sents:
            lines.append(f"- {s}")
    return "\n".join(lines)

# -------------------- ë¼ë²¨/í† í”½ --------------------
def drop_label_token(t: str) -> bool:
    if t in STOP_TERMS:
        return True
    for pat in LABEL_DROP_PAT:
        if re.match(pat, t):
            return True
    if t in {"ì†Œì¬", "ì†Œì¬ì§€", "ì§€ì—­", "ì¥ì†Œ", "ë²„ìŠ¤", "ì˜ì—…ì†Œ", "ì—…ì²´", "ìë£Œ", "í‚¤", "ë©”ì„¸ì§€", "ëª…", "ì•ˆì „ë³´ê±´"}:
        return True
    return False

def top_terms_for_label(text: str, k: int = 3) -> List[str]:
    doc_cnt = Counter([t for t in tokens(text) if not drop_label_token(t)])
    bonus = Counter()
    for t in list(doc_cnt.keys()):
        if t in RISK_KEYWORDS:
            bonus[RISK_KEYWORDS[t]] += doc_cnt[t]
    doc_cnt += bonus
    kb = st.session_state["kb_terms"]
    if kb:
        for t, c in kb.items():
            if not drop_label_token(t):
                doc_cnt[t] += 0.2 * c
    if not doc_cnt:
        return ["ì•ˆì „ë³´ê±´", "êµìœ¡"]
    commons = {"ì•ˆì „", "êµìœ¡", "ì‘ì—…", "í˜„ì¥", "ì˜ˆë°©", "ì¡°ì¹˜", "í™•ì¸", "ê´€ë¦¬", "ì ê²€", "ê°€ì´ë“œ", "ì§€ì¹¨"}
    cand = [(t, doc_cnt[t]) for t in doc_cnt if t not in commons and len(t) >= 2]
    if not cand:
        cand = list(doc_cnt.items())
    cand.sort(key=lambda x: x[1], reverse=True)
    return [t for t, _ in cand[:k]]

def dynamic_topic_label(text: str) -> str:
    terms = top_terms_for_label(text, k=3)
    risks = [RISK_KEYWORDS.get(t, t) for t in terms if t in RISK_KEYWORDS or t in RISK_KEYWORDS.values()]
    extra = [t for t in terms if t not in risks]
    label_core = " ".join(sorted(set(risks), key=risks.index)) or "ì•ˆì „ë³´ê±´"
    tail = " ".join(extra[:1])
    label = (label_core + (" " + tail if tail else "")).strip()
    if "ì¬í•´ì˜ˆë°©" not in label:
        label += " ì¬í•´ì˜ˆë°©"
    return label

# -------------------- DOCX --------------------
_XML_FORBIDDEN = r"[\x00-\x08\x0B\x0C\x0E-\x1F\uD800-\uDFFF\uFFFE\uFFFF]"
def _xml_safe(s: str) -> str:
    if not isinstance(s, str):
        s = "" if s is None else str(s)
    return rxx.sub(_XML_FORBIDDEN, "", s)

def to_docx_bytes(script: str) -> bytes:
    doc = Document()
    try:
        style = doc.styles["Normal"]
        style.font.name = "Malgun Gothic"
        style.font.size = Pt(11)
    except Exception:
        pass
    for raw in script.split("\n"):
        line = _xml_safe(raw)
        p = doc.add_paragraph(line)
        for run in p.runs:
            try:
                run.font.name = "Malgun Gothic"
                run.font.size = Pt(11)
            except Exception:
                pass
    bio = io.BytesIO()
    doc.save(bio)
    bio.seek(0)
    return bio.read()

# -------------------- UI --------------------
with st.sidebar:
    st.header("â„¹ï¸ ì†Œê°œ / ì‚¬ìš©ë²•")
    st.markdown("""
**AI íŒŒì´í”„ë¼ì¸(ì™„ì „ ë¬´ë£Œ, ì˜¤í”ˆì†ŒìŠ¤ë§Œ ì‚¬ìš©)**  
1) ì „ì²˜ë¦¬(ë…¸ì´ì¦ˆ ì œê±°/ì¤„ ë³‘í•©/ë‚ ì§œ-ì‚¬ê³  ê²°í•©)  
2) **ì‚¬ê³  ë¸”ë¡ ë³‘í•©**(ì—°ê²°ì–´Â·í‚¤ì›Œë“œ ê¸°ë°˜ìœ¼ë¡œ ì—°ì† ì„œìˆ ì„ í•œ ë¬¸ì¥ìœ¼ë¡œ)  
3) TextRank + MMR ìš”ì•½ (**ì„¸ì…˜ KB ê°€ì¤‘ì¹˜** ë°˜ì˜)  
4) ê·œì¹™í˜• NLG: ì¡°ì‚¬/ë„ì–´ì“°ê¸°Â·ì¢…ê²° ë³´ì •, í–‰ë™/ì§ˆë¬¸ í…œí”Œë¦¿ ë³´ê°•  
5) ê²°ê³¼ í¬ë§·: **ìì—°ìŠ¤ëŸ¬ìš´ êµìœ¡ëŒ€ë³¸(ë¬´ë£Œ)** / **í•µì‹¬ìš”ì•½**
""")
    st.session_state["domain_toggle"] = st.toggle(
        "ğŸ”§ ë„ë©”ì¸ í…œí”Œë¦¿ ê°•í™”(ì‹ ì¤‘ ì ìš©)",
        value=False,
        help="ë¬¸ì¥Â·ë³¸ë¬¸ íŠ¸ë¦¬ê±° ì¼ì¹˜ + ìœ ì‚¬ë„ ê¸°ì¤€ ì¶©ì¡± ì‹œì—ë§Œ í…œí”Œë¦¿ì„ ì ìš©í•©ë‹ˆë‹¤."
    )

seed_kb_once()
st.title("ğŸ¦º OPS/í¬ìŠ¤í„°ë¥¼ êµìœ¡ ëŒ€ë³¸ìœ¼ë¡œ ìë™ ë³€í™˜ (ì™„ì „ ë¬´ë£Œ)")

def reset_all():
    # ì „ì²´ ìƒíƒœ ì´ˆê¸°í™”(íŒŒì¼ ì…ë ¥ í‚¤ í¬í•¨)
    st.session_state.pop("manual_text", None)
    st.session_state.pop("edited_text", None)
    st.session_state.pop("zip_choice", None)
    st.session_state["kb_terms"] = Counter()
    st.session_state["kb_actions"] = []
    st.session_state["kb_questions"] = []
    st.session_state["uploader_key"] += 1
    st.session_state["seed_loaded"] = False
    st.session_state["last_file_diag"] = {}
    st.session_state["last_extracted_cache"] = ""
    st.rerun()

col_top1, col_top2 = st.columns([4, 1])
with col_top2:
    st.button("ğŸ§¹ ì´ˆê¸°í™”", on_click=reset_all, use_container_width=True)

st.markdown("**ì•ˆë‚´**  \n- í…ìŠ¤íŠ¸ê°€ í¬í•¨ëœ PDF ë˜ëŠ” ë³¸ë¬¸ í…ìŠ¤íŠ¸ë¥¼ ê¶Œì¥í•©ë‹ˆë‹¤.  \n- ì´ë¯¸ì§€/ìŠ¤ìº” PDFëŠ” í˜„ì¬ OCR ë¯¸ì§€ì›ì…ë‹ˆë‹¤.")

col1, col2 = st.columns([1, 1], gap="large")

# ---------- ì¢Œì¸¡ ì…ë ¥/ë¯¸ë¦¬ë³´ê¸° ----------
with col1:
    uploaded = st.file_uploader(
        "OPS ì—…ë¡œë“œ (PDF ë˜ëŠ” ZIP) â€¢ í…ìŠ¤íŠ¸ PDF ê¶Œì¥",
        type=["pdf", "zip"],
        key=f"uploader_{st.session_state['uploader_key']}"
    )
    manual_text = st.text_area(
        "ë˜ëŠ” OPS í…ìŠ¤íŠ¸ ì§ì ‘ ë¶™ì—¬ë„£ê¸°",
        key="manual_text",
        height=220,
        placeholder="ì˜ˆ: í˜„ì¥ ì•ˆë‚´ë¬¸ ë˜ëŠ” OPS ë³¸ë¬¸ í…ìŠ¤íŠ¸â€¦"
    )

    extracted: str = ""
    zip_pdfs: Dict[str, bytes] = {}

    if uploaded is not None:
        fname = (uploaded.name or "").lower()
        try:
            raw_bytes = uploaded.getvalue()
        except Exception:
            raw_bytes = uploaded.read()

        if fname.endswith(".zip"):
            try:
                with zipfile.ZipFile(io.BytesIO(raw_bytes), "r") as zf:
                    for name in zf.namelist():
                        if name.lower().endswith(".pdf"):
                            data = zf.read(name)
                            zip_pdfs[name] = data
                if zip_pdfs:
                    # ZIP ì „ì²´ í•™ìŠµ
                    for nm, data in zip_pdfs.items():
                        txt_all = read_pdf_text_from_bytes(data, fname=f"{fname}::{nm}")
                        if txt_all.strip():
                            kb_ingest_text(txt_all)
                    kb_prune()
                    # ìë™ ì„ íƒ + ì—ë””í„° ì±„ì›€
                    first_name = sorted(zip_pdfs.keys())[0]
                    extracted = read_pdf_text_from_bytes(zip_pdfs[first_name], fname=first_name)
                    if extracted.strip():
                        st.session_state["edited_text"] = extracted
                        st.session_state["last_extracted_cache"] = extracted
                    st.success(f"ZIP ê°ì§€: {len(zip_pdfs)}ê°œ PDF, ì²« ë¬¸ì„œ ìë™ ì„ íƒ â†’ {first_name}")
                else:
                    st.error("ZIP ë‚´ì— PDFê°€ ì—†ìŠµë‹ˆë‹¤.")
            except Exception as e:
                st.error(f"ZIP í•´ì œ ì˜¤ë¥˜: {e}")

            if zip_pdfs:
                chosen = st.selectbox("ZIP ë‚´ PDF ì„ íƒ", sorted(zip_pdfs.keys()), key="zip_choice")
                if chosen and zip_pdfs.get(chosen):
                    extracted2 = read_pdf_text_from_bytes(zip_pdfs[chosen], fname=chosen)
                    if extracted2.strip():
                        st.session_state["edited_text"] = extracted2
                        st.session_state["last_extracted_cache"] = extracted2

        elif fname.endswith(".pdf"):
            extracted = read_pdf_text_from_bytes(raw_bytes, fname=fname)
            if extracted.strip():
                kb_ingest_text(extracted)
                kb_prune()
                st.session_state["edited_text"] = extracted
                st.session_state["last_extracted_cache"] = extracted
            else:
                st.warning("âš ï¸ PDFì—ì„œ ìœ íš¨í•œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

        else:
            st.warning("ì§€ì›í•˜ì§€ ì•ŠëŠ” í˜•ì‹ì…ë‹ˆë‹¤. PDF ë˜ëŠ” ZIPì„ ì—…ë¡œë“œí•˜ì„¸ìš”.")

    pasted = (manual_text or "").strip()
    if pasted:
        kb_ingest_text(pasted)
        kb_prune()
        st.session_state["edited_text"] = pasted
        st.session_state["last_extracted_cache"] = pasted

    # ë¯¸ë¦¬ë³´ê¸°
    base_text = st.session_state.get("edited_text", "")
    st.markdown("**ì¶”ì¶œ/ì…ë ¥ í…ìŠ¤íŠ¸ ë¯¸ë¦¬ë³´ê¸°**")
    edited_text = st.text_area("í…ìŠ¤íŠ¸", value=base_text, height=240, key="edited_text")

    with st.expander("ğŸ§ª íŒŒì¼ ì½ê¸° ì§„ë‹¨(Log-lite)", expanded=False):
        diag = st.session_state.get("last_file_diag", {})
        if diag:
            st.write({
                "íŒŒì¼ëª…": diag.get("name"),
                "í¬ê¸°(bytes)": diag.get("size_bytes"),
                "ì¶”ì¶œëœ ë¬¸ììˆ˜": diag.get("extracted_chars"),
                "ë©”ëª¨": diag.get("note"),
            })
        st.caption(f"í˜„ì¬ í…ìŠ¤íŠ¸ ë°•ìŠ¤ ê¸¸ì´: {len(st.session_state.get('edited_text',''))} chars")

# ---------- ìš°ì¸¡ ì˜µì…˜/ìƒì„±/ë‹¤ìš´ë¡œë“œ ----------
with col2:
    gen_mode = st.selectbox("ğŸ§  ìƒì„± ëª¨ë“œ", ["í•µì‹¬ìš”ì•½", "ìì—°ìŠ¤ëŸ¬ìš´ êµìœ¡ëŒ€ë³¸(ë¬´ë£Œ)"])
    max_points = st.slider("ìš”ì•½ ê°•ë„(í•µì‹¬ë¬¸ì¥ ê°œìˆ˜)", 3, 10, 6)

    if st.button("ğŸ› ï¸ ëŒ€ë³¸ ìƒì„±", type="primary", use_container_width=True):
        text_for_gen = (st.session_state.get("edited_text") or "").strip()
        if not text_for_gen:
            text_for_gen = (st.session_state.get("last_extracted_cache") or "").strip()
            if text_for_gen:
                st.info("ë¹ˆ ì…ë ¥ì„ ìµœê·¼ ì¶”ì¶œ í…ìŠ¤íŠ¸ë¡œ ìë™ ëŒ€ì²´í–ˆìŠµë‹ˆë‹¤.")

        if not text_for_gen:
            st.warning("PDF/ZIP ì—…ë¡œë“œ ë˜ëŠ” í…ìŠ¤íŠ¸ ì…ë ¥ í›„ ì‹œë„í•˜ì„¸ìš”.")
        else:
            with st.spinner("ìƒì„± ì¤‘..."):
                if gen_mode == "ìì—°ìŠ¤ëŸ¬ìš´ êµìœ¡ëŒ€ë³¸(ë¬´ë£Œ)":
                    script = make_structured_script(text_for_gen, max_points=max_points)
                    subtitle = "ìì—°ìŠ¤ëŸ¬ìš´ êµìœ¡ëŒ€ë³¸(ë¬´ë£Œ)"
                else:
                    script = make_concise_report(text_for_gen, max_points=max_points)
                    subtitle = "í•µì‹¬ìš”ì•½"
            st.success(f"ìƒì„± ì™„ë£Œ! ({subtitle})")
            st.text_area("ê²°ê³¼ ë¯¸ë¦¬ë³´ê¸°", value=script, height=420)
            c3, c4 = st.columns(2)
            with c3:
                st.download_button(
                    "â¬‡ï¸ TXT ë‹¤ìš´ë¡œë“œ",
                    data=_xml_safe(script).encode("utf-8"),
                    file_name="tbm_output.txt",
                    use_container_width=True
                )
            with c4:
                st.download_button(
                    "â¬‡ï¸ DOCX ë‹¤ìš´ë¡œë“œ",
                    data=to_docx_bytes(script),
                    file_name="tbm_output.docx",
                    use_container_width=True
                )

st.caption("ì™„ì „ ë¬´ë£Œ. ì‹œë“œ KB + ì—…ë¡œë“œ ëˆ„ì  í•™ìŠµ â†’ ìš”ì•½ ê°€ì¤‘/í–‰ë™/ì§ˆë¬¸ ë³´ê°•. ì‚¬ê³  ë¸”ë¡ ë³‘í•©Â·ì¢…ê²° ë³´ì •ìœ¼ë¡œ ì‚¬ë¡€ íë¦„ ìì—°í™”.")
