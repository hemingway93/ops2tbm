# ==========================================================
# OPS2TBM â€” ì™„ì „ ë¬´ë£Œ ì•ˆì •íŒ
# - ê¸°ì¡´ UI/ë ˆì´ì•„ì›ƒ ê·¸ëŒ€ë¡œ ìœ ì§€
# - AI ìš”ì•½(TextRank+MMR) + ê·œì¹™ ê¸°ë°˜ ë¬¸ì²´ ë³€í™˜
# - "ìì—°ìŠ¤ëŸ¬ìš´ êµìœ¡ëŒ€ë³¸(ë¬´ë£Œ)"ì—ì„œ 3ë¶„í˜• êµ¬ì¡°(ë„ì…~êµ¬í˜¸) ìë™ êµ¬ì„±
# - ZIP/PDF/í…ìŠ¤íŠ¸ ì…ë ¥ ëª¨ë‘ ì§€ì›
# - DOCX ë‚´ë³´ë‚´ê¸° íŠ¹ìˆ˜ë¬¸ì Sanitizer ì ìš©(ì´ì „ ValueError ë°©ì§€)
# ==========================================================

import io, zipfile
from typing import List, Tuple, Dict
import streamlit as st
from docx import Document
from docx.shared import Pt
from pdfminer.high_level import extract_text as pdf_extract_text
import pypdfium2 as pdfium
import numpy as np
import regex as rxx

# ----------------------------
# ì„¸ì…˜ ìƒíƒœ (ì—…ë¡œë” ì´ˆê¸°í™”ìš© í‚¤)
# ----------------------------
if "uploader_key" not in st.session_state:
    st.session_state.uploader_key = 0

# ----------------------------
# ìœ í‹¸: í…ìŠ¤íŠ¸ ì •ë¦¬/ë¶„ë¦¬
# ----------------------------
def normalize_text(t: str) -> str:
    t = t.replace("\x0c", "\n")
    t = rxx.sub(r"[ \t]+\n", "\n", t)
    t = rxx.sub(r"\n{3,}", "\n\n", t)
    return t.strip()

def split_sentences_ko(text: str) -> List[str]:
    # í•œêµ­ì–´/ë§ˆì¹¨í‘œ/ì¤„ë°”ê¿ˆ ê¸°ì¤€ ë¬¸ì¥ ë¶„ë¦¬
    raw = rxx.split(r"(?<=[\.!\?]|ë‹¤\.)\s+|\n+", text)
    sents = [s.strip(" -â€¢â—â–ªâ–¶â–·\t") for s in raw if s and len(s.strip()) > 5]
    return sents

def simple_tokens(s: str) -> List[str]:
    return rxx.findall(r"[ê°€-í£a-z0-9]{2,}", s.lower())

# ----------------------------
# PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ (í…ìŠ¤íŠ¸ PDF ê¶Œì¥)
# ----------------------------
def read_pdf_text(b: bytes) -> str:
    try:
        with io.BytesIO(b) as bio:
            t = pdf_extract_text(bio) or ""
    except Exception:
        t = ""
    t = normalize_text(t)

    if len(t.strip()) < 10:
        # ìŠ¤ìº”/ì´ë¯¸ì§€í˜• PDF ì¶”ì • â€” ì•ˆë‚´ë§Œ
        try:
            with io.BytesIO(b) as bio:
                pdf = pdfium.PdfDocument(bio)
                pages = len(pdf)
            if pages > 0 and not t.strip():
                st.warning("âš ï¸ ì´ PDFëŠ” ì´ë¯¸ì§€/ìŠ¤ìº” ê¸°ë°˜ìœ¼ë¡œ ë³´ì…ë‹ˆë‹¤. í˜„ì¬ ë²„ì „ì€ OCR ì—†ì´ í…ìŠ¤íŠ¸ë§Œ ì²˜ë¦¬í•©ë‹ˆë‹¤.")
        except Exception:
            pass
    return t

# ----------------------------
# AI ìš”ì•½: TextRank + MMR (ì™„ì „ ë¬´ë£Œ)
# ----------------------------
def sentence_tfidf_vectors(sents: List[str]):
    toks = [simple_tokens(s) for s in sents]
    vocab: Dict[str, int] = {}
    for ts in toks:
        for t in ts:
            if t not in vocab:
                vocab[t] = len(vocab)
    if not vocab:
        return np.zeros((len(sents), 0), dtype=np.float32), []
    M = np.zeros((len(sents), len(vocab)), dtype=np.float32)
    df = np.zeros((len(vocab),), dtype=np.float32)
    for i, ts in enumerate(toks):
        for t in ts:
            M[i, vocab[t]] += 1.0
        for t in set(ts):
            df[vocab[t]] += 1.0
    N = float(len(sents))
    idf = np.log((N + 1.0) / (df + 1.0)) + 1.0
    M *= idf
    M /= (np.linalg.norm(M, axis=1, keepdims=True) + 1e-8)
    return M, list(vocab.keys())

def cosine_sim_matrix(X: np.ndarray) -> np.ndarray:
    if X.size == 0:
        return np.zeros((X.shape[0], X.shape[0]), dtype=np.float32)
    sim = np.clip(X @ X.T, 0.0, 1.0)
    np.fill_diagonal(sim, 0.0)
    return sim

def textrank_scores(sents: List[str], d: float = 0.85, max_iter: int = 60, tol: float = 1e-4) -> List[float]:
    n = len(sents)
    if n == 0: return []
    X, _ = sentence_tfidf_vectors(sents)
    W = cosine_sim_matrix(X)
    row_sums = W.sum(axis=1, keepdims=True)
    P = np.divide(W, row_sums, out=np.zeros_like(W), where=row_sums > 0)
    r = np.ones((n, 1), dtype=np.float32) / n
    teleport = np.ones((n, 1), dtype=np.float32) / n
    for _ in range(max_iter):
        r_new = d * (P.T @ r) + (1 - d) * teleport
        if np.linalg.norm(r_new - r, ord=1) < tol:
            r = r_new; break
        r = r_new
    return [float(v) for v in r.flatten()]

def mmr_select(cands: List[str], scores: List[float], X: np.ndarray, k: int, lambda_: float = 0.7) -> List[int]:
    if not cands: return []
    selected: List[int] = []
    remaining = set(range(len(cands)))
    sim = cosine_sim_matrix(X)
    while remaining and len(selected) < k:
        best_idx, best_val = None, -1e9
        for i in remaining:
            rel = scores[i]
            div = max((sim[i, j] for j in selected), default=0.0)
            mmr = lambda_ * rel - (1 - lambda_) * div
            if mmr > best_val:
                best_val, best_idx = mmr, i
        selected.append(best_idx)
        remaining.remove(best_idx)
    return selected

def ai_extract_summary(text: str, limit: int = 8) -> List[str]:
    sents = split_sentences_ko(text)
    if not sents: return []
    X, _ = sentence_tfidf_vectors(sents)
    scores = textrank_scores(sents)
    idx = mmr_select(sents, scores, X, limit, lambda_=0.7)
    return [sents[i] for i in idx]

# ----------------------------
# ê·œì¹™ ê¸°ë°˜ ë¬¸ì²´ ë³€í™˜ + 3ë¶„í˜• ìë™ êµ¬ì„±
# ----------------------------
ACTION_VERBS = [
    "ì„¤ì¹˜","ë°°ì¹˜","ì°©ìš©","ì ê²€","í™•ì¸","ì¸¡ì •","ê¸°ë¡","í‘œì‹œ","ì œê³µ","ë¹„ì¹˜",
    "ë³´ê³ ","ì‹ ê³ ","êµìœ¡","ì£¼ì§€","ì¤‘ì§€","í†µì œ","íœ´ì‹","í™˜ê¸°","ì°¨ë‹¨","êµëŒ€","ë°°ì œ","ë°°ë ¤"
]

def soften(s: str) -> str:
    # ë”±ë”±í•œ í‘œí˜„ì„ ë§í•˜ê¸° í†¤ìœ¼ë¡œ ì™„í™”
    s = s.replace("í•˜ì—¬ì•¼", "í•´ì•¼ í•©ë‹ˆë‹¤")
    s = s.replace("í•œë‹¤", "í•©ë‹ˆë‹¤").replace("í•œë‹¤.", "í•©ë‹ˆë‹¤.")
    s = s.replace("ë°”ëë‹ˆë‹¤", "í•´ì£¼ì„¸ìš”").replace("í™•ì¸ ë°”ëŒ", "í™•ì¸í•´ì£¼ì„¸ìš”")
    s = s.replace("ì¡°ì¹˜í•œë‹¤", "ì¡°ì¹˜í•©ë‹ˆë‹¤").replace("ì°©ìš©í•œë‹¤", "ì°©ìš©í•©ë‹ˆë‹¤")
    s = s.replace("í•„ìš”í•˜ë‹¤", "í•„ìš”í•©ë‹ˆë‹¤").replace("ê¸ˆì§€í•œë‹¤", "ê¸ˆì§€í•©ë‹ˆë‹¤")
    return s.strip(" -â€¢â—\t")

def detect_topic(t: str) -> str:
    if "ì˜¨ì—´" in t or "í­ì—¼" in t: return "ì˜¨ì—´ì§ˆí™˜ ì˜ˆë°©"
    if "ì§ˆì‹" in t or "ë°€í" in t: return "ì§ˆì‹ì¬í•´ ì˜ˆë°©"
    if "ê°ì „" in t: return "ê°ì „ì‚¬ê³  ì˜ˆë°©"
    if "ì§€ë¶•" in t or "ì¬ë¼ì´íŠ¸" in t: return "ì§€ë¶• ì‘ì—… ì¶”ë½ì‚¬ê³  ì˜ˆë°©"
    if "ì»¨ë² ì´ì–´" in t or "ë¼ì„" in t: return "ì»¨ë² ì´ì–´ ë¼ì„ì‚¬ê³  ì˜ˆë°©"
    return "ì•ˆì „ë³´ê±´ êµìœ¡"

def make_structured_script(text: str, max_points: int = 6) -> str:
    """
    3ë¶„í˜• êµìœ¡ëŒ€ë³¸ ìë™ êµ¬ì„±:
    - AI ìš”ì•½(TextRank+MMR)ìœ¼ë¡œ í•µì‹¬ ë¬¸ì¥ ì¶”ì¶œ
    - 'ì‚¬ê³ /ìœ„í—˜/ì¡°ì¹˜/ì§ˆë¬¸' ì„±ê²©ìœ¼ë¡œ ë¶„ë¥˜
    - ë„ì…~êµ¬í˜¸ í¬ë§·ìœ¼ë¡œ ì¡°ë¦½
    """
    topic = detect_topic(text)
    core = [soften(s) for s in ai_extract_summary(text, max_points)]
    if not core:
        return "ë³¸ë¬¸ì´ ì¶©ë¶„í•˜ì§€ ì•Šì•„ ëŒ€ë³¸ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

    case, risk, act, ask, misc = [], [], [], [], []
    for s in core:
        if any(k in s for k in ["ì‚¬ê³ ", "ë°œìƒ", "ì‚¬ë¡€", "ì‚¬ë§"]):
            case.append(s)
        elif any(k in s for k in ["ìœ„í—˜", "ìš”ì¸", "ì›ì¸"]):
            risk.append(s)
        elif any(k in s for k in ["ì¡°ì¹˜", "ì˜ˆë°©"]) or any(v in s for v in ACTION_VERBS):
            act.append(s)
        elif "?" in s or "í™•ì¸" in s:
            ask.append(s if s.endswith("?") else (s + " ë§ìŠµë‹ˆê¹Œ?"))
        else:
            misc.append(s)

    # í˜„ì¥ ì ìš© í¬ì¸íŠ¸ ë¶€ì¡± ì‹œ ACTION ë³´ê°•
    if len(act) < 3:
        extra = [s for s in split_sentences_ko(text) if any(v in s for v in ACTION_VERBS)]
        for s in extra:
            s2 = soften(s)
            if s2 not in act:
                act.append(s2)
            if len(act) >= 5:
                break
    act = act[:5]

    # ì¡°ë¦½
    lines = []
    lines.append(f"ğŸ¦º TBM êµìœ¡ëŒ€ë³¸ â€“ {topic}\n")
    lines.append("â— ë„ì…")
    lines.append(f"ì˜¤ëŠ˜ì€ {topic}ì— ëŒ€í•´ ì´ì•¼ê¸°í•˜ê² ìŠµë‹ˆë‹¤. í˜„ì¥ì—ì„œ ìì£¼ ë°œìƒí•˜ì§€ë§Œ, ì˜¬ë°”ë¥¸ ì ˆì°¨ë§Œ ì§€í‚¤ë©´ ì˜ˆë°©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n")

    if case:
        lines.append("â— ì‚¬ê³  ì‚¬ë¡€")
        for c in case:
            lines.append(f"- {c}")
        lines.append("")

    if risk:
        lines.append("â— ì£¼ìš” ìœ„í—˜ìš”ì¸")
        for r in risk:
            lines.append(f"- {r}")
        lines.append("")

    if act:
        lines.append("â— ì˜ˆë°©ì¡°ì¹˜ / ì‹¤ì²œ ìˆ˜ì¹™")
        for i, a in enumerate(act, 1):
            lines.append(f"{i}ï¸âƒ£ {a}")
        lines.append("")

    if ask:
        lines.append("â— í˜„ì¥ ì ê²€ ì§ˆë¬¸")
        for q in ask:
            lines.append(f"- {q}")
        lines.append("")

    lines.append("â— ë§ˆë¬´ë¦¬ ë‹¹ë¶€")
    lines.append("ì•ˆì „ì€ í•œìˆœê°„ì˜ ê´€ì‹¬ì—ì„œ ì‹œì‘ë©ë‹ˆë‹¤. ì˜¤ëŠ˜ ì‘ì—… ì „ ì„œë¡œ í•œ ë²ˆ ë” í™•ì¸í•©ì‹œë‹¤.")
    lines.append("â— êµ¬í˜¸")
    lines.append("â€œí•œ ë²ˆ ë” í™•ì¸! í•œ ë²ˆ ë” ì ê²€!â€")
    return "\n".join(lines)

# ----------------------------
# DOCX ë‚´ë³´ë‚´ê¸° (XML ê¸ˆì§€ë¬¸ì í•„í„°)
# ----------------------------
_XML_FORBIDDEN = r"[\x00-\x08\x0B\x0C\x0E-\x1F\uD800-\uDFFF\uFFFE\uFFFF]"
def _xml_safe(s: str) -> str:
    if not isinstance(s, str):
        s = "" if s is None else str(s)
    return rxx.sub(_XML_FORBIDDEN, "", s)

def to_docx_bytes(script: str) -> bytes:
    doc = Document()
    # ê¸€ê¼´ì€ í™˜ê²½ì— ë”°ë¼ ê¸°ë³¸ì²´ê°€ ì ìš©ë  ìˆ˜ ìˆìŒ(ì„¤ì • ì‹¤íŒ¨ì‹œ ì˜ˆì™¸ ë¬´ì‹œ)
    try:
        style = doc.styles["Normal"]
        style.font.name = "Malgun Gothic"
        style.font.size = Pt(11)
    except Exception:
        pass
    for raw in script.split("\n"):
        line = _xml_safe(raw)
        p = doc.add_paragraph(line)
        for run in p.runs:
            try:
                run.font.name = "Malgun Gothic"
                run.font.size = Pt(11)
            except Exception:
                pass
    bio = io.BytesIO()
    doc.save(bio)
    bio.seek(0)
    return bio.read()

# ----------------------------
# UI (ê¸°ì¡´ êµ¬ì„± ìœ ì§€)
# ----------------------------
st.set_page_config(page_title="OPS2TBM", page_icon="ğŸ¦º", layout="wide")

with st.sidebar:
    st.header("â„¹ï¸ ì†Œê°œ / ì‚¬ìš©ë²•")
    st.markdown("""
**AI ê¸°ëŠ¥(ì™„ì „ ë¬´ë£Œ)**  
- ê²½ëŸ‰ TextRank(NumPy) + **MMR ë‹¤ì–‘ì„±**ìœ¼ë¡œ í•µì‹¬ ë¬¸ì¥ ì¶”ì¶œ  
- **ìì—°ìŠ¤ëŸ¬ìš´ êµìœ¡ëŒ€ë³¸(ë¬´ë£Œ)**: ê·œì¹™ ê¸°ë°˜ ë¬¸ì²´ ë³€í™˜/3ë¶„í˜• êµ¬ì„± â†’ ë‹´ë‹¹ìê°€ ì½ê¸° ì¢‹ì€ ë§í•˜ê¸° í†¤  
- **ì§§ì€ í—¤ë”/ìŠ¬ë¡œê±´ ì œê±°**, **í–‰ë™ ë™ì‚¬ ë¬¸ì¥ ìš°ì„ **

**ì´ˆê¸°í™”**  
- ìš°ìƒë‹¨ **ğŸ§¹ ì´ˆê¸°í™”** ë²„íŠ¼ â†’ ì—…ë¡œë“œ/ì„ íƒ/í…ìŠ¤íŠ¸ ë¦¬ì…‹
""")

st.title("ğŸ¦º OPS2TBM â€” OPS/í¬ìŠ¤í„°ë¥¼ êµìœ¡ ëŒ€ë³¸ìœ¼ë¡œ ìë™ ë³€í™˜ (ì™„ì „ ë¬´ë£Œ)")

def reset_all():
    st.session_state.pop("manual_text", None)
    st.session_state.pop("edited_text", None)
    st.session_state.pop("zip_choice", None)
    st.session_state.uploader_key += 1
    st.rerun()

col_top1, col_top2 = st.columns([4, 1])
with col_top2:
    st.button("ğŸ§¹ ì´ˆê¸°í™”", on_click=reset_all, use_container_width=True)

st.markdown("""
**ì•ˆë‚´**  
- í…ìŠ¤íŠ¸ê°€ í¬í•¨ëœ PDF ë˜ëŠ” ë³¸ë¬¸ í…ìŠ¤íŠ¸ë¥¼ ê¶Œì¥í•©ë‹ˆë‹¤.  
- ZIP ì—…ë¡œë“œ ì‹œ ë‚´ë¶€ì˜ PDFë“¤ì„ ìë™ ì¸ì‹í•˜ì—¬ ì„ íƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.  
- ì´ë¯¸ì§€/ìŠ¤ìº”í˜• PDFëŠ” í˜„ì¬ OCR ë¯¸ì§€ì›ì…ë‹ˆë‹¤.
""")

col1, col2 = st.columns([1, 1], gap="large")

# ì¢Œì¸¡: ì…ë ¥ (PDF/ZIP/í…ìŠ¤íŠ¸)
with col1:
    uploaded = st.file_uploader("OPS ì—…ë¡œë“œ (PDF ë˜ëŠ” ZIP) â€¢ í…ìŠ¤íŠ¸ PDF ê¶Œì¥",
                                type=["pdf", "zip"], key=f"uploader_{st.session_state.uploader_key}")
    manual_text = st.text_area("ë˜ëŠ” OPS í…ìŠ¤íŠ¸ ì§ì ‘ ë¶™ì—¬ë„£ê¸°", key="manual_text",
                               height=220, placeholder="ì˜ˆ: í­ì—¼ ì‹œ ì˜¨ì—´ì§ˆí™˜ ì˜ˆë°©ì„ ìœ„í•´â€¦")

    # ZIP ë‚´ë¶€ PDF ì„ íƒ
    zip_pdfs: Dict[str, bytes] = {}
    selected_zip_pdf = None
    if uploaded and uploaded.name.lower().endswith(".zip"):
        try:
            with zipfile.ZipFile(uploaded, "r") as zf:
                for name in zf.namelist():
                    if name.lower().endswith(".pdf"):
                        zip_pdfs[name] = zf.read(name)
        except Exception:
            st.error("ZIPì„ í•´ì œí•˜ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. íŒŒì¼ì„ í™•ì¸í•´ ì£¼ì„¸ìš”.")
        if zip_pdfs:
            selected_zip_pdf = st.selectbox("ZIP ë‚´ PDF ì„ íƒ", list(zip_pdfs.keys()), key="zip_choice")
        else:
            st.warning("ZIP ì•ˆì—ì„œ PDFë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")

    # í…ìŠ¤íŠ¸ ì¶”ì¶œ
    extracted = ""
    if uploaded and uploaded.name.lower().endswith(".pdf"):
        with st.spinner("PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ ì¤‘..."):
            extracted = read_pdf_text(uploaded.read())
    elif selected_zip_pdf:
        with st.spinner("ZIP ë‚´ë¶€ PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ ì¤‘..."):
            extracted = read_pdf_text(zip_pdfs[selected_zip_pdf])

    base_text = st.session_state.get("manual_text", "").strip() or extracted.strip()
    st.markdown("**ì¶”ì¶œ/ì…ë ¥ í…ìŠ¤íŠ¸ ë¯¸ë¦¬ë³´ê¸°**")
    # ğŸ”´ ì¤‘ìš”: ì´í›„ ìƒì„± ë¡œì§ì€ ë°˜ë“œì‹œ ì´ ê°’(st.session_state['edited_text'])ì„ ì‚¬ìš©
    edited_text = st.text_area("í…ìŠ¤íŠ¸", value=base_text, height=240, key="edited_text")

# ìš°ì¸¡: ì˜µì…˜/ìƒì„±/ë‹¤ìš´ë¡œë“œ (UI ìœ ì§€)
with col2:
    use_ai = st.toggle("ğŸ”¹ AI ìš”ì•½(TextRank+MMR) ì‚¬ìš©", value=True)
    tmpl_choice = st.selectbox("ğŸ§© í…œí”Œë¦¿", ["ìë™ ì„ íƒ", "ì‚¬ê³ ì‚¬ë¡€í˜•", "ê°€ì´ë“œí˜•"])
    gen_mode = st.selectbox("ğŸ§  ìƒì„± ëª¨ë“œ", ["TBM ê¸°ë³¸(í˜„í–‰)", "ìì—°ìŠ¤ëŸ¬ìš´ êµìœ¡ëŒ€ë³¸(ë¬´ë£Œ)"])
    max_points = st.slider("ìš”ì•½ ê°•ë„(í•µì‹¬ë¬¸ì¥ ê°œìˆ˜)", 3, 10, 6)

    if st.button("ğŸ› ï¸ ëŒ€ë³¸ ìƒì„±", type="primary", use_container_width=True):
        # ğŸ”´ ì…ë ¥ì€ ë°˜ë“œì‹œ ì„¸ì…˜ì˜ edited_textë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì‚¬ìš© (UIì™€ ìƒì„± ë¡œì§ ì¼ê´€ì„± ë³´ì¥)
        text_for_gen = (st.session_state.get("edited_text") or "").strip()
        if not text_for_gen:
            st.warning("í…ìŠ¤íŠ¸ê°€ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤. PDF/ZIP ì—…ë¡œë“œ ë˜ëŠ” í…ìŠ¤íŠ¸ ì…ë ¥ í›„ ì‹œë„í•˜ì„¸ìš”.")
        else:
            # í…œí”Œë¦¿ ìë™/ìˆ˜ë™ ê²°ì • (í˜„ì¬ëŠ” 3ë¶„í˜• êµ¬ì¡°ì™€ ë¬´ê´€í•˜ì§€ë§Œ, ê¸°ë³¸ ëª¨ë“œì—ì„œ ì‚¬ìš©)
            if tmpl_choice == "ìë™ ì„ íƒ":
                detected = "ê°€ì´ë“œí˜•" if ("ê°€ì´ë“œ" in text_for_gen or "ì•ˆë‚´" in text_for_gen) else "ì‚¬ê³ ì‚¬ë¡€í˜•"
            else:
                detected = tmpl_choice

            with st.spinner("ëŒ€ë³¸ ìƒì„± ì¤‘..."):
                if gen_mode == "ìì—°ìŠ¤ëŸ¬ìš´ êµìœ¡ëŒ€ë³¸(ë¬´ë£Œ)":
                    # âœ… 3ë¶„í˜• êµ¬ì¡° ìë™ ìƒì„± (ë„ì…~êµ¬í˜¸)
                    script = make_structured_script(text_for_gen, max_points=max_points)
                    subtitle = f"{detected} Â· ìì—°ìŠ¤ëŸ¬ìš´ êµìœ¡ëŒ€ë³¸(ë¬´ë£Œ)"
                else:
                    # âœ… TBM ê¸°ë³¸(í˜„í–‰): í•µì‹¬ ë¬¸ì¥ ë‚˜ì—´ (ê°„ë‹¨/ì•ˆì •)
                    sents = ai_extract_summary(text_for_gen, max_points if use_ai else 6)
                    script = "\n".join([f"- {soften(s)}" for s in sents]) if sents else "í…ìŠ¤íŠ¸ì—ì„œ í•µì‹¬ ë¬¸ì¥ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."
                    subtitle = f"{detected} Â· TBM ê¸°ë³¸"

            st.success(f"ëŒ€ë³¸ ìƒì„± ì™„ë£Œ! ({subtitle})")
            st.text_area("ëŒ€ë³¸ ë¯¸ë¦¬ë³´ê¸°", value=script, height=420)

            c3, c4 = st.columns(2)
            with c3:
                st.download_button("â¬‡ï¸ TXT ë‹¤ìš´ë¡œë“œ", data=_xml_safe(script).encode("utf-8"),
                                   file_name="tbm_script.txt", use_container_width=True)
            with c4:
                st.download_button("â¬‡ï¸ DOCX ë‹¤ìš´ë¡œë“œ", data=to_docx_bytes(script),
                                   file_name="tbm_script.docx", use_container_width=True)

st.caption("í˜„ì¬: ì™„ì „ ë¬´ë£Œ. TextRank(ê²½ëŸ‰ AI) + ê·œì¹™ ê¸°ë°˜ NLG. í…œí”Œë¦¿ ìë™/ìˆ˜ë™. ZIP/PDF/í…ìŠ¤íŠ¸ ì§€ì›. OCR ë¯¸ì§€ì›. â€˜ìì—°ìŠ¤ëŸ¬ìš´ êµìœ¡ëŒ€ë³¸(ë¬´ë£Œ)â€™ì—ì„œ 3ë¶„í˜• êµ¬ì¡° ìë™ ìƒì„±. DOCX íŠ¹ìˆ˜ë¬¸ì í•„í„° ì ìš©.")
