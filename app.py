# ==========================================================
# OPS2TBM â€” ì•ˆì „ êµìœ¡ ë° ì ê²€ ì¼ì§€ ë°˜ì˜ (TBM ë° ì¼ì¼ì•ˆì „êµìœ¡ ì¼ì§€ í™•ì¥)
#  - ê¸°ì¡´ UI/ë ˆì´ì•„ì›ƒ ìœ ì§€, ë¬¸ì„œì— ì¶”ê°€ í•„ë“œ í¬í•¨
# ==========================================================

import io, zipfile, re
from collections import Counter
from typing import List, Dict
import streamlit as st
from docx import Document
from docx.shared import Pt
from pdfminer.high_level import extract_text as pdf_extract_text  # â† ì•ˆì • ê²½ë¡œ
import pypdfium2 as pdfium
import numpy as np
import regex as rxx

# ----------------------------
# ì„¸ì…˜ ìƒíƒœ
# ----------------------------
# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™” (ì—…ë¡œë“œí•œ íŒŒì¼ì˜ í‚¤ê°’, ìš©ì–´/í–‰ë™/ì§ˆë¬¸ ìš©ì–´ ì§‘í•©ì„ ì €ì¥)
if "uploader_key" not in st.session_state:
    st.session_state.uploader_key = 0
if "kb_terms" not in st.session_state:
    st.session_state.kb_terms: Counter = Counter()
if "kb_actions" not in st.session_state:
    st.session_state.kb_actions: List[str] = []
if "kb_questions" not in st.session_state:
    st.session_state.kb_questions: List[str] = []

# ==========================================================
# ì „ì²˜ë¦¬ ë° í…ìŠ¤íŠ¸ ì •ë¦¬
# ==========================================================
# ì¡ìŒ íŒ¨í„´(ë¶ˆí•„ìš”í•œ í—¤ë”, ë§í¬ ë“±ì„ ì œê±°í•˜ê¸° ìœ„í•œ ì •ê·œì‹)
NOISE_PATTERNS = [
    r"^ì œ?\s?\d{4}\s?[-.]?\s?\d+\s?í˜¸$",
    r"^(ë™ì ˆê¸°\s*ì£¼ìš”ì‚¬ê³ |ì•ˆì „ì‘ì—…ë°©ë²•|ì½˜í…ì¸ ë§í¬|ì±…ì\s*OPS|ìˆí¼\s*OPS)$",
    r"^(í¬ìŠ¤í„°|ì±…ì|ìŠ¤í‹°ì»¤|ì½˜í…ì¸  ë§í¬)$",
    r"^(ìŠ¤ë§ˆíŠ¸í°\s*APP|ì¤‘ëŒ€ì¬í•´\s*ì‚¬ì´ë Œ|ì‚°ì—…ì•ˆì „í¬í„¸|ê³ ìš©ë…¸ë™ë¶€)$",
    r"^https?://\S+$",
    r"^\(?\s*PowerPoint\s*í”„ë ˆì  í…Œì´ì…˜\s*\)?$",
    r"^ì•ˆì „ë³´ê±´ìë£Œì‹¤.*$",
]
# ë¶ˆë¦¿í¬ì¸íŠ¸ ë¬¸ì íŒ¨í„´
BULLET_PREFIX = r"^[\s\-\â€¢\â—\â–ª\â–¶\â–·\Â·\*\u25CF\u25A0\u25B6\u25C6\u2022\u00B7\u279C\u27A4\u25BA\u25AA\u25AB\u2611\u2713\u2714\u2716\u2794\u27A2]+"
# ë‚ ì§œ íŒ¨í„´
DATE_PAT = r"(\d{4})\.\s?(\d{1,2})\.\s?(\d{1,2})\.?"
# ì‚¬ê³ ì‚¬ë¡€ì—ì„œ ë‚˜íƒ€ë‚  ìˆ˜ ìˆëŠ” íŒ¨í„´
META_PATTERNS = [r"<\s*\d+\s*ëª…\s*ì‚¬ë§\s*>", r"<\s*\d+\s*ëª…\s*ì‚¬ìƒ\s*>", r"<\s*\d+\s*ëª…\s*ì˜ì‹ë¶ˆëª…\s*>"]
# ë¶ˆìš©ì–´(êµ¬ë¶„ëœ ìš©ì–´ë¡œë¶€í„° ì œê±°í•  ë‹¨ì–´ë“¤)
STOP_TERMS = set([
    "ë°","ë“±","ê´€ë ¨","ì‚¬í•­","ë‚´ìš©","ì˜ˆë°©","ì•ˆì „","ì‘ì—…","í˜„ì¥","êµìœ¡","ë°©ë²•","ê¸°ì¤€","ì¡°ì¹˜",
    "ì‹¤ì‹œ","í™•ì¸","í•„ìš”","ê²½ìš°","ëŒ€ìƒ","ì‚¬ìš©","ê´€ë¦¬","ì ê²€","ì ìš©","ì •ë„","ì£¼ì˜","ì¤‘","ì „","í›„",
    "ì£¼ìš”","ì‚¬ë¡€","ì•ˆì „ì‘ì—…ë°©ë²•","í¬ìŠ¤í„°","ë™ì˜ìƒ","ë¦¬í”Œë¦¿","ê°€ì´ë“œ","ìë£Œì‹¤","ê²€ìƒ‰",
])

def normalize_text(t: str) -> str:
    """í…ìŠ¤íŠ¸ì—ì„œ ë¶ˆí•„ìš”í•œ ê³µë°±ê³¼ ì¤„ ë°”ê¿ˆì„ ì •ë¦¬"""
    t = t.replace("\x0c","\n")
    t = re.sub(r"[ \t]+\n","\n",t)
    t = re.sub(r"\n{3,}","\n\n",t)
    return t.strip()

def strip_noise_line(line: str) -> str:
    """ë¼ì¸ì—ì„œ ë¶ˆí•„ìš”í•œ ë¬¸ìë¥¼ ì œê±°í•˜ê³  í•„ìš”í•œ ë¬¸ì¥ë§Œ ë°˜í™˜"""
    s = (line or "").strip()
    if not s: return ""
    s = re.sub(BULLET_PREFIX,"",s).strip()
    for pat in NOISE_PATTERNS:
        if re.match(pat,s,re.IGNORECASE):
            return ""
    s = re.sub(r"https?://\S+","",s).strip()
    s = s.strip("â€¢â—â–ªâ–¶â–·Â·-â€”â€“")
    return s

def merge_broken_lines(lines: List[str]) -> List[str]:
    """ì¤„ë°”ê¿ˆì´ ì˜ëª»ëœ ë¬¸ì¥ë“¤ì„ ë³‘í•©"""
    out=[]; buf=""
    for raw in lines:
        s = strip_noise_line(raw)
        if not s: continue
        s = s.lstrip("â†³").strip()
        if buf and (buf.endswith((":", "Â·", "â€¢", "â–ª", "â–¶", "â–·")) or re.match(r"^\(.*\)$", buf)):
            buf += " " + s
        else:
            if buf: out.append(buf)
            buf = s
    if buf: out.append(buf)
    return out

def combine_date_with_next(lines: List[str]) -> List[str]:
    """ë‚ ì§œì™€ ë‹¤ìŒ ë¬¸ì¥ì„ í•©ì³ì„œ ì‚¬ê±´ì„ ë¬¶ì–´ì£¼ëŠ” í•¨ìˆ˜"""
    out=[]; i=0
    while i<len(lines):
        cur=strip_noise_line(lines[i])
        if re.search(DATE_PAT,cur) and (i+1)<len(lines):
            nxt=strip_noise_line(lines[i+1])
            if re.search(r"(ì‚¬ë§|ì‚¬ìƒ|ì‚¬ê³ |ì¤‘ë…|í™”ì¬|ë¶•ê´´|ì§ˆì‹|ì¶”ë½|ê¹”ë¦¼|ë¶€ë”ªí˜|ë¬´ë„ˆì§)",nxt):
                m=re.search(DATE_PAT,cur); y,mo,d=m.groups()
                out.append(f"{int(y)}ë…„ {int(mo)}ì›” {int(d)}ì¼, {nxt}")
                i+=2; continue
        out.append(cur); i+=1
    return out

# ==========================================================
# PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ (í…ìŠ¤íŠ¸í˜•/ì´ë¯¸ì§€í˜• íŒë‹¨)
# ==========================================================
def read_pdf_text(b: bytes) -> str:
    """PDF íŒŒì¼ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
    try:
        with io.BytesIO(b) as bio:
            t = pdf_extract_text(bio) or ""
    except Exception:
        t = ""
    t = normalize_text(t)
    if len(t.strip())<10:
        try:
            with io.BytesIO(b) as bio:
                pdf = pdfium.PdfDocument(bio)
                if len(pdf)>0 and not t.strip():
                    st.warning("âš ï¸ ì´ë¯¸ì§€/ìŠ¤ìº” PDFë¡œ ë³´ì…ë‹ˆë‹¤. í˜„ì¬ OCR ë¯¸ì§€ì›.")
        except Exception:
            pass
    return t

# ==========================================================
# ìš”ì•½(TextRank+MMR) â€” ì„¸ì…˜KB ìš©ì–´ ê°€ì¤‘ì¹˜ ë°˜ì˜
# ==========================================================
def tokens(s: str) -> List[str]:
    """ê°„ë‹¨ í† í°í™”(í•œê¸€/ì˜ë¬¸/ìˆ«ì 2ì ì´ìƒ)"""
    return rxx.findall(r"[ê°€-í£a-z0-9]{2,}", s.lower())

def sentence_tfidf_vectors(sents: List[str], kb_boost: Dict[str,float]=None):
    """ë¬¸ì¥ TF-IDF + KB ìš©ì–´ ê°€ì¤‘ì¹˜"""
    toks=[tokens(s) for s in sents]
    vocab={}
    for ts in toks:
        for t in ts:
            if t not in vocab: vocab[t]=len(vocab)
    if not vocab: return np.zeros((len(sents),0),dtype=np.float32), []
    M=np.zeros((len(sents),len(vocab)),dtype=np.float32)
    df=np.zeros((len(vocab),),dtype=np.float32)
    for i,ts in enumerate(toks):
        for t in ts:
            w = 1.0
            if kb_boost and t in kb_boost:
                w *= kb_boost[t]
            M[i,vocab[t]] += w
        for t in set(ts):
            df[vocab[t]] += 1.0
    N=float(len(sents))
    idf=np.log((N+1.0)/(df+1.0))+1.0
    M*=idf
    if kb_boost:
        for t,idx in vocab.items():
            if t in kb_boost:
                M[:,idx] *= (1.0 + 0.2*kb_boost[t])  # ì‚´ì§ ì¶”ê°€ ë¶€ìŠ¤íŠ¸
    M/= (np.linalg.norm(M,axis=1,keepdims=True)+1e-8)
    return M, list(vocab.keys())

def cosim(X: np.ndarray)->np.ndarray:
    """ë¬¸ì¥ ê°„ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°"""
    if X.size==0: return np.zeros((X.shape[0],X.shape[0]),dtype=np.float32)
    S=np.clip(X@X.T,0.0,1.0); np.fill_diagonal(S,0.0); return S

def textrank_scores(sents: List[str], X: np.ndarray, d:float=0.85, max_iter:int=60, tol:float=1e-4)->List[float]:
    """TextRank ì ìˆ˜ ê³„ì‚°(ìœ ì‚¬ë„ í–‰ë ¬ ê¸°ë°˜)"""
    n=len(sents)
    if n==0: return []
    W=cosim(X)
    row=W.sum(axis=1,keepdims=True); P=np.divide(W,row,out=np.zeros_like(W),where=row>0)
    r=np.ones((n,1),dtype=np.float32)/n; tel=np.ones((n,1),dtype=np.float32)/n
    for _ in range(max_iter):
        r2=d*(P.T@r)+(1-d)*tel
        if np.linalg.norm(r2-r,1)<tol: r=r2; break
        r=r2
    return [float(v) for v in r.flatten()]

def mmr_select(sents, scores, X, k:int, lam:float=0.7)->List[int]:
    """MMRë¡œ ì •ë³´ì„±/ë¹„ì¤‘ë³µì„± ê· í˜•"""
    S=cosim(X); sel=[]; rem=set(range(len(sents)))
    while rem and len(sel)<k:
        best, val=None,-1e9
        for i in rem:
            rel=scores[i]; div=max((S[i,j] for j in sel), default=0.0)
            sc=lam*rel-(1-lam)*div
            if sc>val: val, best=sc, i
        sel.append(best); rem.remove(best)
    return sel

def ai_extract_summary(text:str, limit:int=8)->List[str]:
    """ì „ì²˜ë¦¬ ë¬¸ì¥ â†’ KBê°€ì¤‘ ìš”ì•½ ë¬¸ì¥ top-k"""
    sents=preprocess_text_to_sentences(text)
    if not sents: return []
    kb = st.session_state.kb_terms
    total = sum(kb.values()) or 1
    kb_boost = {t: 1.0 + (cnt/total)*3.0 for t,cnt in kb.items()} if kb else None
    X,_=sentence_tfidf_vectors(sents, kb_boost=kb_boost)
    scores=textrank_scores(sents, X)
    idx=mmr_select(sents,scores,X,limit,lam=0.7)
    return [sents[i] for i in idx]

# ==========================================================
# ë™ì  ì£¼ì œ ë¼ë²¨ â€” ë¬¸ì„œ+KB ìƒìœ„ í•µì‹¬ì–´ ì¡°í•©
# ==========================================================
def top_terms_for_label(text: str, k:int=3) -> List[str]:
    doc_cnt = Counter([t for t in tokens(text) if t not in STOP_TERMS])
    kb = st.session_state.kb_terms
    if kb:
        for t, c in kb.items():
            if t in STOP_TERMS: continue
            doc_cnt[t] += 0.2 * c  # KBëŠ” ì•½í•˜ê²Œ í•©ì‚°
    if not doc_cnt: return ["ì•ˆì „ë³´ê±´","êµìœ¡"]
    commons = {"ì•ˆì „","êµìœ¡","ì‘ì—…","í˜„ì¥","ì˜ˆë°©","ì¡°ì¹˜","í™•ì¸","ê´€ë¦¬","ì ê²€"}
    cand = [(t,doc_cnt[t]) for t in doc_cnt if t not in commons and len(t)>=2]
    if not cand: cand = list(doc_cnt.items())
    cand.sort(key=lambda x:x[1], reverse=True)
    return [t for t,_ in cand[:k]]

def dynamic_topic_label(text: str) -> str:
    return " Â· ".join(top_terms_for_label(text, k=3))

# ==========================================================
# ë¦¬ë¼ì´íŒ… â€” ì‚¬ê±´/í–‰ë™/ì§ˆë¬¸
# ==========================================================
ACTION_VERBS = [
    "ì„¤ì¹˜","ë°°ì¹˜","ì°©ìš©","ì ê²€","í™•ì¸","ì¸¡ì •","ê¸°ë¡","í‘œì‹œ","ì œê³µ","ë¹„ì¹˜",
    "ë³´ê³ ","ì‹ ê³ ","êµìœ¡","ì£¼ì§€","ì¤‘ì§€","í†µì œ","íœ´ì‹","í™˜ê¸°","ì°¨ë‹¨","êµëŒ€","ë°°ì œ","ë°°ë ¤",
    "ê°€ë™","ì¤€ìˆ˜","ìš´ì˜","ìœ ì§€","êµì²´","ì •ë¹„","ì²­ì†Œ","ê³ ì •","ê²©ë¦¬","ë³´í˜¸","ë³´ìˆ˜"
]

ACTION_PAT = r"(?P<obj>[ê°€-í£a-zA-Z0-9Â·\(\)\[\]\/\-\s]{2,}?)\s*(?P<verb>" + "|".join(ACTION_VERBS) + r"|ì‹¤ì‹œ|ìš´ì˜|ê´€ë¦¬)\b|(?P<obj2>[ê°€-í£a-zA-Z0-9Â·\(\)\[\]\/\-\s]{2,}?)\s*(ì„|ë¥¼)\s*(?P<verb2>" + "|".join(ACTION_VERBS) + r"|ì‹¤ì‹œ|ìš´ì˜|ê´€ë¦¬)\b"

def soften(s:str)->str:
    """ëª…ë ¹ì¡°/ì„œìˆ ì¡°ë¥¼ ì™„ê³¡í•œ ì¡´ëŒ“ë§ë¡œ"""
    s = s.replace("í•˜ì—¬ì•¼","í•´ì•¼ í•©ë‹ˆë‹¤").replace("í•œë‹¤","í•©ë‹ˆë‹¤").replace("í•œë‹¤.","í•©ë‹ˆë‹¤.")
    s = s.replace("ë°”ëë‹ˆë‹¤","í•´ì£¼ì„¸ìš”").replace("í™•ì¸ ë°”ëŒ","í™•ì¸í•´ì£¼ì„¸ìš”")
    s = s.replace("ê¸ˆì§€í•œë‹¤","ê¸ˆì§€í•©ë‹ˆë‹¤").replace("í•„ìš”í•˜ë‹¤","í•„ìš”í•©ë‹ˆë‹¤")
    s = re.sub(r"^\(([^)]+)\)\s*","",s)
    for pat in META_PATTERNS:
        s = re.sub(pat,"",s).strip()
    s = re.sub(BULLET_PREFIX,"",s).strip(" -â€¢â—\t")
    return s

def naturalize_case_sentence(s:str)->str:
    """ì‚¬ê±´ë¬¸ í˜•ì‹(YYYY.MM.DD + ë‚´ìš©) â†’ ìì—°ì–´"""
    s = soften(s)
    m=re.search(DATE_PAT,s)
    date_txt=""
    if m:
        y,mo,d=m.groups()
        date_txt=f"{int(y)}ë…„ {int(mo)}ì›” {int(d)}ì¼, "
        s = s.replace(m.group(0),"").strip()
    s = s.strip(" ,.-")
    if not re.search(r"(ë‹¤\.|ì…ë‹ˆë‹¤\.|í–ˆìŠµë‹ˆë‹¤\.|ë°œìƒí–ˆìŠµë‹ˆë‹¤\.)$",s):
        s = s.rstrip(" .") + " ì‚¬ê³ ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
    return (date_txt+s).strip()

def to_action_sentence(s:str)->str:
    """'ëª…ì‚¬êµ¬ + ë™ì‚¬'ë¥¼ ì¡´ëŒ“ë§ ì§€ì‹œí˜•ìœ¼ë¡œ ë³€í™˜"""
    s2 = soften(s)
    m = re.search(ACTION_PAT, s2)
    if not m:
        return s2 if s2.endswith(("ë‹ˆë‹¤.","í•©ë‹ˆë‹¤.","ë‹¤.")) else (s2.rstrip(" .") + " í•©ë‹ˆë‹¤.")
    obj = (m.group("obj") or m.group("obj2") or "").strip()
    verb = (m.group("verb") or m.group("verb2") or "ì‹¤ì‹œ").strip()
    if obj and not re.search(r"(ì„|ë¥¼|ì—|ì—ì„œ|ê³¼|ì™€|ì˜)$", obj):
        obj += "ë¥¼"
    return f"{obj} {verb}í•©ë‹ˆë‹¤." if obj else (s2 if s2.endswith(("ë‹ˆë‹¤.","í•©ë‹ˆë‹¤.","ë‹¤.")) else (s2.rstrip(" .")+" í•©ë‹ˆë‹¤."))

# ==========================================================
# KB êµ¬ì¶•/í™œìš© â€” ì—…ë¡œë“œ/ë¶™ì—¬ë„£ê¸° ì‹œ ìë™ ëˆ„ì 
# ==========================================================
def kb_ingest_text(text: str):
    """ë¬¸ì„œì—ì„œ ìš©ì–´/í–‰ë™/ì§ˆë¬¸ì„ ì¶”ì¶œí•˜ì—¬ ì„¸ì…˜ KBì— ëˆ„ì """
    sents = preprocess_text_to_sentences(text)
    for s in sents:
        for t in tokens(s):
            if len(t)>=2:
                st.session_state.kb_terms[t]+=1
    for s in sents:
        if re.search(ACTION_PAT, s):
            cand = to_action_sentence(s)
            if len(cand)<=100:   # ë„ˆë¬´ ê¸´ ë¬¸ì¥ ì œì™¸
                st.session_state.kb_actions.append(cand)
    for s in sents:
        if "?" in s or "í™•ì¸" in s or "ì ê²€" in s:
            q = soften(s if s.endswith("?") else s + " ë§ìŠµë‹ˆê¹Œ?")
            if len(q)<=100:
                st.session_state.kb_questions.append(q)

def kb_prune():
    """ì¤‘ë³µ ì œê±° ë° ìƒí•œ ì ìš©(ì„¸ì…˜ ë©”ëª¨ë¦¬ ê³¼ë„ ì‚¬ìš© ë°©ì§€)"""
    def dedup_keep_order(lst: List[str]) -> List[str]:
        seen=set(); out=[]
        for x in lst:
            k=re.sub(r"\s+","",x)
            if k not in seen:
                seen.add(k); out.append(x)
        return out
    st.session_state.kb_actions = dedup_keep_order(st.session_state.kb_actions)[:500]
    st.session_state.kb_questions = dedup_keep_order(st.session_state.kb_questions)[:300]
    st.session_state.kb_terms = Counter(dict(st.session_state.kb_terms.most_common(1000)))

def kb_match_candidates(cands: List[str], base_text: str, limit:int) -> List[str]:
    """í˜„ì¬ ë¬¸ì„œ í† í° êµì§‘í•© ê¸°ë°˜ìœ¼ë¡œ KB í›„ë³´ ì„ ë³„(ë„ë©”ì¸ ì¼ì¹˜ë„â†‘)"""
    bt = set(tokens(base_text))
    scored=[]
    for c in cands:
        ct=set(tokens(c))
        j = len(bt & ct) / (len(bt | ct)+1e-8)
        if j>0:
            scored.append((j, c))
    scored.sort(key=lambda x:x[0], reverse=True)
    return [c for _,c in scored[:limit]]

# ==========================================================
# ëŒ€ë³¸ ìƒì„± â€” ë„ì…/ì‚¬ê³ /ìœ„í—˜/í–‰ë™/ì§ˆë¬¸/ë§ˆë¬´ë¦¬
# ==========================================================
def make_structured_script(text:str, max_points:int=6)->str:
    topic_label = dynamic_topic_label(text)  # ë™ì  ì£¼ì œ ë¼ë²¨
    core = [soften(s) for s in ai_extract_summary(text, max_points)]
    if not core:
        return "ë³¸ë¬¸ì´ ì¶©ë¶„í•˜ì§€ ì•Šì•„ ëŒ€ë³¸ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

    case, risk, act, ask = [], [], [], []
    for s in core:
        if re.search(DATE_PAT, s) or re.search(r"(ì‚¬ë§|ì‚¬ìƒ|ì‚¬ë¡€|ì‚¬ê³ |ì˜ì‹ë¶ˆëª…|ì¤‘ë…|ë¶•ê´´|ì§ˆì‹|ì¶”ë½|ê¹”ë¦¼|ë¶€ë”ªí˜|ë¬´ë„ˆì§)", s):
            case.append(naturalize_case_sentence(s))
        elif any(k in s for k in ["ìœ„í—˜","ìš”ì¸","ì›ì¸","ì¦ìƒ","ê²°ë¹™","ê°•í’","í™”ì¬","ì¤‘ë…","ì§ˆì‹","ë¯¸ì„¸ë¨¼ì§€","íšŒì „ì²´","ë¹„ì‚°","ë§ë¦¼"]):
            risk.append(soften(s))
        elif re.search(ACTION_PAT, s) or any(v in s for v in ACTION_VERBS):
            act.append(to_action_sentence(s))
        elif "?" in s or "í™•ì¸" in s or "ì ê²€" in s:
            ask.append(soften(s if s.endswith("?") else s + " ë§ìŠµë‹ˆê¹Œ?"))

    # ë¶€ì¡±ë¶„ì€ KBì—ì„œ ë™ì¼ ë„ë©”ì¸ í›„ë³´ë¡œ ë³´ê°•
    if len(act) < 5 and st.session_state.kb_actions:
        act += kb_match_candidates(st.session_state.kb_actions, text, 5 - len(act))
    act = act[:5]

    if not ask and st.session_state.kb_questions:
        ask = kb_match_candidates(st.session_state.kb_questions, text, 3)
    if not ask:
        ask = ["í•„ìš”í•œ ì•ˆì „ì¡°ì¹˜ê°€ ì˜¤ëŠ˜ ì‘ì—… ë²”ìœ„ì— ë§ê²Œ ì¤€ë¹„ë˜ì–´ ìˆìŠµë‹ˆê¹Œ?"]

    lines=[]
    lines.append(f"ğŸ¦º TBM êµìœ¡ëŒ€ë³¸ â€“ {topic_label}\n")
    lines.append("â— ë„ì…"); lines.append(f"ì˜¤ëŠ˜ì€ {topic_label}ì˜ í•µì‹¬ì„ ì§šì–´ë³´ê² ìŠµë‹ˆë‹¤.\n")
    if case:
        lines.append("â— ì‚¬ê³  ì‚¬ë¡€")
        for c in case: lines.append(f"- {c}")
        lines.append("")
    if risk:
        lines.append("â— ì£¼ìš” ìœ„í—˜ìš”ì¸")
        for r in risk: lines.append(f"- {r}")
        lines.append("")
    if act:
        lines.append("â— ì˜ˆë°©ì¡°ì¹˜ / ì‹¤ì²œ ìˆ˜ì¹™")
        for i,a in enumerate(act,1): lines.append(f"{i}ï¸âƒ£ {a}")
        lines.append("")
    if ask:
        lines.append("â— í˜„ì¥ ì ê²€ ì§ˆë¬¸")
        for q in ask: lines.append(f"- {q}")
        lines.append("")
    lines.append("â— ë§ˆë¬´ë¦¬ ë‹¹ë¶€")
    lines.append("ì•ˆì „ì€ í•œìˆœê°„ì˜ ê´€ì‹¬ì—ì„œ ì‹œì‘ë©ë‹ˆë‹¤. ì˜¤ëŠ˜ ì‘ì—… ì „ ì„œë¡œ í•œ ë²ˆ ë” í™•ì¸í•©ì‹œë‹¤.")
    lines.append("â— êµ¬í˜¸"); lines.append("â€œí•œ ë²ˆ ë” í™•ì¸! í•œ ë²ˆ ë” ì ê²€!â€")
    return "\n".join(lines)

# ==========================================================
# DOCX (íŠ¹ìˆ˜ë¬¸ì í•„í„°)
# ==========================================================
_XML_FORBIDDEN = r"[\x00-\x08\x0B\x0C\x0E-\x1F\uD800-\uDFFF\uFFFE\uFFFF]"
def _xml_safe(s:str)->str:
    if not isinstance(s,str): s = "" if s is None else str(s)
    return rxx.sub(_XML_FORBIDDEN,"",s)

def to_docx_bytes(script:str)->bytes:
    doc=Document()
    try:
        style=doc.styles["Normal"]; style.font.name="Malgun Gothic"; style.font.size=Pt(11)
    except Exception: pass
    for raw in script.split("\n"):
        line=_xml_safe(raw)
        p=doc.add_paragraph(line)
        for run in p.runs:
            try: run.font.name="Malgun Gothic"; run.font.size=Pt(11)
            except Exception: pass
    bio=io.BytesIO(); doc.save(bio); bio.seek(0); return bio.read()

# ==========================================================
# UI (ê·¸ëŒ€ë¡œ ìœ ì§€)
# ==========================================================
st.set_page_config(page_title="OPS2TBM", page_icon="ğŸ¦º", layout="wide")

with st.sidebar:
    st.header("â„¹ï¸ ì†Œê°œ / ì‚¬ìš©ë²•")
    st.markdown("""
**AI íŒŒì´í”„ë¼ì¸(ì™„ì „ ë¬´ë£Œ)**  
- ì „ì²˜ë¦¬ â†’ TextRank+MMR ìš”ì•½(ì„¸ì…˜ KB ê°€ì¤‘) â†’ ë°ì´í„° ê¸°ë°˜ ë¦¬ë¼ì´íŒ…  
- PDF/ZIP/í…ìŠ¤íŠ¸ ëª¨ë‘ ì˜¬ë¦¬ë©´ ì¦‰ì‹œ ëˆ„ì  í•™ìŠµë©ë‹ˆë‹¤(ìš©ì–´/í–‰ë™/ì§ˆë¬¸).
""")

st.title("ğŸ¦º OPS/í¬ìŠ¤í„°ë¥¼ êµìœ¡ ëŒ€ë³¸ìœ¼ë¡œ ìë™ ë³€í™˜ (ì™„ì „ ë¬´ë£Œ)")

def reset_all():
    """ì´ˆê¸°í™”(ì„¸ì…˜ KB/ì…ë ¥/ì—…ë¡œë” í‚¤)"""
    st.session_state.pop("manual_text", None)
    st.session_state.pop("edited_text", None)
    st.session_state.pop("zip_choice", None)
    st.session_state.kb_terms = Counter()
    st.session_state.kb_actions = []
    st.session_state.kb_questions = []
    st.session_state.uploader_key += 1
    st.rerun()

col_top1, col_top2 = st.columns([4,1])
with col_top2: st.button("ğŸ§¹ ì´ˆê¸°í™”", on_click=reset_all, use_container_width=True)

st.markdown("""
**ì•ˆë‚´**  
- í…ìŠ¤íŠ¸ê°€ í¬í•¨ëœ PDF ë˜ëŠ” ë³¸ë¬¸ í…ìŠ¤íŠ¸ë¥¼ ê¶Œì¥í•©ë‹ˆë‹¤.  
- ì´ë¯¸ì§€/ìŠ¤ìº” PDFëŠ” í˜„ì¬ OCR ë¯¸ì§€ì›ì…ë‹ˆë‹¤.
""")

col1, col2 = st.columns([1,1], gap="large")

with col1:
    uploaded = st.file_uploader("OPS ì—…ë¡œë“œ (PDF ë˜ëŠ” ZIP) â€¢ í…ìŠ¤íŠ¸ PDF ê¶Œì¥",
                                type=["pdf","zip"], key=f"uploader_{st.session_state.uploader_key}")
    manual_text = st.text_area("ë˜ëŠ” OPS í…ìŠ¤íŠ¸ ì§ì ‘ ë¶™ì—¬ë„£ê¸°", key="manual_text",
                               height=220, placeholder="ì˜ˆ: í˜„ì¥ ì•ˆë‚´ë¬¸ ë˜ëŠ” OPS ë³¸ë¬¸ í…ìŠ¤íŠ¸â€¦")

    extracted=""
    zip_pdfs: Dict[str,bytes] = {}
    selected_zip_pdf=None

    # ZIP ì—…ë¡œë“œ ì‹œ: ë‚´ë¶€ ëª¨ë“  PDFë¥¼ í•™ìŠµ(KB) + ì„ íƒí•´ì„œ ë¯¸ë¦¬ë³´ê¸° ê°€ëŠ¥
    if uploaded and uploaded.name.lower().endswith(".zip"):
        try:
            with zipfile.ZipFile(uploaded,"r") as zf:
                for name in zf.namelist():
                    if name.lower().endswith(".pdf"):
                        data = zf.read(name)
                        zip_pdfs[name]=data
                        txt = read_pdf_text(data)
                        if txt.strip():
                            kb_ingest_text(txt)      # ZIP ì „ì²´ í•™ìŠµ
            kb_prune()
        except Exception:
            st.error("ZIP í•´ì œ ì˜¤ë¥˜. íŒŒì¼ì„ í™•ì¸í•´ ì£¼ì„¸ìš”.")
        if zip_pdfs:
            selected_zip_pdf = st.selectbox("ZIP ë‚´ PDF ì„ íƒ", list(zip_pdfs.keys()), key="zip_choice")

    # ë‹¨ì¼ PDF ì—…ë¡œë“œ ì‹œ: ì¦‰ì‹œ í…ìŠ¤íŠ¸ ì¶”ì¶œ + KB í•™ìŠµ
    if uploaded and uploaded.name.lower().endswith(".pdf"):
        with st.spinner("PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ ì¤‘..."):
            data = uploaded.read()
            extracted = read_pdf_text(data)
            if extracted.strip():
                kb_ingest_text(extracted)   # ğŸ”¹ ë‹¨ì¼ PDFë„ ì¦‰ì‹œ í•™ìŠµ
                kb_prune()
    elif selected_zip_pdf:
        with st.spinner("ZIP ë‚´ë¶€ PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ ì¤‘..."):
            extracted = read_pdf_text(zip_pdfs[selected_zip_pdf])

    # ë¶™ì—¬ë„£ê¸° í…ìŠ¤íŠ¸ë„ ê°€ë³ê²Œ í•™ìŠµ ë°˜ì˜
    pasted = (st.session_state.get("manual_text") or "").strip()
    if pasted:
        kb_ingest_text(pasted); kb_prune()

    base_text = pasted or extracted.strip()
    st.markdown("**ì¶”ì¶œ/ì…ë ¥ í…ìŠ¤íŠ¸ ë¯¸ë¦¬ë³´ê¸°**")
    edited_text = st.text_area("í…ìŠ¤íŠ¸", value=base_text, height=240, key="edited_text")

with col2:
    use_ai = st.toggle("ğŸ”¹ AI ìš”ì•½(TextRank+MMR) ì‚¬ìš©", value=True)
    tmpl_choice = st.selectbox("ğŸ§© í…œí”Œë¦¿", ["ìë™ ì„ íƒ","ì‚¬ê³ ì‚¬ë¡€í˜•","ê°€ì´ë“œí˜•"])  # í‘œì‹œë§Œ ìœ ì§€
    gen_mode = st.selectbox("ğŸ§  ìƒì„± ëª¨ë“œ", ["TBM ê¸°ë³¸(í˜„í–‰)","ìì—°ìŠ¤ëŸ¬ìš´ êµìœ¡ëŒ€ë³¸(ë¬´ë£Œ)"])
    max_points = st.slider("ìš”ì•½ ê°•ë„(í•µì‹¬ë¬¸ì¥ ê°œìˆ˜)", 3, 10, 6)

    if st.button("ğŸ› ï¸ ëŒ€ë³¸ ìƒì„±", type="primary", use_container_width=True):
        text_for_gen = (st.session_state.get("edited_text") or "").strip()
        if not text_for_gen:
            st.warning("í…ìŠ¤íŠ¸ê°€ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤. PDF/ZIP ì—…ë¡œë“œ ë˜ëŠ” í…ìŠ¤íŠ¸ ì…ë ¥ í›„ ì‹œë„í•˜ì„¸ìš”.")
        else:
            with st.spinner("ëŒ€ë³¸ ìƒì„± ì¤‘..."):
                if gen_mode == "ìì—°ìŠ¤ëŸ¬ìš´ êµìœ¡ëŒ€ë³¸(ë¬´ë£Œ)":
                    script = make_structured_script(text_for_gen, max_points=max_points)
                    subtitle = "ìì—°ìŠ¤ëŸ¬ìš´ êµìœ¡ëŒ€ë³¸(ë¬´ë£Œ)"
                else:
                    sents = ai_extract_summary(text_for_gen, max_points if use_ai else 6)
                    sents = [soften(s) for s in sents]
                    script = "\n".join([f"- {s}" for s in sents]) if sents else "í…ìŠ¤íŠ¸ì—ì„œ í•µì‹¬ ë¬¸ì¥ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."
                    subtitle = "TBM ê¸°ë³¸(í˜„í–‰)"

            st.success(f"ëŒ€ë³¸ ìƒì„± ì™„ë£Œ! ({subtitle})")
            st.text_area("ëŒ€ë³¸ ë¯¸ë¦¬ë³´ê¸°", value=script, height=420)
            c3,c4 = st.columns(2)
            with c3:
                st.download_button("â¬‡ï¸ TXT ë‹¤ìš´ë¡œë“œ", data=_xml_safe(script).encode("utf-8"),
                                   file_name="tbm_script.txt", use_container_width=True)
            with c4:
                st.download_button("â¬‡ï¸ DOCX ë‹¤ìš´ë¡œë“œ", data=to_docx_bytes(script),
                                   file_name="tbm_script.docx", use_container_width=True)

st.caption("ì™„ì „ ë¬´ë£Œ. ì—…ë¡œë“œ/ë¶™ì—¬ë„£ê¸°ë§Œ í•´ë„ ëˆ„ì  í•™ìŠµ â†’ ìš”ì•½ ê°€ì¤‘/í–‰ë™/ì§ˆë¬¸ ë³´ê°•. ë™ì  ì£¼ì œ ë¼ë²¨. UI ë³€ê²½ ì—†ìŒ.")
