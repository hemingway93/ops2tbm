# =========================
# OPS2TBM ‚Äî ÏôÑÏ†Ñ Î¨¥Î£å ÏïàÏ†ïÌåê (DOCX ÎÇ¥Î≥¥ÎÇ¥Í∏∞ Ïò§Î•ò ÏàòÏ†ï Ìè¨Ìï®)
# - ÌÖçÏä§Ìä∏ PDF / ÌÖçÏä§Ìä∏ Î∂ôÏó¨ÎÑ£Í∏∞ / ZIP(Ïó¨Îü¨ PDF) ÏßÄÏõê
# - ÌÖúÌîåÎ¶ø: ÏûêÎèô/ÏÇ¨Í≥†ÏÇ¨Î°ÄÌòï/Í∞ÄÏù¥ÎìúÌòï (ÏàòÎèô ÏÑ†ÌÉù Í∞ÄÎä•)
# - AI ÏöîÏïΩ: ÏàúÏàò NumPy TextRank + MMR(Ï§ëÎ≥µ ÏñµÏ†ú)  ‚Üê Î¨¥Î£å
# - ÏûêÏó∞Ïä§Îü¨Ïö¥ ÍµêÏú°ÎåÄÎ≥∏(Î¨¥Î£å Î™®Îìú): Í∑úÏπô Í∏∞Î∞ò Î¨∏Ï≤¥ Î≥ÄÌôò/Íµ¨ÏÑ±  ‚Üê Î¨¥Î£å(LLM/API ÁÑ°)
# - Ìó§Îçî/Ï§ëÎ≥µ Ï†úÍ±∞, ÎèôÏÇ¨ Ïö∞ÏÑ† ÏÑ†Î≥Ñ, ÏßàÎ¨∏Ìòï Î≥ÄÌôò
# - AIÎ°ú ÎΩëÌûå Î¨∏Ïû• ‚≠ê[AI] Í∞ïÏ°∞ ÌëúÏãú
# - üßπ Ï¥àÍ∏∞Ìôî Î≤ÑÌäº(ÏóÖÎ°úÎçî Ìè¨Ìï®)
# - ‚úÖ DOCX ÎÇ¥Î≥¥ÎÇ¥Í∏∞ Ïãú XML Í∏àÏßÄÎ¨∏Ïûê Ï†úÍ±∞(Ïò§Î•ò Ìï¥Í≤∞)
# =========================

import io
import zipfile
from typing import List, Tuple, Dict

import streamlit as st
from docx import Document
from docx.shared import Pt
from pdfminer.high_level import extract_text as pdf_extract_text
import pypdfium2 as pdfium
import numpy as np
import regex as rxx

# ----------------------------
# ÏÑ∏ÏÖò ÏÉÅÌÉú ÌÇ§ (ÏóÖÎ°úÎçî Ï¥àÍ∏∞ÌôîÏö©)
# ----------------------------
if "uploader_key" not in st.session_state:
    st.session_state.uploader_key = 0

# ----------------------------
# Í≥µÌÜµ Ïú†Ìã∏
# ----------------------------
HEADER_HINTS = [
    "ÏòàÎ∞©Ï°∞Ïπò", "5ÎåÄ Í∏∞Î≥∏ÏàòÏπô", "ÏùëÍ∏âÏ°∞Ïπò", "ÎØºÍ∞êÍµ∞", "Ï≤¥Í∞êÏò®ÎèÑ",
    "Î¨º¬∑Í∑∏Îäò¬∑Ìú¥Ïãù", "Î¨º, Í∑∏Îäò, Ìú¥Ïãù", "Î¨º Í∑∏Îäò Ìú¥Ïãù",
    "Î¨º¬∑Î∞îÎûå¬∑Ìú¥Ïãù", "Î¨º, Î∞îÎûå, Ìú¥Ïãù", "Î¨º Î∞îÎûå Ìú¥Ïãù",
    "ÏúÑÍ∏∞ÌÉàÏ∂ú ÏïàÏ†ÑÎ≥¥Í±¥ Ïï±", "Ï≤¥Í∞êÏò®ÎèÑ Í≥ÑÏÇ∞Í∏∞"
]
ACTION_VERBS = [
    "ÏÑ§Ïπò","Î∞∞Ïπò","Ï∞©Ïö©","Ï†êÍ≤Ä","ÌôïÏù∏","Ï∏°Ï†ï","Í∏∞Î°ù","ÌëúÏãú",
    "Ï†úÍ≥µ","ÎπÑÏπò","Î≥¥Í≥†","Ïã†Í≥†","ÍµêÏú°","Ï£ºÏßÄ","Ï§ëÏßÄ","ÌÜµÏ†ú",
    "ÏßÄÏõê","Ìú¥Ïãù","Ìú¥Í≤å","Ïù¥Îèô","ÌõÑÏÜ°","ÎÉâÍ∞Å","Í≥µÍ∏â","ÌëúÏßÄ","ÌëúÏãú"
]

def normalize_text(text: str) -> str:
    text = text.replace("\x0c", "\n")
    text = rxx.sub(r"[ \t]+\n", "\n", text)
    text = rxx.sub(r"\n{3,}", "\n\n", text)
    return text.strip()

def read_pdf_text(file_bytes: bytes) -> str:
    try:
        with io.BytesIO(file_bytes) as bio:
            text = pdf_extract_text(bio) or ""
    except Exception:
        text = ""
    text = normalize_text(text)

    if len(text.strip()) < 10:
        try:
            with io.BytesIO(file_bytes) as bio:
                pdf = pdfium.PdfDocument(bio)
                pages = len(pdf)
            if pages > 0 and not text.strip():
                st.warning("Ïù¥ PDFÎäî Ïù¥ÎØ∏ÏßÄ/Ïä§Ï∫î Í∏∞Î∞òÏúºÎ°ú Î≥¥Ïó¨Ïöî. ÌòÑÏû¨ Î≤ÑÏ†ÑÏùÄ OCR ÏóÜÏù¥ 'ÌÖçÏä§Ìä∏'Îßå Ï≤òÎ¶¨Ìï©ÎãàÎã§.")
        except Exception:
            pass
    return text

def split_sentences_ko(text: str) -> List[str]:
    raw = rxx.split(r"(?<=[\.!\?]|Îã§\.)\s+|\n+", text)
    sents = [s.strip(" -‚Ä¢‚óè‚ñ™‚ñ∂‚ñ∑\t") for s in raw if s and len(s.strip()) > 1]
    sents = [s for s in sents if len(s) >= 6]
    return sents

def simple_tokens(s: str) -> List[str]:
    s = s.lower()
    return rxx.findall(r"[Í∞Ä-Ìû£a-z0-9]{2,}", s)

def has_action_verb(s: str) -> bool:
    return any(v in s for v in ACTION_VERBS) or bool(rxx.search(r"(Ìï¥Ïïº\s*Ìï©ÎãàÎã§|Ïã≠ÏãúÏò§|Ìï©ÏãúÎã§|ÌïòÏÑ∏Ïöî)", s))

def is_header_like(s: str) -> bool:
    if len(s) <= 10 and not has_action_verb(s):
        return True
    if not has_action_verb(s) and any(h in s for h in HEADER_HINTS):
        return True
    if not rxx.search(r"[\.!\?Îã§]$", s) and not has_action_verb(s) and len(s) < 20:
        return True
    return False

def normalize_for_dedup(s: str) -> str:
    s2 = rxx.sub(r"\s+", "", s)
    s2 = rxx.sub(r"(..)\1{1,}", r"\1", s2)  # 2Í∏ÄÏûê Ïù¥ÏÉÅ Î∞òÎ≥µ Ï∂ïÏïΩ
    return s2

# ----------------------------
# ÏàúÏàò NumPy TextRank + MMR Îã§ÏñëÏÑ± (Î¨¥Î£å)
# ----------------------------
def sentence_tfidf_vectors(sents: List[str]) -> Tuple[np.ndarray, List[str]]:
    toks_list = [simple_tokens(s) for s in sents]
    vocab: Dict[str, int] = {}
    for toks in toks_list:
        for t in toks:
            if t not in vocab:
                vocab[t] = len(vocab)
    if not vocab:
        return np.zeros((len(sents), 0), dtype=np.float32), []
    mat = np.zeros((len(sents), len(vocab)), dtype=np.float32)
    df = np.zeros((len(vocab),), dtype=np.float32)
    for i, toks in enumerate(toks_list):
        for t in toks:
            j = vocab[t]
            mat[i, j] += 1.0
        for t in set(toks):
            df[vocab[t]] += 1.0
    N = float(len(sents))
    idf = np.log((N + 1.0) / (df + 1.0)) + 1.0
    mat *= idf
    norms = np.linalg.norm(mat, axis=1, keepdims=True) + 1e-8
    mat = mat / norms
    return mat, list(vocab.keys())

def cosine_sim_matrix(X: np.ndarray) -> np.ndarray:
    if X.size == 0:
        return np.zeros((X.shape[0], X.shape[0]), dtype=np.float32)
    sim = np.clip(X @ X.T, 0.0, 1.0)
    np.fill_diagonal(sim, 0.0)
    return sim

def textrank_scores(sents: List[str], d: float = 0.85, max_iter: int = 60, tol: float = 1e-4) -> List[float]:
    n = len(sents)
    if n == 0: return []
    if n == 1: return [1.0]
    X, _ = sentence_tfidf_vectors(sents)
    W = cosine_sim_matrix(X)
    row_sums = W.sum(axis=1, keepdims=True)
    P = np.divide(W, row_sums, out=np.zeros_like(W), where=row_sums > 0)
    r = np.ones((n, 1), dtype=np.float32) / n
    teleport = np.ones((n, 1), dtype=np.float32) / n
    for _ in range(max_iter):
        r_new = d * (P.T @ r) + (1 - d) * teleport
        if np.linalg.norm(r_new - r, ord=1) < tol:
            r = r_new; break
        r = r_new
    return [float(v) for v in r.flatten()]

def mmr_select(cands: List[str], scores: List[float], X: np.ndarray, k: int, lambda_: float = 0.7) -> List[int]:
    if not cands: return []
    selected: List[int] = []
    remaining = set(range(len(cands)))
    sim = cosine_sim_matrix(X)
    while remaining and len(selected) < k:
        best_idx, best_val = None, -1e9
        for i in remaining:
            rel = scores[i]
            div = max((sim[i, j] for j in selected), default=0.0)
            mmr = lambda_ * rel - (1 - lambda_) * div
            if mmr > best_val:
                best_val, best_idx = mmr, i
        selected.append(best_idx)
        remaining.remove(best_idx)
    return selected

# ----------------------------
# ÏÑ†ÌÉù Î°úÏßÅ (Í∑úÏπô/AI) ‚Äî Ìó§Îçî Ï†úÍ±∞/ÎèôÏÇ¨ Ïö∞ÏÑ†/Ï§ëÎ≥µ Ï†úÍ±∞ + AI Í∞ïÏ°∞
# ----------------------------
def filter_candidates(sents: List[str]) -> List[str]:
    out = []
    seen = set()
    for s in sents:
        if is_header_like(s): continue
        key = normalize_for_dedup(s)
        if key in seen: continue
        seen.add(key)
        out.append(s.strip())
    return out

def pick_rule(sents: List[str], keywords: List[str], limit: int) -> Tuple[List[str], List[bool]]:
    base = filter_candidates(sents)
    base = sorted(base, key=lambda x: (not has_action_verb(x), len(x)))  # ÎèôÏÇ¨O Î®ºÏ†Ä
    hits = [s for s in base if any(k in s for k in keywords)]
    out = hits[:limit]
    flags = [False] * len(out)
    if len(out) < limit:
        remain = [s for s in base if s not in out]
        remain = sorted(remain, key=lambda x: (not has_action_verb(x), len(x)))
        add = remain[: max(0, limit - len(out))]
        out.extend(add); flags.extend([False]*len(add))
    return out, flags

def pick_tr(sents: List[str], keywords: List[str], limit: int) -> Tuple[List[str], List[bool]]:
    base = filter_candidates(sents)
    if not base: return [], []
    scores = textrank_scores(base)
    scores = np.array(scores, dtype=np.float32)
    if keywords:
        for i, s in enumerate(base):
            if any(k in s for k in keywords): scores[i] += 0.2
            if has_action_verb(s): scores[i] += 0.1
    X, _ = sentence_tfidf_vectors(base)
    idx = mmr_select(base, scores.tolist(), X, limit, lambda_=0.7)
    out = [base[i] for i in idx]
    flags = [True] * len(out)
    return out, flags

def render_with_marks(lines: List[str], ai_flags: List[bool]) -> List[str]:
    return [f"- {'‚≠ê[AI] ' if ai else ''}{s}" for s, ai in zip(lines, ai_flags)]

# ----------------------------
# ÌÖúÌîåÎ¶ø/ÌÇ§ÏõåÎìú + ÏûêÎèô Î∂ÑÎ•ò Î≥¥Ï†ï
# ----------------------------
KW_GUIDE_CORE = ["Í∞ÄÏù¥Îìú","ÏïàÎÇ¥","Î≥¥Ìò∏","Í±¥Í∞ï","ÎåÄÏùë","Ï†àÏ∞®","ÏßÄÏπ®","Îß§Îâ¥Ïñº","ÏòàÎ∞©","ÏÉÅÎã¥","ÏßÄÏõê","Ï°¥Ï§ë","ÎØºÍ∞êÍµ∞"]
KW_GUIDE_STEP = ["Ï†àÏ∞®","ÏàúÏÑú","Î∞©Î≤ï","Ï†êÍ≤Ä","ÌôïÏù∏","Î≥¥Í≥†","Ï°∞Ïπò","Í∏∞Î°ù","Ìú¥Ïãù","Í≥µÍ∏â","Ï†úÍ≥µ","ÎπÑÏπò"]
KW_GUIDE_QA   = ["ÏßàÎ¨∏","Ïôú","Ïñ¥ÎñªÍ≤å","Î¨¥Ïóá","Ï£ºÏùò","ÌôïÏù∏Ìï†", "ÌÜ†Ïùò"]

KW_ACC_CORE = ["ÏÇ¨Í≥†","Ïû¨Ìï¥","ÏúÑÌóò","ÏõêÏù∏","ÏòàÎ∞©","ÎåÄÏ±Ö","ÎÖ∏ÌõÑ","Ï∂îÎùΩ","ÌòëÏ∞©","Í∞êÏ†Ñ","ÌôîÏû¨","ÏßàÏãù","Ï§ëÎèÖ"]
KW_ACC_STEP = ["Î∞úÏÉù","Í≤ΩÏúÑ","Ï°∞Ïπò","Í∞úÏÑ†","ÍµêÏú°","ÏÑ§Ïπò","Î∞∞Ïπò","Ï†êÍ≤Ä","Í¥ÄÎ¶¨"]

GUIDE_STRONG_HINTS = [
    "Î¨º¬∑Í∑∏Îäò¬∑Ìú¥Ïãù","Î¨º, Í∑∏Îäò, Ìú¥Ïãù","Î¨º Í∑∏Îäò Ìú¥Ïãù",
    "Î¨º¬∑Î∞îÎûå¬∑Ìú¥Ïãù","Î¨º, Î∞îÎûå, Ìú¥Ïãù","Î¨º Î∞îÎûå Ìú¥Ïãù",
    "Î≥¥ÎÉâÏû•Íµ¨","ÏùëÍ∏âÏ°∞Ïπò","ÎØºÍ∞êÍµ∞","Ï≤¥Í∞êÏò®ÎèÑ","ÏÇ¨ÏóÖÏ£ºÎäî"
]

def detect_template(text: str) -> str:
    g_hits = sum(text.count(k) for k in (KW_GUIDE_CORE + KW_GUIDE_STEP))
    a_hits = sum(text.count(k) for k in (KW_ACC_CORE + KW_ACC_STEP))
    g_hits += 3 * sum(text.count(k) for k in GUIDE_STRONG_HINTS)
    return "Í∞ÄÏù¥ÎìúÌòï" if g_hits >= a_hits else "ÏÇ¨Í≥†ÏÇ¨Î°ÄÌòï"

# ----------------------------
# ÏßàÎ¨∏Ìòï Î≥ÄÌôò(Í∞ÄÏù¥Îìú/ÏÇ¨Í≥† Í≥µÌÜµ)
# ----------------------------
def to_question(s: str) -> str:
    s = rxx.sub(r"\s{2,}", " ", s).strip(" -‚Ä¢‚óè‚ñ™‚ñ∂‚ñ∑").rstrip(" .")
    if has_action_verb(s): return f"Ïö∞Î¶¨ ÌòÑÏû•Ïóê '{s}' ÌïòÍ≥† ÏûàÎÇòÏöî?"
    return f"Ïù¥ Ìï≠Î™©Ïóê ÎåÄÌï¥ ÌòÑÏû• Ï†ÅÏö©Ïù¥ ÎêòÏóàÎÇòÏöî? ‚Äî {s}"

# ----------------------------
# TBM Í∏∞Î≥∏ ÌÖúÌîåÎ¶ø ÏÉùÏÑ± (ÌòÑÌñâ)
# ----------------------------
def make_tbm_guide(text: str, use_ai: bool) -> Tuple[str, Dict[str, List[str]]]:
    sents = split_sentences_ko(text)
    if use_ai: core, core_f = pick_tr(sents, KW_GUIDE_CORE, 3)
    else:      core, core_f = pick_rule(sents, KW_GUIDE_CORE, 3)
    steps, steps_f = pick_rule(sents, KW_GUIDE_STEP, 5)
    qa_src, _ = pick_rule(sents, KW_GUIDE_QA + KW_GUIDE_STEP, 3)
    qa = [to_question(x) for x in qa_src]
    parts = {"ÌïµÏã¨": core, "Ï†àÏ∞®": steps, "ÏßàÎ¨∏": qa}
    lines = []
    lines.append("ü¶∫ TBM ÎåÄÎ≥∏ ‚Äì Í∞ÄÏù¥ÎìúÌòï"); lines.append("")
    lines.append("‚óé Ïò§ÎäòÏùò ÌïµÏã¨ Ìè¨Ïù∏Ìä∏");   lines += render_with_marks(core, core_f); lines.append("")
    lines.append("‚óé ÏûëÏóÖ Ï†Ñ Ï†àÏ∞®/Ï†êÍ≤Ä");    lines += render_with_marks(steps, steps_f); lines.append("")
    lines.append("‚óé ÌòÑÏû• ÌÜ†Ïùò ÏßàÎ¨∏");      [lines.append(f"- {q}") for q in qa]; lines.append("")
    lines.append("‚óé ÎßàÎ¨¥Î¶¨ Î©òÌä∏")
    lines.append("- ‚ÄúÏò§Îäò ÏûëÏóÖÏùò ÌïµÏã¨ÏùÄ ÏúÑ ÏÑ∏ Í∞ÄÏßÄÏûÖÎãàÎã§. Îã§ Í∞ôÏù¥ ÌôïÏù∏ÌïòÍ≥† ÏãúÏûëÌï©ÏãúÎã§.‚Äù")
    lines.append("- ‚ÄúÏû†ÍπêÏù¥ÎùºÎèÑ Ïù¥ÏÉÅÌïòÎ©¥ Î∞îÎ°ú Ï§ëÏßÄÌïòÍ≥†, Í¥ÄÎ¶¨ÏûêÏóêÍ≤å ÏïåÎ¶ΩÎãàÎã§.‚Äù")
    return "\n".join(lines), parts

def make_tbm_accident(text: str, use_ai: bool) -> Tuple[str, Dict[str, List[str]]]:
    sents = split_sentences_ko(text)
    if use_ai: core, core_f = pick_tr(sents, KW_ACC_CORE, 3)
    else:      core, core_f = pick_rule(sents, KW_ACC_CORE, 3)
    steps, steps_f = pick_rule(sents, KW_ACC_STEP, 5)
    qa_src, _ = pick_rule(sents, KW_ACC_STEP, 3)
    qa = [to_question(x) for x in qa_src]
    parts = {"ÌïµÏã¨": core, "ÏÇ¨Í≥†/Ï°∞Ïπò": steps, "ÏßàÎ¨∏": qa}
    lines = []
    lines.append("ü¶∫ TBM ÎåÄÎ≥∏ ‚Äì ÏÇ¨Í≥†ÏÇ¨Î°ÄÌòï"); lines.append("")
    lines.append("‚óé ÏÇ¨Í≥†/ÏúÑÌóò ÏöîÏù∏ ÏöîÏïΩ");   lines += render_with_marks(core, core_f); lines.append("")
    lines.append("‚óé Î∞úÏÉù Í≤ΩÏúÑ/Ï°∞Ïπò/Í∞úÏÑ†");   lines += render_with_marks(steps, steps_f); lines.append("")
    lines.append("‚óé Ïû¨Î∞ú Î∞©ÏßÄ ÌÜ†Ïùò ÏßàÎ¨∏"); [lines.append(f"- {q}") for q in qa]; lines.append("")
    lines.append("‚óé ÎßàÎ¨¥Î¶¨ Î©òÌä∏")
    lines.append("- ‚ÄúÏù¥ ÏÇ¨Î°ÄÏóêÏÑú Î∞∞Ïö¥ ÏòàÎ∞© Ìè¨Ïù∏Ìä∏Î•º Ïò§Îäò ÏûëÏóÖÏóê Î∞îÎ°ú Ï†ÅÏö©Ìï©ÏãúÎã§.‚Äù")
    lines.append("- ‚ÄúÍ∞ÅÏûê Îß°ÏùÄ Í≥µÏ†ïÏóêÏÑú ÎèôÏùº ÏúÑÌóòÏù¥ ÏóÜÎäîÏßÄ Îã§Ïãú Ï†êÍ≤ÄÌï¥ Ï£ºÏÑ∏Ïöî.‚Äù")
    return "\n".join(lines), parts

# ----------------------------
# ‚úÖ Î¨¥Î£å ‚ÄúÏûêÏó∞Ïä§Îü¨Ïö¥ ÍµêÏú°ÎåÄÎ≥∏‚Äù ÏÉùÏÑ±Í∏∞ (Í∑úÏπô Í∏∞Î∞ò Î¨∏Ï≤¥ Î≥ÄÌôò/Íµ¨ÏÑ±)
# ----------------------------
INTRO_TONES = [
    "Ïò§ÎäòÏùÄ {topic}Ïóê ÎåÄÌï¥ Ïù¥ÏïºÍ∏∞Ìï¥Î≥¥Í≤†ÏäµÎãàÎã§.",
    "ÌòÑÏû•ÏóêÏÑú ÏûêÏ£º ÎÜìÏπòÍ∏∞ Ïâ¨Ïö¥ {topic}ÏùÑ(Î•º) ÏâΩÍ≤å Ï†ïÎ¶¨Ìï¥ ÎìúÎ¶¥Í≤åÏöî.",
    "{topic} ‚Äî Ïñ¥Î†µÏßÄ ÏïäÍ≤å ÌïµÏã¨Îßå ÏßöÏñ¥Î≥¥Í≤†ÏäµÎãàÎã§."
]
CONNECTORS = ["Î®ºÏ†Ä", "Í∑∏Î¶¨Í≥†", "ÎòêÌïú", "Î¨¥ÏóáÎ≥¥Îã§", "ÎßàÏßÄÎßâÏúºÎ°ú", "ÎçßÎ∂ôÏù¥Î©¥"]
CLOSERS = [
    "ÌòÑÏû•ÏùÄ ÏûëÏùÄ ÏäµÍ¥ÄÏóêÏÑú ÏïàÏ†ÑÏù¥ ÏãúÏûëÎê©ÎãàÎã§.",
    "ÏßÄÍ∏à Î∞îÎ°ú Ïö∞Î¶¨ ÏûëÏóÖÏóê Ï†ÅÏö©Ìï¥ Î¥ÖÏãúÎã§.",
    "ÏÑúÎëêÎ•¥ÏßÄ ÎßêÍ≥†, Ìïú Î≤à Îçî ÌôïÏù∏Ìï©ÏãúÎã§."
]
SLOGAN = "Ìïú Î≤à Îçî ÌôïÏù∏! Ìïú Î≤à Îçî Ï†êÍ≤Ä!"

def guess_topic(text: str) -> str:
    first = text.strip().split("\n", 1)[0][:30]
    if "Ïò®Ïó¥" in text or "Ìè≠Ïóº" in text: return "Ïò®Ïó¥ÏßàÌôò ÏòàÎ∞©"
    if "Ï∂îÎùΩ" in text or "ÏßÄÎ∂ï" in text or "Ïç¨ÎùºÏù¥Ìä∏" in text: return "ÏßÄÎ∂ï ÏûëÏóÖ Ï∂îÎùΩÏÇ¨Í≥† ÏòàÎ∞©"
    if "Í∞êÏ†ïÎÖ∏Îèô" in text: return "Í∞êÏ†ïÎÖ∏ÎèôÏûê Í±¥Í∞ïÎ≥¥Ìò∏"
    if "ÏßàÏãù" in text: return "ÏßàÏãù Ïû¨Ìï¥ ÏòàÎ∞©"
    if "Í∞êÏ†Ñ" in text: return "Í∞êÏ†Ñ ÏÇ¨Í≥† ÏòàÎ∞©"
    return first if first else "ÏïàÏ†ÑÎ≥¥Í±¥ ÍµêÏú°"

def soften_style(s: str) -> str:
    s = rxx.sub(r"~?ÌïòÏó¨Ïïº\s*Ìï©ÎãàÎã§", "Ìï¥Ïïº Ìï©ÎãàÎã§", s)
    s = s.replace("Ïã§ÏãúÌïúÎã§", "Ïã§ÏãúÌï©ÎãàÎã§").replace("Ïã§ÏãúÌïòÏó¨Ïïº", "Ïã§ÏãúÌï¥Ïïº Ìï©ÎãàÎã§")
    s = s.replace("ÌôïÏù∏ Î∞îÎûå", "ÌôïÏù∏Ìï¥Ï£ºÏÑ∏Ïöî").replace("ÌôïÏù∏ Î∞îÎûçÎãàÎã§", "ÌôïÏù∏Ìï¥Ï£ºÏÑ∏Ïöî")
    s = s.replace("Ï°∞ÏπòÌïúÎã§", "Ï°∞ÏπòÌï©ÎãàÎã§").replace("Ï∞©Ïö©ÌïúÎã§", "Ï∞©Ïö©Ìï©ÎãàÎã§")
    s = s.replace("ÌïÑÏöîÌïòÎã§", "ÌïÑÏöîÌï©ÎãàÎã§").replace("Í∏àÏßÄÌïúÎã§", "Í∏àÏßÄÌï©ÎãàÎã§")
    return s

def join_sentences_naturally(lines: List[str]) -> str:
    out = []
    for i, s in enumerate(lines):
        s = s.strip(" -‚Ä¢‚óè\t")
        s = soften_style(s)
        prefix = (CONNECTORS[min(i, len(CONNECTORS)-1)] + " ") if i < 5 else ""
        out.append(prefix + s)
    return " ".join(out)

def make_natural_script(text: str, detected: str, max_points: int = 6) -> str:
    sents = split_sentences_ko(text)
    if detected == "Í∞ÄÏù¥ÎìúÌòï":
        core, _ = pick_tr(sents, KW_GUIDE_CORE + KW_GUIDE_STEP, max_points)
    else:
        core, _ = pick_tr(sents, KW_ACC_CORE + KW_ACC_STEP, max_points)
    if not core: core = sents[:max_points]

    topic = guess_topic(text)
    intro = f"ü¶∫ ÍµêÏú°ÎåÄÎ≥∏ ‚Äì {topic}\n\n"
    intro += f"{np.random.choice(INTRO_TONES).format(topic=topic)}\n\n"

    body = join_sentences_naturally(core)

    actions = [c for c in core if has_action_verb(c)]
    if len(actions) < 3:
        extra = [s for s in sents if has_action_verb(s) and s not in actions]
        actions += extra[: 3 - len(actions)]
    actions = actions[:3]
    apply_lines = "\n".join([f"- {soften_style(a)}" for a in actions]) if actions else "- Ïò§Îäò ÏûëÏóÖ Í≥ÑÌöçÍ≥º ÏúÑÌóòÏöîÏù∏ÏùÑ Ìï®Íªò ÌôïÏù∏Ìï©ÎãàÎã§."

    closer = np.random.choice(CLOSERS)

    out = []
    out.append(intro)
    out.append("ÔºªÎèÑÏûÖÔºΩ")
    out.append(f"{np.random.choice(CONNECTORS)} {topic}Ïùò ÌïµÏã¨Îßå ÏßöÏñ¥Î≥ºÍ≤åÏöî.\n")
    out.append("ÔºªÌïµÏã¨ ÏÑ§Î™ÖÔºΩ")
    out.append(body + "\n")
    out.append("ÔºªÌòÑÏû• Ï†ÅÏö©/Ï†êÍ≤ÄÔºΩ")
    out.append(apply_lines + "\n")
    out.append("ÔºªÎßàÎ¨¥Î¶¨ ÎãπÎ∂ÄÔºΩ")
    out.append(closer + "\n")
    out.append("ÔºªÍµ¨Ìò∏ÔºΩ")
    out.append(f"‚Äú{SLOGAN}‚Äù")
    return "\n".join(out)

# ----------------------------
# ‚úÖ DOCXÎ°ú ÎÇ¥Î≥¥ÎÇ¥Í∏∞ ‚Äî XML Í∏àÏßÄÎ¨∏Ïûê Ï†úÍ±∞(Ïò§Î•ò Î∞©ÏßÄ)
# ----------------------------
_XML_FORBIDDEN = r"[\x00-\x08\x0B\x0C\x0E-\x1F\uD800-\uDFFF\uFFFE\uFFFF]"
def _xml_safe(s: str) -> str:
    if not isinstance(s, str):
        s = str(s) if s is not None else ""
    # python-docx/lxmlÏù¥ ÌóàÏö©ÌïòÏßÄ ÏïäÎäî XML 1.0 Í∏àÏßÄÎ¨∏Ïûê Ï†úÍ±∞
    return rxx.sub(_XML_FORBIDDEN, "", s)

def to_docx_bytes(script: str) -> bytes:
    doc = Document()
    # Î≥∏Î¨∏ Í∏ÄÍº¥ ÏÑ§Ï†ï (ÏúàÎèÑ/Îß•/Î¶¨ÎàÖÏä§ Î™®Îëê Î¨¥ÎÇúÌïú Í∏∞Î≥∏Ï≤¥Í∞Ä Ï†ÅÏö©Îê† Ïàò ÏûàÏùå)
    try:
        style = doc.styles["Normal"]
        style.font.name = "Malgun Gothic"
        style.font.size = Pt(11)
    except Exception:
        pass

    # Í∞Å Ï§ÑÏùÑ ÏïàÏ†ÑÌïòÍ≤å Ï∂îÍ∞Ä
    for raw in script.split("\n"):
        line = _xml_safe(raw)
        p = doc.add_paragraph(line)
        for run in p.runs:
            try:
                run.font.name = "Malgun Gothic"
                run.font.size = Pt(11)
            except Exception:
                pass

    bio = io.BytesIO()
    doc.save(bio)
    bio.seek(0)
    return bio.read()

# ----------------------------
# UI (Ï¢å/Ïö∞ 2Ïó¥ + ÏÇ¨Ïù¥ÎìúÎ∞î + Ï¥àÍ∏∞Ìôî)
# ----------------------------
st.set_page_config(page_title="OPS2TBM", page_icon="ü¶∫", layout="wide")

with st.sidebar:
    st.header("‚ÑπÔ∏è ÏÜåÍ∞ú / ÏÇ¨Ïö©Î≤ï")
    st.markdown("""
**AI Í∏∞Îä•(ÏôÑÏ†Ñ Î¨¥Î£å)**  
- Í≤ΩÎüâ TextRank(NumPy) + **MMR Îã§ÏñëÏÑ±**ÏúºÎ°ú ÌïµÏã¨ Î¨∏Ïû• Ï∂îÏ∂ú  
- **ÏûêÏó∞Ïä§Îü¨Ïö¥ ÍµêÏú°ÎåÄÎ≥∏(Î¨¥Î£å Î™®Îìú)**: Í∑úÏπô Í∏∞Î∞ò Î¨∏Ï≤¥ Î≥ÄÌôò/Íµ¨ÏÑ± ‚Üí Îã¥ÎãπÏûêÍ∞Ä ÏùΩÍ∏∞ Ï¢ãÏùÄ ÎßêÌïòÍ∏∞ ÌÜ§ÏúºÎ°ú ÏûêÎèô Ìé∏Ïßë  
- **ÏßßÏùÄ Ìó§Îçî/Ïä¨Î°úÍ±¥ Ï†úÍ±∞**, **ÌñâÎèô ÎèôÏÇ¨ Î¨∏Ïû• Ïö∞ÏÑ†**

**Ï¥àÍ∏∞Ìôî**  
- Ïö∞ÏÉÅÎã® **üßπ Ï¥àÍ∏∞Ìôî** Î≤ÑÌäº ‚Üí ÏóÖÎ°úÎìú/ÏÑ†ÌÉù/ÌÖçÏä§Ìä∏ Î¶¨ÏÖã
""")

st.title("ü¶∫ OPS2TBM ‚Äî OPS/Ìè¨Ïä§ÌÑ∞Î•º ÍµêÏú° ÎåÄÎ≥∏ÏúºÎ°ú ÏûêÎèô Î≥ÄÌôò (ÏôÑÏ†Ñ Î¨¥Î£å)")

def reset_all():
    st.session_state.pop("manual_text", None)
    st.session_state.pop("edited_text", None)
    st.session_state.pop("zip_choice", None)
    st.session_state.uploader_key += 1
    st.rerun()

col_top1, col_top2 = st.columns([4,1])
with col_top2:
    st.button("üßπ Ï¥àÍ∏∞Ìôî", on_click=reset_all, use_container_width=True)

st.markdown("""
**ÏïàÎÇ¥**  
- ÌÖçÏä§Ìä∏Í∞Ä Ìè¨Ìï®Îêú PDF ÎòêÎäî Î≥∏Î¨∏ ÌÖçÏä§Ìä∏Î•º Í∂åÏû•Ìï©ÎãàÎã§.  
- ZIP ÏóÖÎ°úÎìú Ïãú ÎÇ¥Î∂ÄÏùò PDFÎì§ÏùÑ ÏûêÎèô Ïù∏ÏãùÌïòÏó¨ ÏÑ†ÌÉùÌï† Ïàò ÏûàÏäµÎãàÎã§.  
- Ïù¥ÎØ∏ÏßÄ/Ïä§Ï∫îÌòï PDFÎäî ÌòÑÏû¨ OCR ÎØ∏ÏßÄÏõêÏûÖÎãàÎã§.
""")

col1, col2 = st.columns([1, 1], gap="large")

# Ï¢åÏ∏°: ÏûÖÎ†•
with col1:
    uploaded = st.file_uploader("OPS ÏóÖÎ°úÎìú (PDF ÎòêÎäî ZIP) ‚Ä¢ ÌÖçÏä§Ìä∏ PDF Í∂åÏû•",
                                type=["pdf", "zip"], key=f"uploader_{st.session_state.uploader_key}")
    manual_text = st.text_area("ÎòêÎäî OPS ÌÖçÏä§Ìä∏ ÏßÅÏ†ë Î∂ôÏó¨ÎÑ£Í∏∞", key="manual_text",
                               height=220, placeholder="Ïòà: Ìè≠Ïóº Ïãú Ïò®Ïó¥ÏßàÌôò ÏòàÎ∞©ÏùÑ ÏúÑÌï¥‚Ä¶")

    # ZIP Ï≤òÎ¶¨
    zip_pdfs: Dict[str, bytes] = {}
    selected_zip_pdf = None
    if uploaded and uploaded.name.lower().endswith(".zip"):
        try:
            with zipfile.ZipFile(uploaded, "r") as zf:
                for name in zf.namelist():
                    if name.lower().endswith(".pdf"):
                        zip_pdfs[name] = zf.read(name)
        except Exception:
            st.error("ZIPÏùÑ Ìï¥Ï†úÌïòÎäî Ï§ë Ïò§Î•òÍ∞Ä Î∞úÏÉùÌñàÏäµÎãàÎã§. ÌååÏùºÏùÑ ÌôïÏù∏Ìï¥ Ï£ºÏÑ∏Ïöî.")
        if zip_pdfs:
            selected_zip_pdf = st.selectbox("ZIP ÎÇ¥ PDF ÏÑ†ÌÉù", list(zip_pdfs.keys()), key="zip_choice")
        else:
            st.warning("ZIP ÏïàÏóêÏÑú PDFÎ•º Ï∞æÏßÄ Î™ªÌñàÏäµÎãàÎã§.")

    extracted = ""
    if uploaded and uploaded.name.lower().endswith(".pdf"):
        with st.spinner("PDF ÌÖçÏä§Ìä∏ Ï∂îÏ∂ú Ï§ë..."):
            data = uploaded.read()
            extracted = read_pdf_text(data)
    elif selected_zip_pdf:
        with st.spinner("ZIP ÎÇ¥Î∂Ä PDF ÌÖçÏä§Ìä∏ Ï∂îÏ∂ú Ï§ë..."):
            extracted = read_pdf_text(zip_pdfs[selected_zip_pdf])

    base_text = st.session_state.get("manual_text", "").strip() or extracted.strip()
    st.markdown("**Ï∂îÏ∂ú/ÏûÖÎ†• ÌÖçÏä§Ìä∏ ÎØ∏Î¶¨Î≥¥Í∏∞**")
    edited_text = st.text_area("ÌÖçÏä§Ìä∏", value=base_text, height=240, key="edited_text")

# Ïö∞Ï∏°: ÏòµÏÖò/ÏÉùÏÑ±/Îã§Ïö¥Î°úÎìú
with col2:
    use_ai = st.toggle("üîπ AI ÏöîÏïΩ(TextRank+MMR) ÏÇ¨Ïö©", value=True)
    tmpl_choice = st.selectbox("üß© ÌÖúÌîåÎ¶ø", ["ÏûêÎèô ÏÑ†ÌÉù", "ÏÇ¨Í≥†ÏÇ¨Î°ÄÌòï", "Í∞ÄÏù¥ÎìúÌòï"])
    gen_mode = st.selectbox("üß† ÏÉùÏÑ± Î™®Îìú", ["TBM Í∏∞Î≥∏(ÌòÑÌñâ)", "ÏûêÏó∞Ïä§Îü¨Ïö¥ ÍµêÏú°ÎåÄÎ≥∏(Î¨¥Î£å)"])
    max_points = st.slider("ÏöîÏïΩ Í∞ïÎèÑ(ÌïµÏã¨Î¨∏Ïû• Í∞úÏàò)", 3, 10, 6)

    if st.button("üõ†Ô∏è ÎåÄÎ≥∏ ÏÉùÏÑ±", type="primary", use_container_width=True):
        if not edited_text.strip():
            st.warning("ÌÖçÏä§Ìä∏Í∞Ä ÎπÑÏñ¥ ÏûàÏäµÎãàÎã§. PDF/ZIP ÏóÖÎ°úÎìú ÎòêÎäî ÌÖçÏä§Ìä∏ ÏûÖÎ†• ÌõÑ ÏãúÎèÑÌïòÏÑ∏Ïöî.")
        else:
            # ÌÖúÌîåÎ¶ø Í≤∞Ï†ï
            if tmpl_choice == "ÏûêÎèô ÏÑ†ÌÉù":
                detected = detect_template(edited_text)
            else:
                detected = tmpl_choice

            with st.spinner("ÎåÄÎ≥∏ ÏÉùÏÑ± Ï§ë..."):
                if gen_mode == "ÏûêÏó∞Ïä§Îü¨Ïö¥ ÍµêÏú°ÎåÄÎ≥∏(Î¨¥Î£å)":
                    script = make_natural_script(edited_text, detected, max_points=max_points)
                    subtitle = f"{detected} ¬∑ ÏûêÏó∞Ïä§Îü¨Ïö¥ ÍµêÏú°ÎåÄÎ≥∏(Î¨¥Î£å)"
                else:
                    if detected == "ÏÇ¨Í≥†ÏÇ¨Î°ÄÌòï":
                        script, _ = make_tbm_accident(edited_text, use_ai=use_ai)
                        subtitle = "ÏÇ¨Í≥†ÏÇ¨Î°ÄÌòï ¬∑ TBM Í∏∞Î≥∏"
                    else:
                        script, _ = make_tbm_guide(edited_text, use_ai=use_ai)
                        subtitle = "Í∞ÄÏù¥ÎìúÌòï ¬∑ TBM Í∏∞Î≥∏"

            st.success(f"ÎåÄÎ≥∏ ÏÉùÏÑ± ÏôÑÎ£å! ({subtitle})")
            st.text_area("ÎåÄÎ≥∏ ÎØ∏Î¶¨Î≥¥Í∏∞", value=script, height=420)

            c3, c4 = st.columns(2)
            with c3:
                st.download_button("‚¨áÔ∏è TXT Îã§Ïö¥Î°úÎìú", data=_xml_safe(script).encode("utf-8"),
                                   file_name="tbm_script.txt", use_container_width=True)
            with c4:
                st.download_button("‚¨áÔ∏è DOCX Îã§Ïö¥Î°úÎìú", data=to_docx_bytes(script),
                                   file_name="tbm_script.docx", use_container_width=True)

st.caption("ÌòÑÏû¨: ÏôÑÏ†Ñ Î¨¥Î£å. Í∑úÏπô + NumPy TextRank(Í≤ΩÎüâ AI) + MMR. ÌÖúÌîåÎ¶ø ÏûêÎèô/ÏàòÎèô. ZIP ÏßÄÏõê. OCR ÎØ∏ÏßÄÏõê(ÌÖçÏä§Ìä∏ PDF Í∂åÏû•). ‚ÄòÏûêÏó∞Ïä§Îü¨Ïö¥ ÍµêÏú°ÎåÄÎ≥∏(Î¨¥Î£å)‚Äô Î™®ÎìúÎ°ú ÎßêÌïòÍ∏∞ ÌÜ§ ÏûêÎèô Î≥ÄÌôò. (DOCX ÎÇ¥Î≥¥ÎÇ¥Í∏∞ ÌäπÏàòÎ¨∏Ïûê Ïò§Î•ò Ìï¥Í≤∞)")
