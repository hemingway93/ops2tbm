# =========================
# OPS2TBM â€” ì•ˆì •íŒ + ê²½ëŸ‰ AI(TextRank) + MMR ë‹¤ì–‘ì„± + ì´ˆê¸°í™” ë²„íŠ¼
# - í…ìŠ¤íŠ¸ PDF / í…ìŠ¤íŠ¸ ë¶™ì—¬ë„£ê¸° / ZIP(ì—¬ëŸ¬ PDF) ì§€ì›
# - í…œí”Œë¦¿: ìë™/ì‚¬ê³ ì‚¬ë¡€í˜•/ê°€ì´ë“œí˜• (ìˆ˜ë™ ì„ íƒ ê°€ëŠ¥)
# - AI ìš”ì•½: ìˆœìˆ˜ NumPy TextRank (ì„¤ì¹˜ ì´ìŠˆ ç„¡) + MMR ë‹¤ì–‘ì„±
# - í—¤ë”/ì¤‘ë³µ ì œê±°, ë™ì‚¬ ìš°ì„  ì„ ë³„, ì§ˆë¬¸í˜• ë³€í™˜
# - AIë¡œ ë½‘íŒ ë¬¸ì¥ â­[AI] ê°•ì¡° í‘œì‹œ
# - ì‚¬ì´ë“œë°”: ì†Œê°œ/ì‚¬ìš©ë²•/ì‹œì—° ë©˜íŠ¸
# - ğŸ§¹ ì´ˆê¸°í™” ë²„íŠ¼(ì—…ë¡œë” í¬í•¨)
# =========================

import io
import zipfile
from typing import List, Tuple, Dict

import streamlit as st
from docx import Document
from docx.shared import Pt
from pdfminer_high_level import extract_text as pdf_extract_text  # type: ignore
# ì¼ë¶€ í™˜ê²½ì—ì„œ import ê²½ë¡œ ì¶©ëŒ ë°©ì§€ìš© ë³„ì¹­ (pdfminer.six)
try:
    from pdfminer.high_level import extract_text as _et
    pdf_extract_text = _et
except Exception:
    pass

import pypdfium2 as pdfium
import numpy as np
import regex as rxx

# ----------------------------
# ì„¸ì…˜ ìƒíƒœ í‚¤ (ì—…ë¡œë” ì´ˆê¸°í™”ìš©)
# ----------------------------
if "uploader_key" not in st.session_state:
    st.session_state.uploader_key = 0

# ----------------------------
# ê³µí†µ ìœ í‹¸
# ----------------------------
HEADER_HINTS = [
    # í—¤ë”/ìŠ¬ë¡œê±´ìœ¼ë¡œ ìì£¼ ë“±ì¥í•˜ëŠ” í‘œí˜„ë“¤(ë³¸ë¬¸ìœ¼ë¡œ ì“°ë©´ ì–´ìƒ‰)
    "ì˜ˆë°©ì¡°ì¹˜", "5ëŒ€ ê¸°ë³¸ìˆ˜ì¹™", "ì‘ê¸‰ì¡°ì¹˜", "ë¯¼ê°êµ°", "ì²´ê°ì˜¨ë„",
    # í‘œì¤€ ìº í˜ì¸ ìŠ¬ë¡œê±´(ì˜¤íƒˆìì˜€ë˜ 'íœ´íœ´ì‹ì‹' ëŒ€ì‹  í‘œì¤€ í‘œí˜„ ë°˜ì˜)
    "ë¬¼Â·ê·¸ëŠ˜Â·íœ´ì‹", "ë¬¼, ê·¸ëŠ˜, íœ´ì‹", "ë¬¼ ê·¸ëŠ˜ íœ´ì‹",
    "ë¬¼Â·ë°”ëŒÂ·íœ´ì‹", "ë¬¼, ë°”ëŒ, íœ´ì‹", "ë¬¼ ë°”ëŒ íœ´ì‹",
    "ìœ„ê¸°íƒˆì¶œ ì•ˆì „ë³´ê±´ ì•±", "ì²´ê°ì˜¨ë„ ê³„ì‚°ê¸°"
]
ACTION_VERBS = [
    "ì„¤ì¹˜", "ë°°ì¹˜", "ì°©ìš©", "ì ê²€", "í™•ì¸", "ì¸¡ì •", "ê¸°ë¡", "í‘œì‹œ",
    "ì œê³µ", "ë¹„ì¹˜", "ë³´ê³ ", "ì‹ ê³ ", "êµìœ¡", "ì£¼ì§€", "ì¤‘ì§€", "í†µì œ",
    "ì§€ì›", "íœ´ì‹", "íœ´ê²Œ", "ì´ë™", "í›„ì†¡", "ëƒ‰ê°", "ê³µê¸‰"
]

def normalize_text(text: str) -> str:
    text = text.replace("\x0c", "\n")
    text = rxx.sub(r"[ \t]+\n", "\n", text)
    text = rxx.sub(r"\n{3,}", "\n\n", text)
    return text.strip()

def read_pdf_text(file_bytes: bytes) -> str:
    try:
        with io.BytesIO(file_bytes) as bio:
            text = pdf_extract_text(bio) or ""
    except Exception:
        text = ""
    text = normalize_text(text)

    if len(text.strip()) < 10:
        try:
            with io.BytesIO(file_bytes) as bio:
                pdf = pdfium.PdfDocument(bio)
                pages = len(pdf)
            if pages > 0 and not text.strip():
                st.warning("ì´ PDFëŠ” ì´ë¯¸ì§€/ìŠ¤ìº” ê¸°ë°˜ìœ¼ë¡œ ë³´ì—¬ìš”. í˜„ì¬ ë²„ì „ì€ OCR ì—†ì´ 'í…ìŠ¤íŠ¸'ë§Œ ì²˜ë¦¬í•©ë‹ˆë‹¤.")
        except Exception:
            pass
    return text

def split_sentences_ko(text: str) -> List[str]:
    raw = rxx.split(r"(?<=[\.!\?]|ë‹¤\.)\s+|\n+", text)
    sents = [s.strip(" -â€¢â—â–ªâ–¶â–·\t") for s in raw if s and len(s.strip()) > 1]
    # ë„ˆë¬´ ì§§ê±°ë‚˜ ê¸°í˜¸ì„± ë¬¸ì¥ ì œê±°
    sents = [s for s in sents if len(s) >= 6]
    return sents

def simple_tokens(s: str) -> List[str]:
    s = s.lower()
    return rxx.findall(r"[ê°€-í£a-z0-9]{2,}", s)

def has_action_verb(s: str) -> bool:
    return any(v in s for v in ACTION_VERBS) or bool(rxx.search(r"(í•´ì•¼\s*í•©ë‹ˆë‹¤|í•˜ì‹­ì‹œì˜¤|í•©ì‹œë‹¤|í•˜ì„¸ìš”)", s))

def is_header_like(s: str) -> bool:
    # ë™ì‚¬ê°€ ì—†ê³  ì§§ê±°ë‚˜(<=10ì) ìŠ¬ë¡œê±´/í—¤ë” íŒ¨í„´ì´ë©´ í—¤ë”ë¡œ ê°„ì£¼
    if len(s) <= 10 and not has_action_verb(s):
        return True
    if not has_action_verb(s) and any(h in s for h in HEADER_HINTS):
        return True
    if not rxx.search(r"[\.!\?ë‹¤]$", s) and not has_action_verb(s) and len(s) < 20:
        return True
    return False

def normalize_for_dedup(s: str) -> str:
    s2 = rxx.sub(r"\s+", "", s)
    s2 = rxx.sub(r"(..)\1{1,}", r"\1", s2)  # 2ê¸€ì ì´ìƒ ë°˜ë³µ ì¶•ì•½
    return s2

# ----------------------------
# ìˆœìˆ˜ NumPy TextRank + MMR ë‹¤ì–‘ì„±
# ----------------------------
def sentence_tfidf_vectors(sents: List[str]) -> Tuple[np.ndarray, List[str]]:
    toks_list = [simple_tokens(s) for s in sents]
    vocab: Dict[str, int] = {}
    for toks in toks_list:
        for t in toks:
            if t not in vocab:
                vocab[t] = len(vocab)
    if not vocab:
        return np.zeros((len(sents), 0), dtype=np.float32), []
    mat = np.zeros((len(sents), len(vocab)), dtype=np.float32)
    df = np.zeros((len(vocab),), dtype=np.float32)
    for i, toks in enumerate(toks_list):
        for t in toks:
            j = vocab[t]
            mat[i, j] += 1.0
        for t in set(toks):
            df[vocab[t]] += 1.0
    N = float(len(sents))
    idf = np.log((N + 1.0) / (df + 1.0)) + 1.0
    mat *= idf
    norms = np.linalg.norm(mat, axis=1, keepdims=True) + 1e-8
    mat = mat / norms
    return mat, list(vocab.keys())

def cosine_sim_matrix(X: np.ndarray) -> np.ndarray:
    if X.size == 0:
        return np.zeros((X.shape[0], X.shape[0]), dtype=np.float32)
    sim = np.clip(X @ X.T, 0.0, 1.0)
    np.fill_diagonal(sim, 0.0)
    return sim

def textrank_scores(sents: List[str], d: float = 0.85, max_iter: int = 60, tol: float = 1e-4) -> List[float]:
    n = len(sents)
    if n == 0:
        return []
    if n == 1:
        return [1.0]
    X, _ = sentence_tfidf_vectors(sents)
    W = cosine_sim_matrix(X)
    row_sums = W.sum(axis=1, keepdims=True)
    P = np.divide(W, row_sums, out=np.zeros_like(W), where=row_sums > 0)

    r = np.ones((n, 1), dtype=np.float32) / n
    teleport = np.ones((n, 1), dtype=np.float32) / n

    for _ in range(max_iter):
        r_new = d * (P.T @ r) + (1 - d) * teleport
        if np.linalg.norm(r_new - r, ord=1) < tol:
            r = r_new
            break
        r = r_new
    return [float(v) for v in r.flatten()]

def mmr_select(cands: List[str], scores: List[float], X: np.ndarray, k: int, lambda_: float = 0.7) -> List[int]:
    if not cands:
        return []
    selected: List[int] = []
    remaining = set(range(len(cands)))
    sim = cosine_sim_matrix(X)
    while remaining and len(selected) < k:
        best_idx, best_val = None, -1e9
        for i in remaining:
            rel = scores[i]
            div = max((sim[i, j] for j in selected), default=0.0)
            mmr = lambda_ * rel - (1 - lambda_) * div
            if mmr > best_val:
                best_val, best_idx = mmr, i
        selected.append(best_idx)
        remaining.remove(best_idx)
    return selected

# ----------------------------
# ì„ íƒ ë¡œì§ (ê·œì¹™/AI) â€” í—¤ë” ì œê±°/ë™ì‚¬ ìš°ì„ /ì¤‘ë³µ ì œê±° + AI ê°•ì¡°
# ----------------------------
def filter_candidates(sents: List[str]) -> List[str]:
    out = []
    seen = set()
    for s in sents:
        if is_header_like(s):
            continue
        key = normalize_for_dedup(s)
        if key in seen:
            continue
        seen.add(key)
        out.append(s.strip())
    return out

def pick_rule(sents: List[str], keywords: List[str], limit: int) -> Tuple[List[str], List[bool]]:
    base = filter_candidates(sents)
    base = sorted(base, key=lambda x: (not has_action_verb(x), len(x)))  # ë™ì‚¬O ë¨¼ì €
    hits = [s for s in base if any(k in s for k in keywords)]
    out = hits[:limit]
    flags = [False] * len(out)
    if len(out) < limit:
        remain = [s for s in base if s not in out]
        remain = sorted(remain, key=lambda x: (not has_action_verb(x), len(x)))
        add = remain[: max(0, limit - len(out))]
        out.extend(add)
        flags.extend([False] * len(add))
    return out, flags

def pick_tr(sents: List[str], keywords: List[str], limit: int) -> Tuple[List[str], List[bool]]:
    base = filter_candidates(sents)
    if not base:
        return [], []
    scores = textrank_scores(base)
    scores = np.array(scores, dtype=np.float32)
    if keywords:
        for i, s in enumerate(base):
            if any(k in s for k in keywords):
                scores[i] += 0.2
            if has_action_verb(s):
                scores[i] += 0.1  # í–‰ë™ ë¬¸ì¥ ê°€ì¤‘
    X, _ = sentence_tfidf_vectors(base)
    idx = mmr_select(base, scores.tolist(), X, limit, lambda_=0.7)
    out = [base[i] for i in idx]
    flags = [True] * len(out)
    return out, flags

def render_with_marks(lines: List[str], ai_flags: List[bool]) -> List[str]:
    out = []
    for s, ai in zip(lines, ai_flags):
        out.append(f"- {'â­[AI] ' if ai else ''}{s}")
    return out

# ----------------------------
# í…œí”Œë¦¿/í‚¤ì›Œë“œ + ìë™ ë¶„ë¥˜ ë³´ì •
# ----------------------------
KW_GUIDE_CORE = ["ê°€ì´ë“œ","ì•ˆë‚´","ë³´í˜¸","ê±´ê°•","ëŒ€ì‘","ì ˆì°¨","ì§€ì¹¨","ë§¤ë‰´ì–¼","ì˜ˆë°©","ìƒë‹´","ì§€ì›","ì¡´ì¤‘","ë¯¼ê°êµ°"]
KW_GUIDE_STEP = ["ì ˆì°¨","ìˆœì„œ","ë°©ë²•","ì ê²€","í™•ì¸","ë³´ê³ ","ì¡°ì¹˜","ê¸°ë¡","íœ´ì‹","ê³µê¸‰","ì œê³µ","ë¹„ì¹˜"]
KW_GUIDE_QA   = ["ì§ˆë¬¸","ì™œ","ì–´ë–»ê²Œ","ë¬´ì—‡","ì£¼ì˜","í™•ì¸í• ", "í† ì˜"]

KW_ACC_CORE = ["ì‚¬ê³ ","ì¬í•´","ìœ„í—˜","ì›ì¸","ì˜ˆë°©","ëŒ€ì±…","ë…¸í›„","ì¶”ë½","í˜‘ì°©","ê°ì „","í™”ì¬","ì§ˆì‹","ì¤‘ë…"]
KW_ACC_STEP = ["ë°œìƒ","ê²½ìœ„","ì¡°ì¹˜","ê°œì„ ","êµìœ¡","ì„¤ì¹˜","ë°°ì¹˜","ì ê²€","ê´€ë¦¬"]

# ê°€ì´ë“œí˜• ê°•í•œ ì‹ í˜¸ (ìë™ ë¶„ë¥˜ ë³´ì •)
GUIDE_STRONG_HINTS = [
    "ë¬¼Â·ê·¸ëŠ˜Â·íœ´ì‹", "ë¬¼, ê·¸ëŠ˜, íœ´ì‹", "ë¬¼ ê·¸ëŠ˜ íœ´ì‹",
    "ë¬¼Â·ë°”ëŒÂ·íœ´ì‹", "ë¬¼, ë°”ëŒ, íœ´ì‹", "ë¬¼ ë°”ëŒ íœ´ì‹",
    "ë³´ëƒ‰ì¥êµ¬", "ì‘ê¸‰ì¡°ì¹˜", "ë¯¼ê°êµ°", "ì²´ê°ì˜¨ë„", "ì‚¬ì—…ì£¼ëŠ”"
]

def detect_template(text: str) -> str:
    g_hits = sum(text.count(k) for k in (KW_GUIDE_CORE + KW_GUIDE_STEP))
    a_hits = sum(text.count(k) for k in (KW_ACC_CORE + KW_ACC_STEP))
    g_hits += 3 * sum(text.count(k) for k in GUIDE_STRONG_HINTS)  # ê°•í•œ íŒíŠ¸ ê°€ì¤‘
    return "ê°€ì´ë“œí˜•" if g_hits >= a_hits else "ì‚¬ê³ ì‚¬ë¡€í˜•"

# ----------------------------
# ì§ˆë¬¸í˜• ë³€í™˜(ê°€ì´ë“œ/ì‚¬ê³  ê³µí†µ)
# ----------------------------
def to_question(s: str) -> str:
    s = rxx.sub(r"\s{2,}", " ", s).strip(" -â€¢â—â–ªâ–¶â–·").rstrip(" .")
    if has_action_verb(s):
        return f"ìš°ë¦¬ í˜„ì¥ì— '{s}' í•˜ê³  ìˆë‚˜ìš”?"
    return f"ì´ í•­ëª©ì— ëŒ€í•´ í˜„ì¥ ì ìš©ì´ ë˜ì—ˆë‚˜ìš”? â€” {s}"

# ----------------------------
# TBM ìƒì„±
# ----------------------------
def make_tbm_guide(text: str, use_ai: bool) -> Tuple[str, Dict[str, List[str]]]:
    sents = split_sentences_ko(text)
    if use_ai:
        core, core_f = pick_tr(sents, KW_GUIDE_CORE, 3)
    else:
        core, core_f = pick_rule(sents, KW_GUIDE_CORE, 3)
    steps, steps_f = pick_rule(sents, KW_GUIDE_STEP, 5)
    qa_src, _      = pick_rule(sents, KW_GUIDE_QA + KW_GUIDE_STEP, 3)
    qa = [to_question(x) for x in qa_src]

    parts = {"í•µì‹¬": core, "ì ˆì°¨": steps, "ì§ˆë¬¸": qa}
    lines = []
    lines.append("ğŸ¦º TBM ëŒ€ë³¸ â€“ ê°€ì´ë“œí˜•")
    lines.append("")
    lines.append("â— ì˜¤ëŠ˜ì˜ í•µì‹¬ í¬ì¸íŠ¸")
    lines += render_with_marks(core, core_f)
    lines.append("")
    lines.append("â— ì‘ì—… ì „ ì ˆì°¨/ì ê²€")
    lines += render_with_marks(steps, steps_f)
    lines.append("")
    lines.append("â— í˜„ì¥ í† ì˜ ì§ˆë¬¸")
    for q in qa: lines.append(f"- {q}")
    lines.append("")
    lines.append("â— ë§ˆë¬´ë¦¬ ë©˜íŠ¸")
    lines.append("- â€œì˜¤ëŠ˜ ì‘ì—…ì˜ í•µì‹¬ì€ ìœ„ ì„¸ ê°€ì§€ì…ë‹ˆë‹¤. ë‹¤ ê°™ì´ í™•ì¸í•˜ê³  ì‹œì‘í•©ì‹œë‹¤.â€")
    lines.append("- â€œì ê¹ì´ë¼ë„ ì´ìƒí•˜ë©´ ë°”ë¡œ ì¤‘ì§€í•˜ê³ , ê´€ë¦¬ìì—ê²Œ ì•Œë¦½ë‹ˆë‹¤.â€")
    script = "\n".join(lines)
    return script, parts

def make_tbm_accident(text: str, use_ai: bool) -> Tuple[str, Dict[str, List[str]]]:
    sents = split_sentences_ko(text)
    if use_ai:
        core, core_f = pick_tr(sents, KW_ACC_CORE, 3)
    else:
        core, core_f = pick_rule(sents, KW_ACC_CORE, 3)
    steps, steps_f = pick_rule(sents, KW_ACC_STEP, 5)
    qa_src, _      = pick_rule(sents, KW_ACC_STEP, 3)
    qa = [to_question(x) for x in qa_src]

    parts = {"í•µì‹¬": core, "ì‚¬ê³ /ì¡°ì¹˜": steps, "ì§ˆë¬¸": qa}
    lines = []
    lines.append("ğŸ¦º TBM ëŒ€ë³¸ â€“ ì‚¬ê³ ì‚¬ë¡€í˜•")
    lines.append("")
    lines.append("â— ì‚¬ê³ /ìœ„í—˜ ìš”ì¸ ìš”ì•½")
    lines += render_with_marks(core, core_f)
    lines.append("")
    lines.append("â— ë°œìƒ ê²½ìœ„/ì¡°ì¹˜/ê°œì„ ")
    lines += render_with_marks(steps, steps_f)
    lines.append("")
    lines.append("â— ì¬ë°œ ë°©ì§€ í† ì˜ ì§ˆë¬¸")
    for q in qa: lines.append(f"- {q}")
    lines.append("")
    lines.append("â— ë§ˆë¬´ë¦¬ ë©˜íŠ¸")
    lines.append("- â€œì´ ì‚¬ë¡€ì—ì„œ ë°°ìš´ ì˜ˆë°© í¬ì¸íŠ¸ë¥¼ ì˜¤ëŠ˜ ì‘ì—…ì— ë°”ë¡œ ì ìš©í•©ì‹œë‹¤.â€")
    lines.append("- â€œê°ì ë§¡ì€ ê³µì •ì—ì„œ ë™ì¼ ìœ„í—˜ì´ ì—†ëŠ”ì§€ ë‹¤ì‹œ ì ê²€í•´ ì£¼ì„¸ìš”.â€")
    script = "\n".join(lines)
    return script, parts

# ----------------------------
# ë‚´ë³´ë‚´ê¸°
# ----------------------------
def to_docx_bytes(script: str) -> bytes:
    doc = Document()
    style = doc.styles["Normal"]
    style.font.name = "Malgun Gothic"
    style.font.size = Pt(11)
    for line in script.split("\n"):
        p = doc.add_paragraph(line)
        for run in p.runs:
            run.font.name = "Malgun Gothic"
            run.font.size = Pt(11)
    bio = io.BytesIO()
    doc.save(bio)
    bio.seek(0)
    return bio.read()

# ----------------------------
# UI (ì¢Œ/ìš° 2ì—´ + ì‚¬ì´ë“œë°” + ì´ˆê¸°í™”)
# ----------------------------
st.set_page_config(page_title="OPS2TBM", page_icon="ğŸ¦º", layout="wide")

with st.sidebar:
    st.header("â„¹ï¸ ì†Œê°œ / ì‚¬ìš©ë²•")
    st.markdown("""
**AI ê¸°ëŠ¥**  
- ê²½ëŸ‰ TextRank(NumPy) + **MMR ë‹¤ì–‘ì„±**ìœ¼ë¡œ í•µì‹¬ ë¬¸ì¥ì„ ì¶”ì¶œ  
- ëŒ€ë³¸ì—ì„œ í•´ë‹¹ ë¬¸ì¥ì„ **â­[AI]** ë¡œ ê°•ì¡°  
- **ì§§ì€ í—¤ë”/ìŠ¬ë¡œê±´ ì œê±°**, **í–‰ë™ ë™ì‚¬ ë¬¸ì¥ ìš°ì„ **

**ì´ˆê¸°í™” ë°©ë²•**  
- ìš°ìƒë‹¨ **ğŸ§¹ ì´ˆê¸°í™”** ë²„íŠ¼ì„ ëˆ„ë¥´ë©´ ì—…ë¡œë“œ/ì„ íƒ/í…ìŠ¤íŠ¸ê°€ ëª¨ë‘ ë¦¬ì…‹ë©ë‹ˆë‹¤.
""")

st.title("ğŸ¦º OPS2TBM â€” OPS/í¬ìŠ¤í„°ë¥¼ TBM ëŒ€ë³¸ìœ¼ë¡œ ìë™ ë³€í™˜")

def reset_all():
    st.session_state.pop("manual_text", None)
    st.session_state.pop("edited_text", None)
    st.session_state.pop("zip_choice", None)
    st.session_state.uploader_key += 1
    st.rerun()

col_top1, col_top2 = st.columns([4,1])
with col_top2:
    st.button("ğŸ§¹ ì´ˆê¸°í™”", on_click=reset_all, use_container_width=True)

st.markdown("""
**ì•ˆë‚´**  
- í…ìŠ¤íŠ¸ê°€ í¬í•¨ëœ PDF ë˜ëŠ” ë³¸ë¬¸ í…ìŠ¤íŠ¸ë¥¼ ê¶Œì¥í•©ë‹ˆë‹¤.  
- ZIP ì—…ë¡œë“œ ì‹œ ë‚´ë¶€ì˜ PDFë“¤ì„ ìë™ ì¸ì‹í•˜ì—¬ ì„ íƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.  
- ì´ë¯¸ì§€/ìŠ¤ìº”í˜• PDFëŠ” í˜„ì¬ OCR ë¯¸ì§€ì›ì…ë‹ˆë‹¤.
""")

col1, col2 = st.columns([1, 1], gap="large")

# ì¢Œì¸¡: ì…ë ¥
with col1:
    uploaded = st.file_uploader("OPS ì—…ë¡œë“œ (PDF ë˜ëŠ” ZIP) â€¢ í…ìŠ¤íŠ¸ PDF ê¶Œì¥",
                                type=["pdf", "zip"], key=f"uploader_{st.session_state.uploader_key}")
    manual_text = st.text_area("ë˜ëŠ” OPS í…ìŠ¤íŠ¸ ì§ì ‘ ë¶™ì—¬ë„£ê¸°", key="manual_text",
                               height=220, placeholder="ì˜ˆ: í­ì—¼ ì‹œ ì˜¨ì—´ì§ˆí™˜ ì˜ˆë°©ì„ ìœ„í•´â€¦")

    # ZIP ì²˜ë¦¬
    zip_pdfs: Dict[str, bytes] = {}
    selected_zip_pdf = None
    if uploaded and uploaded.name.lower().endswith(".zip"):
        try:
            with zipfile.ZipFile(uploaded, "r") as zf:
                for name in zf.namelist():
                    if name.lower().endswith(".pdf"):
                        zip_pdfs[name] = zf.read(name)
        except Exception:
            st.error("ZIPì„ í•´ì œí•˜ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. íŒŒì¼ì„ í™•ì¸í•´ ì£¼ì„¸ìš”.")
        if zip_pdfs:
            selected_zip_pdf = st.selectbox("ZIP ë‚´ PDF ì„ íƒ", list(zip_pdfs.keys()), key="zip_choice")
        else:
            st.warning("ZIP ì•ˆì—ì„œ PDFë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")

    extracted = ""
    if uploaded and uploaded.name.lower().endswith(".pdf"):
        with st.spinner("PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ ì¤‘..."):
            data = uploaded.read()
            extracted = read_pdf_text(data)
    elif selected_zip_pdf:
        with st.spinner("ZIP ë‚´ë¶€ PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ ì¤‘..."):
            extracted = read_pdf_text(zip_pdfs[selected_zip_pdf])

    base_text = st.session_state.get("manual_text", "").strip() or extracted.strip()
    st.markdown("**ì¶”ì¶œ/ì…ë ¥ í…ìŠ¤íŠ¸ ë¯¸ë¦¬ë³´ê¸°**")
    edited_text = st.text_area("í…ìŠ¤íŠ¸", value=base_text, height=240, key="edited_text")

# ìš°ì¸¡: ì˜µì…˜/ìƒì„±/ë‹¤ìš´ë¡œë“œ
with col2:
    use_ai = st.toggle("ğŸ”¹ AI ìš”ì•½(TextRank+MMR) ì‚¬ìš©", value=True)
    tmpl_choice = st.selectbox("ğŸ§© í…œí”Œë¦¿", ["ìë™ ì„ íƒ", "ì‚¬ê³ ì‚¬ë¡€í˜•", "ê°€ì´ë“œí˜•"])

    if st.button("ğŸ› ï¸ TBM ëŒ€ë³¸ ìƒì„±", type="primary", use_container_width=True):
        if not edited_text.strip():
            st.warning("í…ìŠ¤íŠ¸ê°€ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤. PDF/ZIP ì—…ë¡œë“œ ë˜ëŠ” í…ìŠ¤íŠ¸ ì…ë ¥ í›„ ì‹œë„í•˜ì„¸ìš”.")
        else:
            if tmpl_choice == "ìë™ ì„ íƒ":
                detected = detect_template(edited_text)
            else:
                detected = tmpl_choice

            with st.spinner("ëŒ€ë³¸ ìƒì„± ì¤‘..."):
                if detected == "ì‚¬ê³ ì‚¬ë¡€í˜•":
                    script, parts = make_tbm_accident(edited_text, use_ai=use_ai)
                    subtitle = "ì‚¬ê³ ì‚¬ë¡€í˜• í…œí”Œë¦¿ ì ìš©"
                else:
                    script, parts = make_tbm_guide(edited_text, use_ai=use_ai)
                    subtitle = "ê°€ì´ë“œí˜• í…œí”Œë¦¿ ì ìš©"

            st.success(f"ëŒ€ë³¸ ìƒì„± ì™„ë£Œ! ({subtitle}{' Â· AI ìš”ì•½' if use_ai else ''})")
            st.text_area("TBM ëŒ€ë³¸ ë¯¸ë¦¬ë³´ê¸°", value=script, height=420)

            c3, c4 = st.columns(2)
            with c3:
                st.download_button("â¬‡ï¸ TXT ë‹¤ìš´ë¡œë“œ", data=script.encode("utf-8"),
                                   file_name="tbm_script.txt", use_container_width=True)
            with c4:
                st.download_button("â¬‡ï¸ DOCX ë‹¤ìš´ë¡œë“œ", data=to_docx_bytes(script),
                                   file_name="tbm_script.docx", use_container_width=True)

st.caption("í˜„ì¬: ê·œì¹™ + NumPy TextRank(ê²½ëŸ‰ AI) + MMR ë‹¤ì–‘ì„±. í…œí”Œë¦¿ ìë™/ìˆ˜ë™. ZIP ì§€ì›. OCR ë¯¸ì§€ì›(í…ìŠ¤íŠ¸ PDF ê¶Œì¥).")
