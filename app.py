# ==========================================================
# OPS2TBM â€” OPS/í¬ìŠ¤í„° â†’ TBM êµìœ¡ ëŒ€ë³¸ ìë™ ë³€í™˜ (ì™„ì „ ë¬´ë£Œ)
# v2025-11-05-a (annotated, ì‚¬ë¡€/ì˜ˆë°© â€˜ì „ë¶€â€™ ë°˜ì˜ ê°•í™”)
#  - [í•«í”½ìŠ¤] ì‚¬ê³ ì‚¬ë¡€ê°€ ëˆ„ë½ë˜ì§€ ì•Šë„ë¡ â€˜ì‚¬ë¡€ ë¸”ë¡ ëª…ì‹œì  íŒŒì‹±â€™ ì¶”ê°€
#  - [í•«í”½ìŠ¤] ì˜ˆë°©ìˆ˜ì¹™ì€ ê¸€ììˆ˜/ê°œìˆ˜ ì œí•œ ì—†ì´ â€˜ì „ë¶€â€™ ë°˜ì˜
#  - [ë³´ê°•] ì„¹ì…˜ í—¤ë”(ì˜ˆ: ë°€íê³µê°„ì‘ì—…ì‹œ / ìœ„í—˜ë¬¼ì§ˆ ì·¨ê¸‰ì‹œ / ì•ˆì „ ì‘ì—…ë°©ë²• ë“±) í•˜ìœ„ ë¶ˆë¦¿ì„
#          ì¤„ê²°í•©â†’í–‰ë™ë¬¸ ìì—°í™” í›„ ëª¨ë‘ ì¶œë ¥
#  - [ìœ ì§€] UI/ë ˆì´ì•„ì›ƒ/ë²„íŠ¼/ëª¨ë“œëŠ” ë³€ê²½ ì—†ìŒ
#
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ğŸ“Œ ì•„í‚¤í…ì²˜/ì ìš© ê¸°ìˆ  ë§¤í•‘ (ë¬¸ì„œìš© ì„¤ëª…)
# [ì…ë ¥ ê³„ì¸µ]
#   - Streamlit file_uploader/textareaë¡œ PDF/ZIP/í…ìŠ¤íŠ¸ ì…ë ¥
#   - PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ: pdfminer.six (í…ìŠ¤íŠ¸í˜• ìš°ì„ ), pypdfium2ë¡œ ìŠ¤ìº” ê°ì§€
#
# [ì „ì²˜ë¦¬ ê³„ì¸µ]
#   - ì¤„ ë³‘í•©(merge_broken_lines): í¬ìŠ¤í„°/ë¦¬í”Œë¦¿ì²˜ëŸ¼ ì¤„ë°”ê¿ˆì´ ì¦ì€ ë¬¸ì„œë¥¼ ë¬¸ì¥ ë‹¨ìœ„ë¡œ ì •ë¦¬
#   - ë‚ ì§œ+ì‚¬ê³  ê²°í•©(combine_date_with_next): "yy.mm.dd" ë‹¤ìŒ ì¤„ì˜ ì‚¬ê³ ë¬¸ì„ í•œ ë¬¸ì¥ìœ¼ë¡œ ë¬¶ìŒ
#   - ì‚¬ê³  ë¸”ë¡ ë´‰í•©(stitch_case_blocks): â€œì“°ëŸ¬ì§€ì/êµ¬ì¡°í•˜ë˜ ì¤‘/ì°¨ë¡€ë¡œ â€¦â€ ë“± ì—°ê²°ì–´ ê°ì§€í•´ ì¸ì ‘ ë¬¸ì¥ ê²°í•©
#
# [ëª…ì‹œì  ì„¹ì…˜ íŒŒì„œ(NEW)]
#   - extract_section_bullets: â€œì£¼ìš” ì‚¬ê³ ì‚¬ë¡€/ì‚¬ê³ ì‚¬ë¡€â€ ë“± ì‚¬ë¡€ ì„¹ì…˜ê³¼
#     â€œì•ˆì „ ì‘ì—…ë°©ë²•/ë°€íê³µê°„ì‘ì—…ì‹œ/ìœ„í—˜ë¬¼ì§ˆ ì·¨ê¸‰ì‹œ/ì˜ˆë°©ìˆ˜ì¹™/ì‹¤ì²œìˆ˜ì¹™/ì˜ˆë°©ì¡°ì¹˜â€ ë“± ì˜ˆë°© ì„¹ì…˜ì„
#     ì›ë¬¸ í…ìŠ¤íŠ¸ì—ì„œ ì§ì ‘ íŒŒì‹±(ìš”ì•½ê³¼ ë¬´ê´€í•˜ê²Œ 100% ë°˜ì˜)
#
# [ìš”ì•½/í‘œí˜„ í•™ìŠµ ê³„ì¸µ]
#   - TF-IDF ê°€ì¤‘ ë²¡í„°(sentence_tfidf_vectors): ìì²´ êµ¬í˜„ (numpy/regex)
#   - TextRank(textrank_scores): ê·¸ë˜í”„ ê¸°ë°˜ ë­í‚¹(transition matrix ìˆ˜ë ´)
#   - MMR(mmr_select): ì¤‘ë³µ ì–µì œ ìš”ì•½(ê´€ë ¨ì„±-ë‹¤ì–‘ì„± ê· í˜•)
#   - ì„¸ì…˜ KB ê°€ì¤‘ì¹˜: ì—…ë¡œë“œ ëˆ„ì  ìš©ì–´ì— ê°€ì¤‘(Boost) â†’ ë„ë©”ì¸ ì í•© ìš”ì•½
#
# [NLG ê³„ì¸µ(ê·œì¹™ ê¸°ë°˜)]
#   - ë¬¸ì¥ ì™„í™”/ìì—°í™”(soften), ëª©ì ê²© ì¡°ì‚¬ ì‚½ì…(add_obj_particle)
#   - ì˜ˆë°©ì¡°ì¹˜ ì¡°ê° ë³µì›(repair_action_fragments) + ë™ì‚¬ íŒ¨í„´(ACTION_PAT)ë¡œ â€œ~ í•©ë‹ˆë‹¤.â€ ì¢…ê²° ë³´ì •
#   - ë„ë©”ì¸ í…œí”Œë¦¿(ì„ íƒ): í‚¤ì›Œë“œ íŠ¸ë¦¬ê±° ì¼ì¹˜ ì‹œ ë³´ìˆ˜ì ìœ¼ë¡œ ë¬¸ì¥ ì¹˜í™˜
#
# [ìƒì„±ê¸°]
#   - ìì—°ìŠ¤ëŸ¬ìš´ êµìœ¡ëŒ€ë³¸(make_structured_script) â€”â€” â¬… ì‚¬ë¡€/ì˜ˆë°©ì€ â€˜ì„¹ì…˜ íŒŒì„œâ€™ ê²°ê³¼ë¥¼ ìš°ì„  ì±„íƒ
#   - í•µì‹¬ìš”ì•½(make_concise_report) â€”â€” ìš”ì•½ ì¤‘ì‹¬ + (ê°€ëŠ¥ ì‹œ) ì„¹ì…˜ íŒŒì„œ ë³´ê°•
#
# [ì¶œë ¥]
#   - TXT/DOCX ë‹¤ìš´ë¡œë“œ: python-docx (Malgun Gothic)
#
# [ì˜¤í”ˆì†ŒìŠ¤/í”„ë ˆì„ì›Œí¬]
#   - Streamlit(ì›¹ UI), pdfminer.six(í…ìŠ¤íŠ¸ ì¶”ì¶œ), pypdfium2(PDF í™•ì¸), numpy(ìˆ˜ì¹˜ì—°ì‚°), regex(í† í¬ë‚˜ì´ì¦ˆ), python-docx(ë¬¸ì„œ)
#   - 100% ë¡œì»¬ ì¶”ë¡ (ìœ ë£Œ/ì™¸ë¶€ API ì—†ìŒ) â€” í•´ì»¤í†¤ ê°€ì´ë“œ ì¤€ìˆ˜
# ==========================================================

import io
import zipfile
import re
from collections import Counter
from typing import List, Dict, Tuple

import numpy as np
import regex as rxx
import streamlit as st
from docx import Document
from docx.shared import Pt

# ---- pdfminer ê²½ë¡œ í˜¸í™˜ ì²˜ë¦¬ ----
pdf_extract_text = None
try:
    from pdfminer.high_level import extract_text as _extract_text
    pdf_extract_text = _extract_text
except Exception:
    try:
        from pdfminer_high_level import extract_text as _extract_text_compat  # type: ignore
        pdf_extract_text = _extract_text_compat
    except Exception:
        pdf_extract_text = None

# pypdfium2: PDF í˜ì´ì§€ ì¡´ì¬/ìŠ¤ìº” ì¶”ì • í™•ì¸(í…ìŠ¤íŠ¸ ì¶”ì¶œì€ í•˜ì§€ ì•ŠìŒ)
import pypdfium2 as pdfium

st.set_page_config(page_title="OPS2TBM", page_icon="ğŸ¦º", layout="wide")

# -------------------- ì‹œë“œ KB --------------------
SEED_RISK_MAP = {
    "ì¤‘ë…":"ì¤‘ë…","ë–¨ì–´ì§":"ë–¨ì–´ì§","ë¼ì„":"ë¼ì„","ì§ˆì‹":"ì§ˆì‹","í™”ì¬":"í™”ì¬","ê¹”ë¦¼":"ê¹”ë¦¼",
    "ë§ìŒ":"ë§ìŒ","ê°ì „":"ê°ì „","ì§€ë¶•":"ì§€ë¶•ì‘ì—…","ì˜ˆì´ˆ":"ì˜ˆì´ˆ","í­ë°œ":"í­ë°œ","ì²œê³µê¸°":"ì²œê³µ",
    "ì„ ë°˜":"ì ˆì‚­","ì»¨ë² ì´ì–´":"í˜‘ì°©","ë¶€ë”ªí˜":"ì¶©ëŒ","ë¯¸ì„¸ë¨¼ì§€":"ë¯¸ì„¸ë¨¼ì§€","í¬ë ˆì¸":"ì–‘ì¤‘",
    "ë¬´ë„ˆì§":"ë¶•ê´´","ë¹„ê³„":"ë¹„ê³„","ì¶”ë½":"ì¶”ë½","í­ì—¼":"í­ì—¼","ë²Œëª©":"ë²Œëª©","ë‚™í•˜":"ë‚™í•˜","ë¶•ê´´":"ë¶•ê´´",
    "ê°±í¼":"ë¹„ê³„","ë°œíŒ":"ë¹„ê³„","í™”í•™ë¬¼ì§ˆ":"í™”í•™ë¬¼ì§ˆ","ë°€íê³µê°„":"ë°€íê³µê°„"
}
SEED_ACTIONS = [
    "ë°€íê³µê°„ì‘ì—… êµìœ¡ ë° í›ˆë ¨ ì‹¤ì‹œ","ì¶œì… ì „ ì¶©ë¶„í•œ í™˜ê¸° ì‹¤ì‹œ","ì‘ì—… ì „ ê°€ìŠ¤ë†ë„ ì¸¡ì • ë° ê¸°ë¡",
    "ì‘ì—… ìƒí™© ê°ì‹œì ë°°ì¹˜","ì¶œì…Â·í‡´ì¥ ì¸ì› ì ê²€","ë³´í˜¸ì¥êµ¬ ì—†ì´ êµ¬ì¡° ê¸ˆì§€",
    "MSDS í™•ì¸ ë° ìœ í•´ì„± êµìœ¡ ì‹¤ì‹œ","êµ­ì†Œë°°ê¸°ì¥ì¹˜ ì„¤ì¹˜Â·ê°€ë™","í™˜ê¸°ê°€ ë¶ˆì¶©ë¶„í•œ ê³µê°„ì—ì„œëŠ” ê¸‰ê¸°/ë°°ê¸°íŒ¬ ì‚¬ìš©",
    "ìœ ê¸°í™”í•©ë¬¼ ì·¨ê¸‰ ì‹œ ë°©ë…ë§ˆìŠ¤í¬(ê°ˆìƒ‰ ì •í™”í†µ) ì°©ìš©","ì†¡ê¸°ë§ˆìŠ¤í¬Â·ê³µê¸°í˜¸í¡ê¸° ì ì • ì‚¬ìš©",
    "ì˜ˆì´ˆê¸° ì •ì§€ í›„ ì´ë¬¼ì§ˆ ì œê±°Â·ì ê²€","ì˜ˆì´ˆÂ·ë²Œëª© ì‘ì—… ì•ˆì „ê±°ë¦¬ ìœ ì§€ ë° ëŒ€í”¼ë¡œ í™•ë³´",
    "ì‘ì—…ë°œíŒ ê²¬ê³ íˆ ì„¤ì¹˜ ë° ìƒíƒœ ì ê²€","ê°œêµ¬ë¶€Â·ê°œêµ¬ì°½ ì¶”ë½ ìœ„í—˜ êµ¬ê°„ ì•ˆì „ë‚œê°„ ì„¤ì¹˜",
    "ì•ˆì „ëŒ€ ì§€ì§€ì  ì—°ê²° ë° ë¼ì´í”„ë¼ì¸ ì‚¬ìš©","ìœ„í—˜êµ¬ì—­ ì„¤ì •Â·ì¶œì…í†µì œÂ·ê°ì‹œì ë°°ì¹˜",
    "ì–‘ì¤‘ ê³„íš ìˆ˜ë¦½ ë° ì‹ í˜¸ìˆ˜ ì§€ì •Â·í†µì‹  ìœ ì§€","íšŒì „ì²´Â·ë¬¼ë¦¼ì  ë°©í˜¸ì¥ì¹˜ ì„¤ì¹˜ ë° ì ê²€",
    "ì‘ì—… ì „ ì‘ì—…ê³„íšì„œ ì‘ì„± ë° ì‘ì—…ì§€íœ˜ì ì§€ì •","ê°œì¸ë³´í˜¸êµ¬ ì°©ìš©(ì•ˆì „ëª¨Â·ë³´í˜¸ì•ˆê²½Â·ì•ˆì „í™” ë“±)",
    "í™”ê¸°ì‘ì—… í—ˆê°€ ë° ì•ˆì „ì ê²€","ì •ë¹„Â·ì²­ì†ŒÂ·ì ê²€ ì‹œ ê¸°ê³„ ì „ì› ì°¨ë‹¨",
    # í™”í•™ë¬¼ì§ˆ/ì¤‘ë… ì˜ˆë°© ê´€ìš©ë¬¸ ì¶”ê°€
    "ë°€íê³µê°„ ì‘ì—… ì‹œ ì‚°ì†ŒÂ·ìœ í•´ê°€ìŠ¤ ë†ë„ ì¸¡ì •","ìœ„í—˜ë¬¼ì§ˆ ì·¨ê¸‰ ì‹œ MSDS ë¹„ì¹˜Â·ê²Œì‹œ ë° êµìœ¡",
    "ìœ„í—˜ë¬¼ì§ˆ ì·¨ê¸‰ ì‹œ ë¶ˆì¹¨íˆ¬ì„± ë³´í˜¸ë³µÂ·ë°©ë…ë§ˆìŠ¤í¬ ì°©ìš©","í™˜ê¸° ì‹¤ì‹œ ë° ê°ì‹œì¸ ë°°ì¹˜"
]
SEED_QUESTIONS = [
    "ì‘ì—… ì „ ì‘ì—…ê³„íšì„œì™€ ìœ„í—˜ì„±í‰ê°€ë¥¼ ê²€í† í–ˆìŠµë‹ˆê¹Œ?",
    "ê°œêµ¬ë¶€Â·ê°œêµ¬ì°½ ë“± ì¶”ë½ ìœ„í—˜ êµ¬ê°„ì— ì•ˆì „ë‚œê°„ì„ ì„¤ì¹˜í–ˆìŠµë‹ˆê¹Œ?",
    "ì‘ì—…ë°œíŒì´ ê²¬ê³ í•˜ê²Œ ì„¤ì¹˜ë˜ê³  ìƒíƒœê°€ ì–‘í˜¸í•©ë‹ˆê¹Œ?",
    "ì•ˆì „ëŒ€ ì—°ê²°ì ê³¼ ë¼ì´í”„ë¼ì¸ì´ í™•ë³´ë˜ì—ˆìŠµë‹ˆê¹Œ?",
    "êµ­ì†Œë°°ê¸°ì¥ì¹˜ë¥¼ ê°€ë™í•˜ê³  í™˜ê¸° ê²½ë¡œê°€ í™•ë³´ë˜ì—ˆìŠµë‹ˆê¹Œ?",
    "í˜¸í¡ë³´í˜¸êµ¬ê°€ ì‘ì—…ì— ì í•©í•˜ë©° ê´€ë¦¬ê°€ ë˜ê³  ìˆìŠµë‹ˆê¹Œ?",
    "ë°€íê³µê°„ ì¶œì…Â·í‡´ì¥ ì¸ì› ì ê²€ì´ ì´ë£¨ì–´ì§€ê³  ìˆìŠµë‹ˆê¹Œ?",
    "ì˜ˆì´ˆÂ·ë²Œëª© ì‘ì—… ì‹œ ì•ˆì „ê±°ë¦¬ì™€ ëŒ€í”¼ë¡œë¥¼ í™•ë³´í–ˆìŠµë‹ˆê¹Œ?",
    "ì–‘ì¤‘ ì‘ì—…ì— ì‹ í˜¸ìˆ˜ ì§€ì • ë° í†µì‹ ì²´ê³„ê°€ ë§ˆë ¨ë˜ì—ˆìŠµë‹ˆê¹Œ?",
    "íšŒì „ì²´Â·ë¬¼ë¦¼ì  ë°©í˜¸ì¥ì¹˜ê°€ ì •ìƒ ë™ì‘í•©ë‹ˆê¹Œ?"
]

# -------------------- ì„¸ì…˜ ìƒíƒœ --------------------
def _init_once():
    ss = st.session_state
    ss.setdefault("uploader_key", 0)
    ss.setdefault("kb_terms", Counter())
    ss.setdefault("kb_actions", [])
    ss.setdefault("kb_questions", [])
    ss.setdefault("domain_toggle", False)
    ss.setdefault("seed_loaded", False)
    ss.setdefault("last_file_diag", {})
    ss.setdefault("last_extracted_cache", "")
_init_once()

# -------------------- í•œê¸€ ì¡°ì‚¬ì§€ì› --------------------
def _has_final_consonant(k: str) -> bool:
    if not k: return False
    ch = k[-1]
    base = ord('ê°€'); code = ord(ch) - base
    if code < 0 or code > 11171: return False
    jong = code % 28
    return jong != 0

def add_obj_particle(noun: str) -> str:
    noun = noun.strip()
    if not noun: return noun
    particle = "ì„" if _has_final_consonant(noun[-1]) else "ë¥¼"
    return f"{noun}{particle}"

def tidy_korean_spaces(s: str) -> str:
    s = re.sub(r"\s+", " ", s)
    s = s.replace("ì „ì¶©ë¶„í•œ","ì „ ì¶©ë¶„í•œ").replace("ì „ì¶©ë¶„íˆ","ì „ ì¶©ë¶„íˆ")
    s = re.sub(r"\s([,.])", r"\1", s)
    return s.strip()

# -------------------- ì „ì²˜ë¦¬/íŒ¨í„´ --------------------
NOISE_PATTERNS = [
    r"^ì œ?\s?\d{4}\s?[-.]?\s?\d+\s?í˜¸$",
    r"^(ë™ì ˆê¸°\s*ì£¼ìš”ì‚¬ê³ |ì•ˆì „ì‘ì—…ë°©ë²•|ì½˜í…ì¸ \s*ë§í¬|ì±…ì\s*OPS|ìˆí¼\s*OPS)$",
    r"^(í¬ìŠ¤í„°|ì±…ì|ìŠ¤í‹°ì»¤|ì½˜í…ì¸  ë§í¬)$",
    r"^(ìŠ¤ë§ˆíŠ¸í°\s*APP|ì¤‘ëŒ€ì¬í•´\s*ì‚¬ì´ë Œ|ì‚°ì—…ì•ˆì „í¬í„¸|ê³ ìš©ë…¸ë™ë¶€)$",
    r"^https?://\S+$", r"^\(?\s*PowerPoint\s*í”„ë ˆì  í…Œì´ì…˜\s*\)?$",
    r"^ì•ˆì „ë³´ê±´ìë£Œì‹¤.*$", r"^ë°°í¬ì²˜\s+.*$", r"^í™ˆí˜ì´ì§€\s+.*$",
    r"^VR\s+.*$", r"^ë¦¬í”Œë¦¿\s+.*$", r"^ë™ì˜ìƒ\s+.*$", r"^APP\s+.*$",
    r".*ê²€ìƒ‰í•´\s*ë³´ì„¸ìš”.*$",
]
BULLET_PREFIX = r"^[\s\-\â€¢\â—\â–ª\â–¶\â–·\Â·\*\u25CF\u25A0\u25B6\u25C6\u2022\u00B7\u279C\u27A4\u25BA\u25AA\u25AB\u2611\u2713\u2714\u2716\u2794\u27A2]+"
DATE_PAT = r"([â€™']?\d{2,4})\.\s?(\d{1,2})\.\s?(\d{1,2})\.?"
META_PATTERNS = [
    r"<\s*\d+\s*ëª…\s*ì‚¬ë§\s*>", r"<\s*\d+\s*ëª…\s*ì‚¬ìƒ\s*>", r"<\s*\d+\s*ëª…\s*ì˜ì‹ë¶ˆëª…\s*>",
    r"<\s*ì‚¬ë§\s*\d+\s*ëª…\s*>", r"<\s*ì‚¬ìƒ\s*\d+\s*ëª…\s*>"
]
STOP_TERMS = set("""
ë° ë“± ê´€ë ¨ ì‚¬í•­ ë‚´ìš© ì˜ˆë°© ì•ˆì „ ì‘ì—… í˜„ì¥ êµìœ¡ ë°©ë²• ê¸°ì¤€ ì¡°ì¹˜
ì‹¤ì‹œ í™•ì¸ í•„ìš” ê²½ìš° ëŒ€ìƒ ì‚¬ìš© ê´€ë¦¬ ì ê²€ ì ìš© ì •ë„ ì£¼ì˜ ì¤‘ ì „ í›„
ì£¼ìš” ì‚¬ë¡€ ì•ˆì „ì‘ì—…ë°©ë²• í¬ìŠ¤í„° ë™ì˜ìƒ ë¦¬í”Œë¦¿ ê°€ì´ë“œ ìë£Œì‹¤ ê²€ìƒ‰
í‚¤ë©”ì„¸ì§€ êµìœ¡í˜ì‹ ì‹¤ ì•ˆì „ë³´ê±´ê³µë‹¨ ê³µë‹¨ ìë£Œ êµ¬ë… ì•ˆë‚´ ì—°ë½ ì°¸ê³  ì¶œì²˜
ì†Œì¬ ì†Œì¬ì§€ ìœ„ì¹˜ ì¥ì†Œ ì§€ì—­ ì‹œêµ°êµ¬ ì„œìš¸ ì¸ì²œ ë¶€ì‚° ëŒ€êµ¬ ëŒ€ì „ ê´‘ì£¼ ìš¸ì‚° ì„¸ì¢… ê²½ê¸°ë„ ì¶©ì²­ ì „ë¼ ê²½ìƒ ê°•ì› ì œì£¼
ëª… ê±´ í˜¸ í˜¸ì°¨ í˜¸ìˆ˜ í˜ì´ì§€ ìª½ ë¶€ë¡ ì°¸ê³  ê·¸ë¦¼ í‘œ ëª©ì°¨
ì•ˆì „ë³´ê±´ ops í‚¤ ë©”ì„¸ì§€ í‚¤ë©”ì„¸ì§€ ìë£Œ opsêµì•ˆ êµì•ˆ
""".split())
LABEL_DROP_PAT = [
    r"^\d+$", r"^\d{2,4}[-_]\d{1,}$", r"^\d{4}$", r"^(ì œ)?\d+í˜¸$", r"^(í˜¸|í˜¸ìˆ˜|í˜¸ì°¨)$",
    r"^(ì‚¬ì—…ì¥|ì—…ì²´|ì†Œì¬|ì†Œì¬ì§€|ì¥ì†Œ|ì§€ì—­)$", r"^\d+\s*(ëª…|ê±´)$"
]
RISK_KEYWORDS = dict(SEED_RISK_MAP)

def tokens(s: str) -> List[str]:
    return rxx.findall(r"[ê°€-í£a-z0-9]{2,}", s.lower())

def normalize_text(t: str) -> str:
    t = t.replace("\x0c","\n")
    t = re.sub(r"[ \t]+\n","\n", t)
    t = re.sub(r"\n{3,}","\n\n", t)
    return t.strip()

def strip_noise_line(line: str) -> str:
    s = (line or "").strip()
    if not s: return ""
    s = re.sub(BULLET_PREFIX,"", s).strip()
    for pat in NOISE_PATTERNS:
        if re.match(pat, s, re.IGNORECASE):
            return ""
    s = re.sub(r"https?://\S+","", s).strip()
    s = s.strip("â€¢â—â–ªâ–¶â–·Â·-â€”â€“")
    return s

def _looks_like_heading(s: str) -> bool:
    return bool(re.search(r"(ë°©ë²•|ìˆ˜ì¹™|ëŒ€ì±…|ì•ˆì „ì¡°ì¹˜|ì˜ˆë°©|ì‘ì—…ë°©ë²•|ì‚¬ê³ ì‚¬ë¡€|ì£¼ìš”\s*ì‚¬ê³ ì‚¬ë¡€)\s*[:ï¼š]?$", s))

def merge_broken_lines(lines: List[str]) -> List[str]:
    out, buf = [], ""
    for raw in lines:
        s = strip_noise_line(raw)
        if not s: continue
        if _looks_like_heading(s) or s.endswith((":", "ï¼š", "-", "Â·")):
            if buf: out.append(buf)
            buf = s; continue
        if buf:
            if buf.endswith((":", "ï¼š", "-", "Â·")):
                buf = tidy_korean_spaces(buf.rstrip(" :ï¼š-Â·") + " " + s); continue
            if (len(buf) < 20 and not re.search(r"[.?!ë‹¤]$", buf)) or (len(s) < 20 and not re.search(r"[.?!ë‹¤]$", s)):
                buf = tidy_korean_spaces(buf + " " + s); continue
            if not re.search(r"[.?!ë‹¤]$", buf):
                buf = tidy_korean_spaces(buf + " " + s); continue
            out.append(buf); buf = s
        else:
            buf = s
    if buf: out.append(buf)
    return out

def combine_date_with_next(lines: List[str]) -> List[str]:
    out = []; i = 0
    while i < len(lines):
        cur = strip_noise_line(lines[i])
        if re.search(DATE_PAT, cur) and (i+1) < len(lines):
            nxt = strip_noise_line(lines[i+1])
            if re.search(r"(ì‚¬ë§|ì‚¬ìƒ|ì‚¬ê³ |ì¤‘ë…|í™”ì¬|ë¶•ê´´|ì§ˆì‹|ì¶”ë½|ê¹”ë¦¼|ë¶€ë”ªí˜|ê°ì „|í­ë°œ)", nxt):
                m = re.search(DATE_PAT, cur)
                y, mo, d = m.groups()
                y = int(str(y).replace("â€™","").replace("'","")); y = 2000 + y if y < 100 else y
                out.append(f"{int(y)}ë…„ {int(mo)}ì›” {int(d)}ì¼, {nxt}")
                i += 2; continue
        out.append(cur); i += 1
    return out

CASE_JOIN_TRIG = ("ì“°ëŸ¬ì§€ì","êµ¬ì¡°í•˜ë˜ ì¤‘","ì°¨ë¡€ë¡œ","ì´ì–´","ì´í›„","ë™ì‹œì—","ê²°êµ­","ê·¸ ê³¼ì •ì—ì„œ","ì™¸ë¶€ì— ìˆë˜","í˜„ì¥ì— ìˆë˜")
CASE_KEYWORDS = ("ì‚¬ë§","ì‚¬ìƒ","ì¤‘ë…","ì¶”ë½","ë¶•ê´´","ë‚™í•˜","ì§ˆì‹","ë¼ì„","ê¹”ë¦¼","ë¶€ë”ªí˜","ê°ì „","í­ë°œ")

def stitch_case_blocks(sents: List[str]) -> List[str]:
    if not sents: return sents
    out = []; i = 0
    while i < len(sents):
        cur = sents[i].strip(); merged = cur; j = i + 1; merged_any = False
        while j < len(sents):
            nxt = sents[j].strip()
            cond_keyword = (any(k in cur for k in CASE_KEYWORDS) and any(k in nxt for k in CASE_KEYWORDS))
            cond_trigger = (any(t in nxt for t in CASE_JOIN_TRIG) or any(t in cur for t in CASE_JOIN_TRIG))
            if cond_keyword or cond_trigger:
                sep = ", " if not merged.endswith(("ë‹¤.","ìŠµë‹ˆë‹¤.","í–ˆë‹¤.",".")) else " "
                merged = tidy_korean_spaces(merged.rstrip(" .") + sep + nxt.lstrip(" ,"))
                cur = merged; j += 1; merged_any = True
            else:
                break
        out.append(merged); i = j if merged_any else i + 1
    seen, dedup = set(), []
    for s in out:
        k = re.sub(r"\s+","", s)
        if k not in seen:
            seen.add(k); dedup.append(s)
    return dedup

def preprocess_text_to_sentences(text: str) -> List[str]:
    # ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ â†’ ë¬¸ì¥ ë¦¬ìŠ¤íŠ¸
    text = normalize_text(text)
    raw_lines = [ln for ln in text.splitlines() if ln.strip()]
    lines = merge_broken_lines(raw_lines)
    lines = combine_date_with_next(lines)
    joined = "\n".join(lines)
    raw = rxx.split(r"(?<=[\.!\?]|ë‹¤\.)\s+|\n+", joined)
    sents = []
    for s in raw:
        s2 = strip_noise_line(s)
        if not s2: continue
        if re.search(r"(ì£¼ìš”ì‚¬ê³ |ì•ˆì „ì‘ì—…ë°©ë²•|ì½˜í…ì¸ ë§í¬|ì£¼ìš” ì‚¬ê³ ê°œìš”)$", s2): continue
        if len(re.sub(r"\s+","", s2)) < 4:  # [ì™„í™”] ë„ˆë¬´ ì§§ì€ ë¼ì¸ë§Œ ì œê±°(6â†’4)
            continue
        sents.append(s2)
    sents = stitch_case_blocks(sents)
    return sents

# -------------------- (NEW) ì„¹ì…˜ íŒŒì„œ: ì‚¬ë¡€/ì˜ˆë°© ë¶ˆë¦¿ 100% ë°˜ì˜ --------------------
#  â€¢ ì‚¬ë¡€ ì„¹ì…˜ í—¤ë” ì˜ˆ: "ì£¼ìš” ì‚¬ê³ ì‚¬ë¡€", "ì‚¬ê³ ì‚¬ë¡€"
#  â€¢ ì˜ˆë°© ì„¹ì…˜ í—¤ë” ì˜ˆ: "ì•ˆì „ ì‘ì—…ë°©ë²•", "ë°€íê³µê°„ì‘ì—…ì‹œ", "ìœ„í—˜ë¬¼ì§ˆ ì·¨ê¸‰ì‹œ", "ì˜ˆë°©ìˆ˜ì¹™", "ì‹¤ì²œìˆ˜ì¹™", "ì˜ˆë°©ì¡°ì¹˜"
SECTION_HEADERS_CASE = [
    r"ì£¼ìš”\s*ì‚¬ê³ ì‚¬ë¡€", r"ì‚¬ê³ ì‚¬ë¡€", r"ì‚¬ê³ \s*ì‚¬ë¡€"
]
SECTION_HEADERS_PREV = [
    r"ì•ˆì „\s*ì‘ì—…ë°©ë²•", r"ë°€íê³µê°„\s*ì‘ì—…\s*ì‹œ", r"ë°€íê³µê°„ì‘ì—…\s*ì‹œ",
    r"ìœ„í—˜ë¬¼ì§ˆ\s*ì·¨ê¸‰\s*ì‹œ", r"ì˜ˆë°©\s*ìˆ˜ì¹™", r"ì‹¤ì²œ\s*ìˆ˜ì¹™", r"ì˜ˆë°©\s*ì¡°ì¹˜",
    r"ì•ˆì „\s*ìˆ˜ì¹™", r"ì‘ì—…\s*ìˆ˜ì¹™"
]

def _compile_headers(headers: List[str]) -> List[re.Pattern]:
    return [re.compile(h, re.IGNORECASE) for h in headers]

HDR_CASE = _compile_headers(SECTION_HEADERS_CASE)
HDR_PREV = _compile_headers(SECTION_HEADERS_PREV)

def split_keep_lines(text: str) -> List[str]:
    # í—¤ë”/ë¶ˆë¦¿ íŒŒì‹±ì„ ìœ„í•´ ì›ë¬¸ ë¼ì¸ ê·¸ëŒ€ë¡œ í™•ë³´(ê°€ë²¼ìš´ ì •ê·œí™”ë§Œ)
    t = normalize_text(text)
    lines = [ln.rstrip() for ln in t.splitlines()]
    return lines

def _is_header(line: str, hdrs: List[re.Pattern]) -> bool:
    s = strip_noise_line(line)
    return any(h.search(s) for h in hdrs)

def _is_bullet(line: str) -> bool:
    return bool(re.match(BULLET_PREFIX, line.strip()) or re.match(r"^\s*[\-Â·â€¢â–¶â–·\*]\s+", line.strip()))

def extract_section_bullets(text: str, which: str = "case") -> List[str]:
    """ì›ë¬¸ì—ì„œ ì„¹ì…˜ í—¤ë”ë¥¼ ì¸ì‹í•˜ê³ , ë°”ë¡œ ì•„ë˜ ë¶ˆë¦¿/í–‰ì„ ëª¨ë‘ ìˆ˜ì§‘í•´ ë°˜í™˜."""
    lines = split_keep_lines(text)
    hdrs = HDR_CASE if which == "case" else HDR_PREV
    items: List[str] = []
    capture = False
    for i, raw in enumerate(lines):
        s = raw.strip()
        if not s: 
            if capture: break
            continue
        if _is_header(raw, hdrs):
            capture = True
            continue
        if capture:
            # ë‹¤ìŒ í—¤ë”ê°€ ë‚˜ì˜¤ë©´ ì¤‘ë‹¨
            if _is_header(raw, HDR_CASE + HDR_PREV):
                break
            # ë¶ˆë¦¿/ì¼ë°˜í–‰ ëª¨ë‘ í—ˆìš©í•˜ë˜, ë…¸ì´ì¦ˆëŠ” ì œê±°
            clean = strip_noise_line(raw)
            if not clean: 
                # ë¹ˆ ì¤„ì´ë©´ ì„¹ì…˜ ì¢…ë£Œë¡œ ê°„ì£¼
                if _is_bullet(raw) is False:
                    # ë¹ˆ ì¤„ ë’¤ ê³„ì† ë¶ˆë¦¿ì´ë©´ ê°™ì€ ì„¹ì…˜ì¼ ìˆ˜ ìˆì–´ ìœ ì§€
                    pass
                continue
            items.append(clean)
    # ì¤„ë°”ê¿ˆìœ¼ë¡œ ëŠê¸´ ë¶ˆë¦¿ì„ ê²°í•©(ì˜ˆ: "ë°€íê³µê°„ ì‘ì—… ì‹œ" / "í™˜ê¸° ì‹¤ì‹œ ë° ê°ì‹œì¸ ë°°ì¹˜")
    merged = merge_broken_lines(items)
    return [x for x in merged if len(re.sub(r"\s+","", x)) >= 2]

# -------------------- PDF ì²˜ë¦¬/ì§„ë‹¨ --------------------
def read_pdf_text_from_bytes(b: bytes, fname: str = "") -> str:
    t = ""
    try:
        if pdf_extract_text is not None:
            with io.BytesIO(b) as bio:
                t = pdf_extract_text(bio) or ""
        else:
            st.warning("âš ï¸ pdfminer ì¶”ì¶œ ëª¨ë“ˆì„ ë¶ˆëŸ¬ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. í…ìŠ¤íŠ¸ ì¶”ì¶œì„ ê±´ë„ˆëœë‹ˆë‹¤.")
            t = ""
    except Exception:
        t = ""
    t = normalize_text(t)
    if len(t.strip()) < 10:
        try:
            with io.BytesIO(b) as bio:
                pdf = pdfium.PdfDocument(bio)
                if len(pdf) > 0 and not t.strip():
                    st.warning("âš ï¸ ì´ë¯¸ì§€/ìŠ¤ìº” PDFë¡œ ë³´ì…ë‹ˆë‹¤. í˜„ì¬ OCR ë¯¸ì§€ì›.")
        except Exception:
            pass
    st.session_state["last_file_diag"] = {
        "name": fname, "size_bytes": len(b), "extracted_chars": len(t),
        "note": "empty_or_scanned" if (len(t.strip()) < 10) else "ok"
    }
    return t

# -------------------- ìš”ì•½/ê°€ì¤‘ --------------------
def tokens_for_vec(s: str) -> List[str]:
    return tokens(s)

def sentence_tfidf_vectors(sents: List[str], kb_boost: Dict[str, float] = None) -> Tuple[np.ndarray, List[str]]:
    toks = [tokens_for_vec(s) for s in sents]
    vocab: Dict[str,int] = {}
    for ts in toks:
        for t in ts:
            if t not in vocab: vocab[t] = len(vocab)
    if not vocab:
        return np.zeros((len(sents),0), dtype=np.float32), []
    M = np.zeros((len(sents), len(vocab)), dtype=np.float32)
    df = np.zeros((len(vocab),), dtype=np.float32)
    for i, ts in enumerate(toks):
        for t in ts:
            w = 1.0
            if kb_boost and t in kb_boost: w *= kb_boost[t]
            M[i, vocab[t]] += w
        for t in set(ts):
            df[vocab[t]] += 1.0
    N = float(len(sents))
    idf = np.log((N+1.0)/(df+1.0)) + 1.0
    M *= idf
    if kb_boost:
        for t, idx in vocab.items():
            if t in kb_boost: M[:, idx] *= (1.0 + 0.2*kb_boost[t])
    M /= (np.linalg.norm(M, axis=1, keepdims=True) + 1e-8)
    return M, list(vocab.keys())

def cosim(X: np.ndarray) -> np.ndarray:
    if X.size == 0: return np.zeros((X.shape[0], X.shape[0]), dtype=np.float32)
    S = np.clip(X @ X.T, 0.0, 1.0); np.fill_diagonal(S, 0.0)
    return S

def textrank_scores(sents: List[str], X: np.ndarray, d: float=0.85, max_iter: int=60, tol: float=1e-4) -> List[float]:
    n = len(sents)
    if n == 0: return []
    W = cosim(X); row = W.sum(axis=1, keepdims=True)
    P = np.divide(W, row, out=np.zeros_like(W), where=row>0)
    r = np.ones((n,1), dtype=np.float32)/n; tel = np.ones((n,1), dtype=np.float32)/n
    for _ in range(max_iter):
        r2 = d*(P.T @ r) + (1-d)*tel
        if np.linalg.norm(r2-r,1) < tol: r = r2; break
        r = r2
    return [float(v) for v in r.flatten()]

def mmr_select(sents: List[str], scores: List[float], X: np.ndarray, k: int, lam: float=0.7) -> List[int]:
    S = cosim(X); sel: List[int] = []; rem = set(range(len(sents)))
    while rem and len(sel) < k:
        best, val = None, -1e9
        for i in rem:
            rel = scores[i]; div = max((S[i,j] for j in sel), default=0.0)
            sc = lam*rel - (1-lam)*div
            if sc > val: val, best = sc, i
        sel.append(best); rem.remove(best)
    return sel

def ai_extract_summary(text: str, limit: int=8) -> List[str]:
    # â›ï¸ ìš”ì•½ íŒŒì´í”„ë¼ì¸(ì°¸ê³ : ì˜ˆë°©/ì‚¬ë¡€ëŠ” ì„¹ì…˜ íŒŒì„œê°€ ìš°ì„  ë³´ì¥)
    sents = preprocess_text_to_sentences(text)
    if not sents: return []
    kb = st.session_state["kb_terms"]; total = sum(kb.values()) or 1
    kb_boost = {t: 1.0 + (cnt/total)*3.0 for t, cnt in kb.items()} if kb else None
    X, _ = sentence_tfidf_vectors(sents, kb_boost=kb_boost)
    scores = textrank_scores(sents, X)
    idx = mmr_select(sents, scores, X, limit, lam=0.7)
    return [sents[i] for i in idx]

# -------------------- í…œí”Œë¦¿/ë¶„ë¥˜/NLG --------------------
DOMAIN_TEMPLATES = [
    ({"ë¹„ê³„","ë°œíŒ","ê°±í¼","ì¶”ë½"}, "ì‘ì—…ë°œíŒì„ ê²¬ê³ í•˜ê²Œ ì„¤ì¹˜í•˜ê³  ì•ˆì „ë‚œê°„ ë° ì¶”ë½ë°©í˜¸ë§ì„ í™•ë³´í•©ë‹ˆë‹¤."),
    ({"ì•ˆì „ë‚œê°„","ë‚œê°„","ê°œêµ¬ë¶€"}, "ê°œêµ¬ë¶€Â·ê°œêµ¬ì°½ ë“± ì¶”ë½ ìœ„í—˜ êµ¬ê°„ì— ì•ˆì „ë‚œê°„ì„ ì„¤ì¹˜í•©ë‹ˆë‹¤."),
    ({"MSDS","êµ­ì†Œë°°ê¸°","í™˜ê¸°"}, "ì·¨ê¸‰ ë¬¼ì§ˆì˜ MSDSë¥¼ í™•ì¸í•˜ê³  êµ­ì†Œë°°ê¸°ì¥ì¹˜ë¥¼ ê°€ë™í•˜ì—¬ ì¶©ë¶„íˆ í™˜ê¸°í•©ë‹ˆë‹¤."),
    ({"ì˜ˆì´ˆ","ë²Œëª©","ì˜ˆì´ˆê¸°"}, "ì˜ˆì´ˆÂ·ë²Œëª© ì‘ì—… ì‹œ ì‘ì—…ì ê°„ ì•ˆì „ê±°ë¦¬ë¥¼ ìœ ì§€í•˜ê³  ëŒ€í”¼ë¡œë¥¼ í™•ë³´í•©ë‹ˆë‹¤."),
    ({"í¬ë ˆì¸","ì–‘ì¤‘"}, "ì–‘ì¤‘ ê³„íšì„ ìˆ˜ë¦½í•˜ê³  ì‹ í˜¸ìˆ˜ë¥¼ ì§€ì •í•˜ì—¬ í†µì‹ ì„ ìœ ì§€í•©ë‹ˆë‹¤."),
    ({"ì»¨ë² ì´ì–´","í˜‘ì°©","íšŒì „ì²´"}, "íšŒì „ì²´Â·ë¬¼ë¦¼ì  ì ‘ì´‰ì„ ë°©ì§€í•˜ë„ë¡ ë°©í˜¸ì¥ì¹˜ë¥¼ ì„¤ì¹˜í•˜ê³  ì ê²€í•©ë‹ˆë‹¤."),
]

def jaccard(a: set, b: set) -> float:
    return len(a & b) / (len(a | b) + 1e-8)

ACTION_VERBS = [
    "ì„¤ì¹˜","ë°°ì¹˜","ì°©ìš©","ì ê²€","í™•ì¸","ì¸¡ì •","ê¸°ë¡","í‘œì‹œ","ì œê³µ","ë¹„ì¹˜","ë³´ê³ ","ì‹ ê³ ",
    "êµìœ¡","ì£¼ì§€","ì¤‘ì§€","í†µì œ","íœ´ì‹","í™˜ê¸°","ì°¨ë‹¨","êµëŒ€","ë°°ì œ","ë°°ë ¤","ê°€ë™","ì¤€ìˆ˜",
    "ìš´ì˜","ìœ ì§€","êµì²´","ì •ë¹„","ì²­ì†Œ","ê³ ì •","ê²©ë¦¬","ë³´í˜¸","ë³´ìˆ˜","ì‘ì„±","ì§€ì •"
]
ACTION_PAT = (
    r"(?P<obj>[ê°€-í£a-zA-Z0-9Â·\(\)\[\]\/\-\s]{2,}?)\s*(?P<verb>" + "|".join(ACTION_VERBS) + r"|ì‹¤ì‹œ|ìš´ì˜|ê´€ë¦¬)\b"
    r"|(?P<obj2>[ê°€-í£a-zA-Z0-9Â·\(\)\[\]\/\-\s]{2,}?)\s*(ì„|ë¥¼)\s*(?P<verb2>" + "|".join(ACTION_VERBS) + r"|ì‹¤ì‹œ|ìš´ì˜|ê´€ë¦¬)\b"
)

def is_meaningful_sentence(s: str) -> bool:
    raw = re.sub(r"\s+","", s)
    if len(raw) < 4:
        return False
    if re.fullmatch(r"[ê°€-í£\s]*í•©ë‹ˆë‹¤\.", s.strip()):
        return False
    return True

def soften(s: str) -> str:
    s = s.replace("í•˜ì—¬ì•¼","í•´ì•¼ í•©ë‹ˆë‹¤").replace("í•œë‹¤","í•©ë‹ˆë‹¤").replace("í•œë‹¤.","í•©ë‹ˆë‹¤.")
    s = s.replace("ë°”ëë‹ˆë‹¤","í•´ì£¼ì„¸ìš”").replace("í™•ì¸ ë°”ëŒ","í™•ì¸í•´ì£¼ì„¸ìš”")
    s = s.replace("ê¸ˆì§€í•œë‹¤","ê¸ˆì§€í•©ë‹ˆë‹¤").replace("í•„ìš”í•˜ë‹¤","í•„ìš”í•©ë‹ˆë‹¤")
    s = re.sub(r"^\(([^)]+)\)\s*","", s)
    for pat in META_PATTERNS:
        s = re.sub(pat,"", s).strip()
    s = re.sub(BULLET_PREFIX,"", s).strip(" -â€¢â—\t")
    return tidy_korean_spaces(s)

def is_accident_sentence(s: str) -> bool:
    if any(w in s for w in ["ì˜ˆë°©","ëŒ€ì±…","ì§€ì¹¨","ìˆ˜ì¹™"]):
        return False
    return bool(re.search(DATE_PAT, s) or re.search(r"(ì‚¬ë§|ì‚¬ìƒ|ì‚¬ê³ |ì¤‘ë…|í™”ì¬|ë¶•ê´´|ì§ˆì‹|ì¶”ë½|ê¹”ë¦¼|ë¶€ë”ªí˜|ê°ì „|í­ë°œ)", s))

def is_prevention_sentence(s: str) -> bool:
    return any(w in s for w in ["ì˜ˆë°©","ëŒ€ì±…","ì§€ì¹¨","ìˆ˜ì¹™","ì•ˆì „ì¡°ì¹˜","ì‘ì—…ë°©ë²•"])

def is_risk_sentence(s: str) -> bool:
    return any(w in s for w in ["ìœ„í—˜","ìš”ì¸","ì›ì¸","ì¦ìƒ","ê²°ë¹™","ê°•í’","í­ì—¼","ë¯¸ì„¸ë¨¼ì§€","íšŒì „ì²´","ë¹„ì‚°","ë§ë¦¼","ì¶”ë½","ë‚™í•˜","í˜‘ì°©"])

def _domain_template_apply(s: str, base_text: str) -> str:
    if not st.session_state.get("domain_toggle"): return s
    sent_toks = set(tokens(s)); base_toks = set(tokens(base_text))
    if jaccard(sent_toks, base_toks) < 0.05: return s
    best = None; best_hits = 0
    for triggers, render in DOMAIN_TEMPLATES:
        if (sent_toks & triggers) and (base_toks & triggers):
            hits = len((sent_toks | base_toks) & triggers)
            if hits > best_hits: best_hits = hits; best = render
    return best if best else s

def to_action_sentence(s: str, base_text: str) -> str:
    s2 = soften(s)
    s2 = re.sub(r"(ìœ„ê¸°íƒˆì¶œ\s*ì•ˆì „ë³´ê±´)", "", s2).strip()
    s2 = re.sub(r"\s*ì—\s*ë”°ë¥¸\s*", " ì‹œ ", s2)
    s2 = re.sub(r"\s*ì—\s*ë”°ë¼\s*", " ì‹œ ", s2)
    s2_tpl = _domain_template_apply(s2, base_text)
    if s2_tpl != s2:
        txt = s2_tpl
        if not txt.endswith(("ë‹¤.","í•©ë‹ˆë‹¤.","ìŠµë‹ˆë‹¤.")):
            txt = txt.rstrip(" .") + " í•©ë‹ˆë‹¤."
        return tidy_korean_spaces(txt)
    m = re.search(ACTION_PAT, s2)
    if not m:
        nounish = re.sub(r"(ì˜|ì—|ì—ì„œ|ì„|ë¥¼|ì™€|ê³¼|ë°)$","", s2).strip()
        if nounish and len(nounish) >= 4:
            guess_verb = "ì„¤ì¹˜" if any(k in nounish for k in ["ë‚œê°„","ë°©í˜¸ë§","ë°œíŒ","ë°©í˜¸ì¥ì¹˜","ì¥ë¹„","ì¥ì¹˜","í‘œì§€"]) else "í™•ì¸"
            obj = add_obj_particle(nounish)
            return tidy_korean_spaces(f"{obj} {guess_verb} í•©ë‹ˆë‹¤.")
        txt = s2 if s2.endswith(("ë‹ˆë‹¤.","í•©ë‹ˆë‹¤.","ë‹¤.")) else (s2.rstrip(" .") + " í•©ë‹ˆë‹¤.")
        return tidy_korean_spaces(txt)
    obj = (m.group("obj") or m.group("obj2") or "").strip()
    verb = (m.group("verb") or m.group("verb2") or "ì‹¤ì‹œ").strip()
    if obj and not re.search(r"(ì„|ë¥¼|ì—|ì—ì„œ|ê³¼|ì™€|ì˜)$", obj):
        obj = add_obj_particle(obj)
    prefix = "ë°˜ë“œì‹œ " if "ì„¤ì¹˜" in verb else ("ì‘ì—… ì „ " if verb in ("í™•ì¸","ì ê²€","ì¸¡ì •","ê¸°ë¡","ì‘ì„±","ì§€ì •") else "")
    core = tidy_korean_spaces(f"{prefix}{obj} {verb}")
    if re.fullmatch(r"(ë°˜ë“œì‹œ |ì‘ì—… ì „ )?\s*(ì„|ë¥¼)\s*(ì‹¤ì‹œ|ê´€ë¦¬|ìš´ì˜)\s*$", core):
        if obj.strip():
            core = tidy_korean_spaces(f"{prefix}{obj} ì‹¤ì‹œ")
        else:
            core = "ì‘ì—… ì „ ì•ˆì „ì¡°ì¹˜ í™•ì¸"
    return core.rstrip(" .") + " í•©ë‹ˆë‹¤."

def repair_action_fragments(lines: List[str]) -> List[str]:
    out = []
    i = 0
    while i < len(lines):
        cur = soften(lines[i])
        cur_no_sp = re.sub(r"\s+","", cur)
        has_verb = bool(re.search(ACTION_PAT, cur)) or any(v in cur for v in ["í•©ë‹ˆë‹¤","í•œë‹¤","ì‹¤ì‹œ","ì„¤ì¹˜","ì°©ìš©","ì ê²€","í™•ì¸","ë°°ì¹˜"])
        if (len(cur_no_sp) < 20) and (not has_verb):
            merged = cur
            j = i + 1
            while j < len(lines):
                nxt = soften(lines[j])
                merged = tidy_korean_spaces(merged + " " + nxt)
                if re.search(ACTION_PAT, merged) or any(v in merged for v in ["í•©ë‹ˆë‹¤","í•œë‹¤","ì‹¤ì‹œ","ì„¤ì¹˜","ì°©ìš©","ì ê²€","í™•ì¸","ë°°ì¹˜"]):
                    break
                j += 1
            out.append(merged); i = j + 1
        else:
            out.append(cur); i += 1
    return out

def classify_sentence(s: str) -> str:
    if is_accident_sentence(s): return "case"
    if re.search(ACTION_PAT, s) or is_prevention_sentence(s): return "action"
    if is_risk_sentence(s): return "risk"
    if "?" in s or "í™•ì¸" in s or "ì ê²€" in s: return "question"
    return "other"

# -------------------- KB ëˆ„ì /ì‹œë“œ --------------------
def seed_kb_once():
    if not st.session_state["seed_loaded"]:
        for t, k in SEED_RISK_MAP.items():
            if t not in RISK_KEYWORDS: RISK_KEYWORDS[t] = k
        for a in SEED_ACTIONS:
            if 2 <= len(a) <= 160:
                st.session_state["kb_actions"].append(a if a.endswith(("ë‹¤","ë‹¤.","í•©ë‹ˆë‹¤","í•©ë‹ˆë‹¤.")) else a + " í•©ë‹ˆë‹¤.")
        for q in SEED_QUESTIONS:
            st.session_state["kb_questions"].append(q if q.endswith("?") else q + "?")
        for t in SEED_RISK_MAP.keys():
            st.session_state["kb_terms"][t] += 5
        st.session_state["seed_loaded"] = True

def kb_ingest_text(text: str) -> None:
    if not (text or "").strip(): return
    sents = preprocess_text_to_sentences(text)
    for s in sents:
        for t in tokens(s):
            if len(t) >= 2:
                st.session_state["kb_terms"][t] += 1
                if re.search(r"(ì¶”ë½|ë‚™í•˜|ê¹”ë¦¼|ë¼ì„|ì¤‘ë…|ì§ˆì‹|í™”ì¬|í­ë°œ|ê°ì „|í­ì—¼|ë¶•ê´´|ë¹„ê³„|ê°±í¼|ì˜ˆì´ˆ|ë²Œëª©|ì»¨ë² ì´ì–´|í¬ë ˆì¸|ì§€ë¶•|ì„ ë°˜|ì²œê³µ|í™”í•™ë¬¼ì§ˆ|ë°€íê³µê°„)", t):
                    if t not in RISK_KEYWORDS: RISK_KEYWORDS[t] = t
    action_candidates = [s for s in sents if (re.search(ACTION_PAT, s) or is_prevention_sentence(s))]
    action_candidates = repair_action_fragments(action_candidates)
    for s in action_candidates:
        cand = to_action_sentence(s, text)
        if 2 <= len(cand) <= 180:
            st.session_state["kb_actions"].append(cand)
    for s in sents:
        if "?" in s or "í™•ì¸" in s or "ì ê²€" in s:
            q = soften(s if s.endswith("?") else s + " ë§ìŠµë‹ˆê¹Œ?")
            if 2 <= len(q) <= 160:
                st.session_state["kb_questions"].append(q)

def kb_prune() -> None:
    def dedup_keep_order(lst: List[str]) -> List[str]:
        seen, out = set(), []
        for x in lst:
            k = re.sub(r"\s+","", x)
            if k not in seen:
                seen.add(k); out.append(x)
        return out
    st.session_state["kb_actions"] = dedup_keep_order(st.session_state["kb_actions"])[:2000]
    st.session_state["kb_questions"] = dedup_keep_order(st.session_state["kb_questions"])[:800]
    st.session_state["kb_terms"] = Counter(dict(st.session_state["kb_terms"].most_common(4000)))

def kb_match_candidates(cands: List[str], base_text: str, limit: int, min_sim: float = 0.12) -> List[str]:
    bt = set(tokens(base_text))
    present_risks = {t for t in bt if (t in RISK_KEYWORDS or t in RISK_KEYWORDS.values())}
    scored: List[Tuple[float,str]] = []
    for c in cands:
        ct = set(tokens(c))
        cand_risks = {RISK_KEYWORDS.get(t, t) for t in ct if (t in RISK_KEYWORDS or t in RISK_KEYWORDS.values())}
        if cand_risks and not (cand_risks & present_risks):
            continue
        j = len(bt & ct) / (len(bt | ct) + 1e-8)
        if j >= min_sim:
            scored.append((j, c))
    scored.sort(key=lambda x: x[0], reverse=True)
    return [c for _, c in scored[:limit]]

# -------------------- ìƒì„±ê¸° --------------------
def naturalize_case_sentence(s: str) -> str:
    s = soften(s)
    death = re.search(r"ì‚¬ë§\s*(\d+)\s*ëª…", s)
    inj = re.search(r"ì‚¬ìƒ\s*(\d+)\s*ëª…", s)
    unconscious = re.search(r"ì˜ì‹ë¶ˆëª…", s)
    info = []
    if death: info.append(f"ê·¼ë¡œì {death.group(1)}ëª… ì‚¬ë§")
    if inj and not death: info.append(f"{inj.group(1)}ëª… ì‚¬ìƒ")
    if unconscious: info.append("ì˜ì‹ë¶ˆëª… ë°œìƒ")
    m = re.search(DATE_PAT, s); date_txt=""
    if m:
        y, mo, d = m.groups()
        y = int(str(y).replace("â€™","").replace("'","")); y = 2000 + y if y < 100 else y
        date_txt = f"{int(y)}ë…„ {int(mo)}ì›” {int(d)}ì¼, "
        s = s.replace(m.group(0), "").strip()
    s = s.strip(" ,.-")
    if not re.search(r"(ë‹¤\.|ì…ë‹ˆë‹¤\.|í–ˆìŠµë‹ˆë‹¤\.)$", s):
        if re.search(r"(ì‚¬ë§|ì‚¬ìƒ|ì¤‘ë…|ì¶”ë½|ë‚™í•˜|ë¶•ê´´|ì§ˆì‹|ë¼ì„|ê¹”ë¦¼|ë¶€ë”ªí˜|ê°ì „|í­ë°œ)\s*$", s):
            s = s.rstrip(" .") + "í–ˆìŠµë‹ˆë‹¤."
        elif re.search(r"(ì‚¬ê±´|ì‚¬ê³ )\s*$", s):
            s = s.rstrip(" .") + "ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
        else:
            s = s.rstrip(" .") + " ì‚¬ê³ ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
    if info and not s.endswith("í–ˆìŠµë‹ˆë‹¤."):
        s = tidy_korean_spaces(s.rstrip(" .") + " " + (", ".join(info)) + "í–ˆìŠµë‹ˆë‹¤.")
    return tidy_korean_spaces((date_txt + s).strip())

def make_structured_script(text: str, max_points: int=6) -> str:
    # ìì—°ìŠ¤ëŸ¬ìš´ êµìœ¡ëŒ€ë³¸(ë¬´ë£Œ) â€” âœ… ì‚¬ë¡€/ì˜ˆë°© ì„¹ì…˜ì€ ëª…ì‹œì  íŒŒì„œ ê²°ê³¼ë¥¼ â€˜ì „ë¶€â€™ ë°˜ì˜
    topic_label = dynamic_topic_label(text)

    # 1) ìš”ì•½ ì½”ì–´(ë„ì…/ìœ„í—˜/ì§ˆë¬¸ ë“± ë³´ê°•ìš©)
    core = [soften(s) for s in ai_extract_summary(text, max_points)] if max_points > 0 else []
    core_actions = [s for s in core if (re.search(ACTION_PAT, s) or is_prevention_sentence(s))]
    core_actions = repair_action_fragments(core_actions)

    # 2) ì‚¬ë¡€ ì„¹ì…˜(ì „ë¶€)
    case_block_raw = extract_section_bullets(text, which="case")
    cases_block = [naturalize_case_sentence(s) for s in case_block_raw if is_meaningful_sentence(s)]

    # 3) ì˜ˆë°© ì„¹ì…˜(ì „ë¶€)
    prev_block_raw = extract_section_bullets(text, which="prev")
    prev_block_raw = repair_action_fragments(prev_block_raw)
    prev_block = [to_action_sentence(s, text) for s in prev_block_raw if is_meaningful_sentence(s)]

    # 4) ìš”ì•½ ê¸°ë°˜ ë¶„ë¥˜(ë³´ì¡°)
    case_aux, risk_aux, ask_aux = [], [], []
    for s in core:
        c = classify_sentence(s)
        if c == "case": case_aux.append(naturalize_case_sentence(s))
        elif c == "risk": risk_aux.append(soften(s))
        elif c == "question": ask_aux.append(soften(s if s.endswith("?") else s + " ë§ìŠµë‹ˆê¹Œ?"))

    # 5) í–‰ë™ ë³´ì¡°(ìš”ì•½ì—ì„œ ë½‘íŒ ê²ƒ ì¶”ê°€)
    act_aux = [to_action_sentence(s, text) for s in core_actions if is_meaningful_sentence(s)]

    # 6) KB ë³´ê°•(ìœ ì‚¬ë„ ê¸°ë°˜ ì¶”ì²œ) â€” ì˜ˆë°©ì€ ì´ë¯¸ â€˜ì „ë¶€â€™ ë°˜ì˜í–ˆìœ¼ë¯€ë¡œ, ë¶€ì¡± ì‹œì—ë§Œ ë³´ì¡°
    acts = prev_block + act_aux
    if len(acts) < 3 and st.session_state["kb_actions"]:
        acts += kb_match_candidates(st.session_state["kb_actions"], text, 6, min_sim=0.12)

    # 7) ì‚¬ê³ /ìœ„í—˜/ì§ˆë¬¸ ë³‘í•©(ì¤‘ë³µ ì œê±°, ìˆœì„œ ìœ ì§€)
    def uniq_keep(seq: List[str]) -> List[str]:
        seen, out = set(), []
        for x in seq:
            k = re.sub(r"\s+","", x)
            if k not in seen:
                seen.add(k); out.append(x)
        return out

    cases = uniq_keep(cases_block + case_aux)         # âœ… ì‚¬ë¡€: ì œí•œ ì—†ìŒ(ì „ë¶€)
    risks  = uniq_keep(risk_aux)                      # ìœ„í—˜ìš”ì¸ì€ ìš”ì•½ ê¸°ë°˜(í•„ìš”ì‹œ ëŠ˜ë¦´ ìˆ˜ ìˆìŒ)
    asks   = uniq_keep(ask_aux or kb_match_candidates(st.session_state["kb_questions"], text, 3, min_sim=0.12))

    # 8) ì¶œë ¥ êµ¬ì„± â€” ì˜ˆë°©ì€ â€˜ì „ë¶€â€™ ë³´ì´ë„ë¡ ì œí•œ ì œê±°
    lines = []
    lines.append(f"ğŸ¦º TBM êµìœ¡ëŒ€ë³¸ â€“ {topic_label}\n")
    lines.append("â— ë„ì…")
    lines.append(f"ì˜¤ëŠ˜ì€ ìµœê·¼ ë°œìƒí•œ '{topic_label.replace(' ì¬í•´ì˜ˆë°©','')}' ì‚¬ê³  ì‚¬ë¡€ë¥¼ ì¤‘ì‹¬ìœ¼ë¡œ, ìš°ë¦¬ í˜„ì¥ì—ì„œ ê°™ì€ ì‚¬ê³ ë¥¼ ì˜ˆë°©í•˜ê¸° ìœ„í•œ ì•ˆì „ì¡°ì¹˜ë¥¼ í•¨ê»˜ ì‚´í´ë³´ê² ìŠµë‹ˆë‹¤.\n")

    if cases:
        lines.append("â— ì‚¬ê³  ì‚¬ë¡€")
        for c in cases:
            lines.append(f"- {c}")
        lines.append("")

    if risks:
        lines.append("â— ì£¼ìš” ìœ„í—˜ìš”ì¸")
        for r in risks:
            lines.append(f"- {r}")
        lines.append("")

    if acts:
        lines.append("â— ì˜ˆë°©ì¡°ì¹˜ / ì‹¤ì²œ ìˆ˜ì¹™")
        # âœ… ê°œìˆ˜ ì œí•œ ì—†ìŒ(ì›ë¬¸ ì„¹ì…˜ ë¶ˆë¦¿ ì „ë¶€ + ë³´ì¡°)
        for i, a in enumerate(acts, 1):
            lines.append(f"{i}ï¸âƒ£ {a}")
        lines.append("")

    if asks:
        lines.append("â— í˜„ì¥ ì ê²€ ì§ˆë¬¸")
        for q in asks:
            lines.append(f"- {q}")
        lines.append("")

    lines.append("â— ë§ˆë¬´ë¦¬ ë‹¹ë¶€")
    lines.append("ì˜ˆë°©ì¡°ì¹˜ëŠ” 'ì„ ì¡°ì¹˜ í›„ì‘ì—…'ì´ ì›ì¹™ì…ë‹ˆë‹¤. ì˜¤ëŠ˜ ì‘ì—… ì „, ê° ê³µì •ë³„ ìœ„í—˜ìš”ì¸ì„ ë‹¤ì‹œ í•œ ë²ˆ ì ê²€í•˜ê³  í•„ìš”í•œ ë³´í˜¸êµ¬ì™€ ì•ˆì „ì¡°ì¹˜ë¥¼ ë°˜ë“œì‹œ ì¤€ë¹„í•©ì‹œë‹¤.")
    lines.append("â— êµ¬í˜¸")
    lines.append("â€œí•œ ë²ˆ ë” í™•ì¸! í•œ ë²ˆ ë” ì ê²€!â€")
    return "\n".join(lines)

def make_concise_report(text: str, max_points: int=6) -> str:
    # í•µì‹¬ìš”ì•½ â€” ìš”ì•½ ì¤‘ì‹¬ + (ê°€ëŠ¥ ì‹œ) ì„¹ì…˜ íŒŒì„œ ì¼ë¶€ ë°˜ì˜
    sents = ai_extract_summary(text, max_points)
    sents = [soften(s) for s in sents if not re.match(r"(ë°°í¬ì²˜|ì£¼ì†Œ|í™ˆí˜ì´ì§€|VR|ë¦¬í”Œë¦¿)", s)]
    # ì„¹ì…˜ íŒŒì„œ(ì‚¬ë¡€/ì˜ˆë°©)ì—ì„œ ì¼ë¶€ ì°¨ìš©í•˜ì—¬ ìš”ì•½ì— ë³´ê°•(ë„ˆë¬´ ê¸¸ì–´ì§€ì§€ ì•Šê²Œ ìƒí•œë§Œ ì„¤ì •)
    cases_blk = [naturalize_case_sentence(s) for s in extract_section_bullets(text, "case")]
    prev_blk  = [to_action_sentence(s, text) for s in repair_action_fragments(extract_section_bullets(text, "prev"))]

    act_src = [s for s in sents if (not is_accident_sentence(s)) and (is_prevention_sentence(s) or re.search(ACTION_PAT, s))]
    act_src = repair_action_fragments(act_src)
    cases = [naturalize_case_sentence(s) for s in sents if is_accident_sentence(s)]
    risks  = [soften(s) for s in sents if (not is_accident_sentence(s)) and is_risk_sentence(s)]
    acts   = [to_action_sentence(s, text) for s in act_src]

    def uniq_keep(seq: List[str]) -> List[str]:
        seen, out = set(), []
        for x in seq:
            k = re.sub(r"\s+","", x)
            if k not in seen:
                seen.add(k); out.append(x)
        return out

    # ìš”ì•½ ëª¨ë“œëŠ” ë„ˆë¬´ ê¸¸ì–´ì§€ì§€ ì•Šë„ë¡ ìƒí•œë§Œ ì¡´ì¬(ì‚¬ë¡€/ì˜ˆë°© ì„ì–´ ìµœëŒ€ì¹˜ ì œí•œ)
    cases = uniq_keep(cases_blk + cases)[:5]
    risks  = uniq_keep(risks)[:5]
    acts   = uniq_keep(prev_blk + acts)[:8]

    topic = dynamic_topic_label(text)
    lines = [f"ğŸ“„ í•µì‹¬ìš”ì•½ â€” {topic}\n"]
    if cases:
        lines.append("ã€ì‚¬ê³  ê°œìš”ã€‘"); lines.append("ìë£Œì—ì„œ í™•ì¸ëœ ì£¼ìš” ì‚¬ê³ ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.")
        for c in cases: lines.append(f"- {c}")
        lines.append("")
    if risks:
        lines.append("ã€ì£¼ìš” ìœ„í—˜ìš”ì¸ã€‘"); lines.append("ìë£Œ ì „ë°˜ì—ì„œ ë‹¤ìŒ ìš”ì¸ì´ ë°˜ë³µì ìœ¼ë¡œ ë‚˜íƒ€ë‚¬ìŠµë‹ˆë‹¤.")
        for r in risks: lines.append(f"- {r}")
        lines.append("")
    if acts:
        lines.append("ã€ì˜ˆë°©/ì‹¤ì²œ ìš”ì•½ã€‘"); lines.append("í˜„ì¥ì—ì„œ ì¦‰ì‹œ ì ìš© ê°€ëŠ¥í•œ í•µì‹¬ ìˆ˜ì¹™ì…ë‹ˆë‹¤.")
        for a in acts: lines.append(f"- {a}")
        lines.append("")
    if not (cases or risks or acts):
        lines.append("ìë£Œì˜ í•µì‹¬ì„ ê°„ë‹¨íˆ ì •ë¦¬í•˜ë©´ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.")
        for s in sents: lines.append(f"- {s}")
    return "\n".join(lines)

# -------------------- ë¼ë²¨/í† í”½ --------------------
def drop_label_token(t: str) -> bool:
    if t in STOP_TERMS: return True
    for pat in LABEL_DROP_PAT:
        if re.match(pat, t): return True
    if t in {"ì†Œì¬","ì†Œì¬ì§€","ì§€ì—­","ì¥ì†Œ","ë²„ìŠ¤","ì˜ì—…ì†Œ","ì—…ì²´","ìë£Œ","í‚¤","ë©”ì„¸ì§€","ëª…","ì•ˆì „ë³´ê±´"}:
        return True
    return False

def top_terms_for_label(text: str, k: int=3) -> List[str]:
    doc_cnt = Counter([t for t in tokens(text) if not drop_label_token(t)])
    bonus = Counter()
    for t in list(doc_cnt.keys()):
        if t in RISK_KEYWORDS:
            bonus[RISK_KEYWORDS[t]] += doc_cnt[t]
    doc_cnt += bonus
    kb = st.session_state["kb_terms"]
    if kb:
        for t, c in kb.items():
            if not drop_label_token(t):
                doc_cnt[t] += 0.2 * c
    if not doc_cnt: return ["ì•ˆì „ë³´ê±´","êµìœ¡"]
    commons = {"ì•ˆì „","êµìœ¡","ì‘ì—…","í˜„ì¥","ì˜ˆë°©","ì¡°ì¹˜","í™•ì¸","ê´€ë¦¬","ì ê²€","ê°€ì´ë“œ","ì§€ì¹¨"}
    cand = [(t, doc_cnt[t]) for t in doc_cnt if t not in commons and len(t) >= 2]
    if not cand: cand = list(doc_cnt.items())
    cand.sort(key=lambda x: x[1], reverse=True)
    return [t for t,_ in cand[:k]]

def dynamic_topic_label(text: str) -> str:
    terms = top_terms_for_label(text, k=3)
    risks = [RISK_KEYWORDS.get(t, t) for t in terms if t in RISK_KEYWORDS or t in RISK_KEYWORDS.values()]
    extra = [t for t in terms if t not in risks]
    label_core = " ".join(sorted(set(risks), key=risks.index)) or "ì•ˆì „ë³´ê±´"
    tail = " ".join(extra[:1])
    label = (label_core + (" " + tail if tail else "")).strip()
    if "ì¬í•´ì˜ˆë°©" not in label:
        label += " ì¬í•´ì˜ˆë°©"
    return label

# -------------------- DOCX --------------------
_XML_FORBIDDEN = r"[\x00-\x08\x0B\x0C\x0E-\x1F\uD800-\uDFFF\uFFFE\uFFFF]"
def _xml_safe(s: str) -> str:
    if not isinstance(s, str): s = "" if s is None else str(s)
    return rxx.sub(_XML_FORBIDDEN, "", s)

def to_docx_bytes(script: str) -> bytes:
    doc = Document()
    try:
        style = doc.styles["Normal"]; style.font.name = "Malgun Gothic"; style.font.size = Pt(11)
    except Exception: pass
    for raw in script.split("\n"):
        line = _xml_safe(raw)
        p = doc.add_paragraph(line)
        for run in p.runs:
            try:
                run.font.name = "Malgun Gothic"; run.font.size = Pt(11)
            except Exception: pass
    bio = io.BytesIO(); doc.save(bio); bio.seek(0)
    return bio.read()

# -------------------- UI --------------------
with st.sidebar:
    st.header("â„¹ï¸ ì†Œê°œ / ì‚¬ìš©ë²•")
    st.markdown("""
**AI íŒŒì´í”„ë¼ì¸(ì™„ì „ ë¬´ë£Œ, ì˜¤í”ˆì†ŒìŠ¤ë§Œ ì‚¬ìš©)**  
1) ì „ì²˜ë¦¬(ë…¸ì´ì¦ˆ ì œê±°/ì¤„ ë³‘í•©/ë‚ ì§œ-ì‚¬ê³  ê²°í•©)  
2) **ì‚¬ê³  ë¸”ë¡ ë³‘í•©**(ì—°ê²°ì–´Â·í‚¤ì›Œë“œë¡œ ì—°ì† ì„œìˆ ì„ í•œ ë¬¸ì¥ìœ¼ë¡œ)  
3) TextRank + MMR ìš”ì•½ (**ì„¸ì…˜ KB ê°€ì¤‘ì¹˜** ë°˜ì˜)  
4) ê·œì¹™í˜• NLG: ì¡°ì‚¬/ë„ì–´ì“°ê¸°Â·ì¢…ê²° ë³´ì •, **ì˜ˆë°©ì¡°ì¹˜ ì¤„ê²°í•© ë° ë¬¸ë§¥ ë³´ì •**  
5) ê²°ê³¼ í¬ë§·: **ìì—°ìŠ¤ëŸ¬ìš´ êµìœ¡ëŒ€ë³¸(ë¬´ë£Œ)** / **í•µì‹¬ìš”ì•½**  
*NEW: ì‚¬ë¡€Â·ì˜ˆë°© ì„¹ì…˜ì€ ì›ë¬¸ í—¤ë”ë¥¼ ì¸ì‹í•´ â€˜ëª¨ë“  ë¶ˆë¦¿â€™ì„ ê·¸ëŒ€ë¡œ ë°˜ì˜í•©ë‹ˆë‹¤.*
""")
    st.session_state["domain_toggle"] = st.toggle(
        "ğŸ”§ ë„ë©”ì¸ í…œí”Œë¦¿ ê°•í™”(ì‹ ì¤‘ ì ìš©)",
        value=False,
        help="ë¬¸ì¥Â·ë³¸ë¬¸ íŠ¸ë¦¬ê±° ì¼ì¹˜ + ìœ ì‚¬ë„ ê¸°ì¤€ ì¶©ì¡± ì‹œì—ë§Œ í…œí”Œë¦¿ì„ ì ìš©í•©ë‹ˆë‹¤."
    )

seed_kb_once()
st.title("ğŸ¦º OPS/í¬ìŠ¤í„°ë¥¼ êµìœ¡ ëŒ€ë³¸ìœ¼ë¡œ ìë™ ë³€í™˜ (ì™„ì „ ë¬´ë£Œ)")

def reset_all():
    st.session_state.pop("manual_text", None)
    st.session_state.pop("edited_text", None)
    st.session_state.pop("zip_choice", None)
    st.session_state["kb_terms"] = Counter()
    st.session_state["kb_actions"] = []
    st.session_state["kb_questions"] = []
    st.session_state["uploader_key"] += 1
    st.session_state["seed_loaded"] = False
    st.session_state["last_file_diag"] = {}
    st.session_state["last_extracted_cache"] = ""
    st.rerun()

col_top1, col_top2 = st.columns([4,1])
with col_top2:
    st.button("ğŸ§¹ ì´ˆê¸°í™”", on_click=reset_all, use_container_width=True)

st.markdown("**ì•ˆë‚´**  \n- í…ìŠ¤íŠ¸ê°€ í¬í•¨ëœ PDF ë˜ëŠ” ë³¸ë¬¸ í…ìŠ¤íŠ¸ë¥¼ ê¶Œì¥í•©ë‹ˆë‹¤.  \n- ì´ë¯¸ì§€/ìŠ¤ìº” PDFëŠ” í˜„ì¬ OCR ë¯¸ì§€ì›ì…ë‹ˆë‹¤.")

col1, col2 = st.columns([1,1], gap="large")

with col1:
    uploaded = st.file_uploader(
        "OPS ì—…ë¡œë“œ (PDF ë˜ëŠ” ZIP) â€¢ í…ìŠ¤íŠ¸ PDF ê¶Œì¥",
        type=["pdf","zip"],
        key=f"uploader_{st.session_state['uploader_key']}"
    )
    manual_text = st.text_area(
        "ë˜ëŠ” OPS í…ìŠ¤íŠ¸ ì§ì ‘ ë¶™ì—¬ë„£ê¸°",
        key="manual_text",
        height=220,
        placeholder="ì˜ˆ: í˜„ì¥ ì•ˆë‚´ë¬¸ ë˜ëŠ” OPS ë³¸ë¬¸ í…ìŠ¤íŠ¸â€¦"
    )

    extracted: str = ""
    zip_pdfs: Dict[str, bytes] = {}

    if uploaded is not None:
        fname = (uploaded.name or "").lower()
        try:
            raw_bytes = uploaded.getvalue()
        except Exception:
            raw_bytes = uploaded.read()

        if fname.endswith(".zip"):
            try:
                with zipfile.ZipFile(io.BytesIO(raw_bytes), "r") as zf:
                    for name in zf.namelist():
                        if name.lower().endswith(".pdf"):
                            data = zf.read(name); zip_pdfs[name] = data
                if zip_pdfs:
                    for nm, data in zip_pdfs.items():
                        txt_all = read_pdf_text_from_bytes(data, fname=f"{fname}::{nm}")
                        if txt_all.strip():
                            kb_ingest_text(txt_all)
                    kb_prune()
                    first_name = sorted(zip_pdfs.keys())[0]
                    extracted = read_pdf_text_from_bytes(zip_pdfs[first_name], fname=first_name)
                    if extracted.strip():
                        st.session_state["edited_text"] = extracted
                        st.session_state["last_extracted_cache"] = extracted
                    st.success(f"ZIP ê°ì§€: {len(zip_pdfs)}ê°œ PDF, ì²« ë¬¸ì„œ ìë™ ì„ íƒ â†’ {first_name}")
                else:
                    st.error("ZIP ë‚´ì— PDFê°€ ì—†ìŠµë‹ˆë‹¤.")
            except Exception as e:
                st.error(f"ZIP í•´ì œ ì˜¤ë¥˜: {e}")

            if zip_pdfs:
                chosen = st.selectbox("ZIP ë‚´ PDF ì„ íƒ", sorted(zip_pdfs.keys()), key="zip_choice")
                if chosen and zip_pdfs.get(chosen):
                    extracted2 = read_pdf_text_from_bytes(zip_pdfs[chosen], fname=chosen)
                    if extracted2.strip():
                        st.session_state["edited_text"] = extracted2
                        st.session_state["last_extracted_cache"] = extracted2

        elif fname.endswith(".pdf"):
            extracted = read_pdf_text_from_bytes(raw_bytes, fname=fname)
            if extracted.strip():
                kb_ingest_text(extracted); kb_prune()
                st.session_state["edited_text"] = extracted
                st.session_state["last_extracted_cache"] = extracted
            else:
                st.warning("âš ï¸ PDFì—ì„œ ìœ íš¨í•œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        else:
            st.warning("ì§€ì›í•˜ì§€ ì•ŠëŠ” í˜•ì‹ì…ë‹ˆë‹¤. PDF ë˜ëŠ” ZIPì„ ì—…ë¡œë“œí•˜ì„¸ìš”.")

    pasted = (manual_text or "").strip()
    if pasted:
        kb_ingest_text(pasted); kb_prune()
        st.session_state["edited_text"] = pasted
        st.session_state["last_extracted_cache"] = pasted

    base_text = st.session_state.get("edited_text","")
    st.markdown("**ì¶”ì¶œ/ì…ë ¥ í…ìŠ¤íŠ¸ ë¯¸ë¦¬ë³´ê¸°**")
    edited_text = st.text_area("í…ìŠ¤íŠ¸", value=base_text, height=240, key="edited_text")

    with st.expander("ğŸ§ª íŒŒì¼ ì½ê¸° ì§„ë‹¨(Log-lite)", expanded=False):
        diag = st.session_state.get("last_file_diag", {})
        if diag:
            st.write({
                "íŒŒì¼ëª…": diag.get("name"),
                "í¬ê¸°(bytes)": diag.get("size_bytes"),
                "ì¶”ì¶œëœ ë¬¸ììˆ˜": diag.get("extracted_chars"),
                "ë©”ëª¨": diag.get("note"),
            })
        st.caption(f"í˜„ì¬ í…ìŠ¤íŠ¸ ë°•ìŠ¤ ê¸¸ì´: {len(st.session_state.get('edited_text',''))} chars")

with col2:
    gen_mode = st.selectbox("ğŸ§  ìƒì„± ëª¨ë“œ", ["í•µì‹¬ìš”ì•½","ìì—°ìŠ¤ëŸ¬ìš´ êµìœ¡ëŒ€ë³¸(ë¬´ë£Œ)"])
    max_points = st.slider("ìš”ì•½ ê°•ë„(í•µì‹¬ë¬¸ì¥ ê°œìˆ˜)", 3, 10, 6)

    if st.button("ğŸ› ï¸ ëŒ€ë³¸ ìƒì„±", type="primary", use_container_width=True):
        text_for_gen = (st.session_state.get("edited_text") or "").strip()
        if not text_for_gen:
            text_for_gen = (st.session_state.get("last_extracted_cache") or "").strip()
            if text_for_gen:
                st.info("ë¹ˆ ì…ë ¥ì„ ìµœê·¼ ì¶”ì¶œ í…ìŠ¤íŠ¸ë¡œ ìë™ ëŒ€ì²´í–ˆìŠµë‹ˆë‹¤.")
        if not text_for_gen:
            st.warning("PDF/ZIP ì—…ë¡œë“œ ë˜ëŠ” í…ìŠ¤íŠ¸ ì…ë ¥ í›„ ì‹œë„í•˜ì„¸ìš”.")
        else:
            with st.spinner("ìƒì„± ì¤‘..."):
                if gen_mode == "ìì—°ìŠ¤ëŸ¬ìš´ êµìœ¡ëŒ€ë³¸(ë¬´ë£Œ)":
                    script = make_structured_script(text_for_gen, max_points=max_points)
                    subtitle = "ìì—°ìŠ¤ëŸ¬ìš´ êµìœ¡ëŒ€ë³¸(ë¬´ë£Œ)"
                else:
                    script = make_concise_report(text_for_gen, max_points=max_points)
                    subtitle = "í•µì‹¬ìš”ì•½"
            st.success(f"ìƒì„± ì™„ë£Œ! ({subtitle})")
            st.text_area("ê²°ê³¼ ë¯¸ë¦¬ë³´ê¸°", value=script, height=420)
            c3, c4 = st.columns(2)
            with c3:
                st.download_button(
                    "â¬‡ï¸ TXT ë‹¤ìš´ë¡œë“œ",
                    data=_xml_safe(script).encode("utf-8"),
                    file_name="tbm_output.txt",
                    use_container_width=True
                )
            with c4:
                st.download_button(
                    "â¬‡ï¸ DOCX ë‹¤ìš´ë¡œë“œ",
                    data=to_docx_bytes(script),
                    file_name="tbm_output.docx",
                    use_container_width=True
                )

st.caption("ì™„ì „ ë¬´ë£Œ. â€˜ì‚¬ë¡€/ì˜ˆë°©â€™ ì„¹ì…˜ì€ ì›ë¬¸ í—¤ë”ë¥¼ ì¸ì‹í•´ ëª¨ë“  ë¶ˆë¦¿ì„ ë°˜ì˜í•©ë‹ˆë‹¤. ìš”ì•½/ìì—°í™”ëŠ” TextRank+MMR+ê·œì¹™NLG.")
