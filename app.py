# ==========================================================
# OPS2TBM â€” ì™„ì „ ë¬´ë£Œ ì•ˆì •íŒ v2 (í™”í•™ë¬¼ì§ˆ/ì¤‘ë… í’ˆì§ˆ ê°œì„ )
# - UI/ë ˆì´ì•„ì›ƒ/ì»¨íŠ¸ë¡¤ ì´ë¦„ ê·¸ëŒ€ë¡œ ìœ ì§€
# - ê°œì„  í¬ì¸íŠ¸:
#   1) í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ ê°•í™”: ë¶ˆë¦¿/ë¨¸ë¦¬ë§/URL/ì½”ë“œ ì œê±°, ì¤„ ë³‘í•©
#   2) ì‚¬ê±´ë¬¸ì¥(ë‚ ì§œ/ì‚¬ë§ì í‘œê¸°) ìì—°ì–´ ë³€í™˜
#   3) í™”í•™ë¬¼ì§ˆ/ì¤‘ë… íŠ¹í™” í‚¤ì›Œë“œë¡œ ì£¼ì œ ê°ì§€ ë° ì„¹ì…˜ ë°°ì¹˜
#   4) 3ë¶„í˜• 'ìì—°ìŠ¤ëŸ¬ìš´ êµìœ¡ëŒ€ë³¸(ë¬´ë£Œ)' í’ˆì§ˆ í–¥ìƒ
#   5) TBM ê¸°ë³¸(í˜„í–‰)ë„ ì „ì²˜ë¦¬ëœ ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½í•´ì„œ ëœ ì–´ìƒ‰í•˜ê²Œ
# ==========================================================

import io, zipfile, re
from typing import List, Tuple, Dict
import streamlit as st
from docx import Document
from docx.shared import Pt
from pdfminer.high_level import extract_text as pdf_extract_text
import pypdfium2 as pdfium
import numpy as np
import regex as rxx

# ----------------------------
# ì„¸ì…˜ ìƒíƒœ (ì—…ë¡œë” ì´ˆê¸°í™”ìš© í‚¤)
# ----------------------------
if "uploader_key" not in st.session_state:
    st.session_state.uploader_key = 0

# ==========================================================
# 1) í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬(ë…¸ì´ì¦ˆ ì œê±°/ë¶ˆë¦¿ ì²˜ë¦¬/ë³‘í•©)
# ==========================================================
NOISE_PATTERNS = [
    r"^ì œ?\s?\d{4}\s?[-.]?\s?\d+\s?í˜¸$",            # ì œ2024-6 í˜¸
    r"^(í¬ìŠ¤í„°|ì±…ì|ìŠ¤í‹°ì»¤|ì½˜í…ì¸  ë§í¬)$",
    r"^(ìŠ¤ë§ˆíŠ¸í°\s*APP|ì¤‘ëŒ€ì¬í•´\s*ì‚¬ì´ë Œ|ì‚°ì—…ì•ˆì „í¬í„¸|ê³ ìš©ë…¸ë™ë¶€)$",
    r"^https?://\S+$",
    r"^\(?\s*PowerPoint\s*í”„ë ˆì  í…Œì´ì…˜\s*\)?$",
    r"^ì•ˆì „ë³´ê±´ìë£Œì‹¤.*$",
]

BULLET_PREFIX = r"^[\-\â€¢\â—\â–ª\â–¶\â–·\Â·\*]+[\s]*"  # ë¶ˆë¦¿ ë¨¸ë¦¬í‘œ ì œê±°ìš©
TRAILING_CODES = [
    r"<\s*\d+\s*ëª…\s*ì‚¬ë§\s*>", r"<\s*\d+\s*ëª…\s*ì˜ì‹ë¶ˆëª…\s*>",
]

def normalize_text(t: str) -> str:
    t = t.replace("\x0c", "\n")
    t = re.sub(r"[ \t]+\n", "\n", t)
    t = re.sub(r"\n{3,}", "\n\n", t)
    return t.strip()

def strip_noise_line(line: str) -> str:
    s = line.strip()
    if not s:
        return ""
    # ë¶ˆë¦¿Â·í™”ì‚´í‘œ ì œê±°
    s = re.sub(BULLET_PREFIX, "", s).strip()
    # ë…¸ì´ì¦ˆ ë¼ì¸ í•„í„°ë§
    for pat in NOISE_PATTERNS:
        if re.match(pat, s, re.IGNORECASE):
            return ""
    # URL/ì´ìƒë¬¸ì ì œê±°
    s = re.sub(r"https?://\S+", "", s).strip()
    # ì¤„ ë ë¶ˆí•„ìš” ê¸°í˜¸
    s = s.strip("â€¢â—â–ªâ–¶â–·Â·-").strip()
    return s

def merge_broken_lines(lines: List[str]) -> List[str]:
    """
    OPSëŠ” í•œ ë¬¸ì¥ì„ ì—¬ëŸ¬ ì¤„ë¡œ ë¶„ë¦¬í•˜ëŠ” ê²½ìš°ê°€ ë§ìŒ:
    - ê´„í˜¸ ë¨¸ë¦¬ë§(ì˜ˆ: (í˜¸í¡ë³´í˜¸êµ¬)) ë‹¤ìŒ ì¤„ì´ ë³¸ë¬¸ì¸ ê²½ìš° ë³‘í•©
    - 'â†³' ì„œë¸Œ í•­ëª©ì€ ë’¤ì¤„ê³¼ ì´ì–´ë¶™ì—¬ ë¬¸ì¥í™”
    """
    out = []
    buf = ""
    for raw in lines:
        s = strip_noise_line(raw)
        if not s:
            continue
        # 'â†³' ë“± ë“¤ì—¬ì“°ê¸° ì„œë¸Œí¬ì¸íŠ¸ëŠ” ì´ì–´ë¶™ì„
        s = s.lstrip("â†³").strip()
        if buf and (s[0].islower() or s.startswith(("ë° ", "ë°", "ë“± ", "ë“±"))):
            buf += " " + s
        elif buf and (buf.endswith((":", "Â·", "â€¢", "â–ª", "â–¶", "â–·")) or re.match(r"^\(.*\)$", buf)):
            buf += " " + s
        else:
            if buf:
                out.append(buf)
            buf = s
    if buf:
        out.append(buf)
    return out

def preprocess_text_to_sentences(text: str) -> List[str]:
    text = normalize_text(text)
    # ì¤„ ë‹¨ìœ„ ì²˜ë¦¬ í›„ ë³‘í•©
    lines = [ln for ln in text.splitlines() if ln.strip()]
    lines = merge_broken_lines(lines)
    # ë¬¸ì¥ ë¶„ë¦¬ (í•œêµ­ì–´ ë§ˆì¹¨í‘œ/ì˜ë¬¸ë¶€í˜¸/ì¤„ë°”ê¿ˆ)
    joined = "\n".join(lines)
    raw = rxx.split(r"(?<=[\.!\?]|ë‹¤\.)\s+|\n+", joined)
    sents = []
    for s in raw:
        s = strip_noise_line(s)
        if not s:
            continue
        # ë„ˆë¬´ ì§§ì€ ì¡°ê°/ë‹¨ë… ë²ˆí˜¸ ì œê±°
        if len(s) < 6:
            continue
        sents.append(s)
    # ì¤‘ë³µ ì œê±°
    seen = set(); dedup = []
    for s in sents:
        key = re.sub(r"\s+", "", s)
        if key not in seen:
            seen.add(key); dedup.append(s)
    return dedup

# ==========================================================
# 2) PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ
# ==========================================================
def read_pdf_text(b: bytes) -> str:
    try:
        with io.BytesIO(b) as bio:
            t = pdf_extract_text(bio) or ""
    except Exception:
        t = ""
    t = normalize_text(t)

    if len(t.strip()) < 10:
        try:
            with io.BytesIO(b) as bio:
                pdf = pdfium.PdfDocument(bio)
                pages = len(pdf)
            if pages > 0 and not t.strip():
                st.warning("âš ï¸ ì´ PDFëŠ” ì´ë¯¸ì§€/ìŠ¤ìº” ê¸°ë°˜ìœ¼ë¡œ ë³´ì…ë‹ˆë‹¤. í˜„ì¬ ë²„ì „ì€ OCR ì—†ì´ í…ìŠ¤íŠ¸ë§Œ ì²˜ë¦¬í•©ë‹ˆë‹¤.")
        except Exception:
            pass
    return t

# ==========================================================
# 3) AI ìš”ì•½(TextRank + MMR) â€” ì „ì²˜ë¦¬ëœ ë¬¸ì¥ ì‚¬ìš©
# ==========================================================
def simple_tokens(s: str) -> List[str]:
    return rxx.findall(r"[ê°€-í£a-z0-9]{2,}", s.lower())

def sentence_tfidf_vectors(sents: List[str]):
    toks = [simple_tokens(s) for s in sents]
    vocab: Dict[str, int] = {}
    for ts in toks:
        for t in ts:
            if t not in vocab:
                vocab[t] = len(vocab)
    if not vocab:
        return np.zeros((len(sents), 0), dtype=np.float32), []
    M = np.zeros((len(sents), len(vocab)), dtype=np.float32)
    df = np.zeros((len(vocab),), dtype=np.float32)
    for i, ts in enumerate(toks):
        for t in ts:
            M[i, vocab[t]] += 1.0
        for t in set(ts):
            df[vocab[t]] += 1.0
    N = float(len(sents))
    idf = np.log((N + 1.0) / (df + 1.0)) + 1.0
    M *= idf
    M /= (np.linalg.norm(M, axis=1, keepdims=True) + 1e-8)
    return M, list(vocab.keys())

def cosine_sim_matrix(X: np.ndarray) -> np.ndarray:
    if X.size == 0:
        return np.zeros((X.shape[0], X.shape[0]), dtype=np.float32)
    sim = np.clip(X @ X.T, 0.0, 1.0)
    np.fill_diagonal(sim, 0.0)
    return sim

def textrank_scores(sents: List[str], d: float = 0.85, max_iter: int = 60, tol: float = 1e-4) -> List[float]:
    n = len(sents)
    if n == 0: return []
    X, _ = sentence_tfidf_vectors(sents)
    W = cosine_sim_matrix(X)
    row_sums = W.sum(axis=1, keepdims=True)
    P = np.divide(W, row_sums, out=np.zeros_like(W), where=row_sums > 0)
    r = np.ones((n, 1), dtype=np.float32) / n
    teleport = np.ones((n, 1), dtype=np.float32) / n
    for _ in range(max_iter):
        r_new = d * (P.T @ r) + (1 - d) * teleport
        if np.linalg.norm(r_new - r, ord=1) < tol:
            r = r_new; break
        r = r_new
    return [float(v) for v in r.flatten()]

def mmr_select(cands: List[str], scores: List[float], X: np.ndarray, k: int, lambda_: float = 0.7) -> List[int]:
    if not cands: return []
    selected: List[int] = []
    remaining = set(range(len(cands)))
    sim = cosine_sim_matrix(X)
    while remaining and len(selected) < k:
        best_idx, best_val = None, -1e9
        for i in remaining:
            rel = scores[i]
            div = max((sim[i, j] for j in selected), default=0.0)
            mmr = lambda_ * rel - (1 - lambda_) * div
            if mmr > best_val:
                best_val, best_idx = mmr, i
        selected.append(best_idx)
        remaining.remove(best_idx)
    return selected

def ai_extract_summary(text: str, limit: int = 8) -> List[str]:
    sents = preprocess_text_to_sentences(text)
    if not sents: return []
    X, _ = sentence_tfidf_vectors(sents)
    scores = textrank_scores(sents)
    idx = mmr_select(sents, scores, X, limit, lambda_=0.7)
    return [sents[i] for i in idx]

# ==========================================================
# 4) ë§íˆ¬ ì™„í™”/ì‚¬ê±´ë¬¸ ë³€í™˜/ì£¼ì œ ê°ì§€/ì„¹ì…˜ êµ¬ì„±
# ==========================================================
ACTION_VERBS = [
    "ì„¤ì¹˜","ë°°ì¹˜","ì°©ìš©","ì ê²€","í™•ì¸","ì¸¡ì •","ê¸°ë¡","í‘œì‹œ","ì œê³µ","ë¹„ì¹˜",
    "ë³´ê³ ","ì‹ ê³ ","êµìœ¡","ì£¼ì§€","ì¤‘ì§€","í†µì œ","íœ´ì‹","í™˜ê¸°","ì°¨ë‹¨","êµëŒ€","ë°°ì œ","ë°°ë ¤","ê°€ë™"
]

CHEM_KEYS = ["í™”í•™ë¬¼ì§ˆ","ì¤‘ë…","ìœ ê¸°ìš©ì œ","í†¨ë£¨ì—”","MSDS","ë°©ë…ë§ˆìŠ¤í¬","ì •í™”í†µ","ì†¡ê¸°ë§ˆìŠ¤í¬","ê³µê¸°í˜¸í¡ê¸°","êµ­ì†Œë°°ê¸°"]
CONFINED_KEYS = ["ì§ˆì‹","ë°€í","ì‚°ì†Œê²°í•","ìœ í•´ê°€ìŠ¤","í™©í™”ìˆ˜ì†Œ","ì¼ì‚°í™”íƒ„ì†Œ","ë©”íƒ„"]

def soften(s: str) -> str:
    s = s.replace("í•˜ì—¬ì•¼", "í•´ì•¼ í•©ë‹ˆë‹¤")
    s = s.replace("í•œë‹¤", "í•©ë‹ˆë‹¤").replace("í•œë‹¤.", "í•©ë‹ˆë‹¤.")
    s = s.replace("ë°”ëë‹ˆë‹¤", "í•´ì£¼ì„¸ìš”").replace("í™•ì¸ ë°”ëŒ", "í™•ì¸í•´ì£¼ì„¸ìš”")
    s = s.replace("ì¡°ì¹˜í•œë‹¤", "ì¡°ì¹˜í•©ë‹ˆë‹¤").replace("ì°©ìš©í•œë‹¤", "ì°©ìš©í•©ë‹ˆë‹¤")
    s = s.replace("í•„ìš”í•˜ë‹¤", "í•„ìš”í•©ë‹ˆë‹¤").replace("ê¸ˆì§€í•œë‹¤", "ê¸ˆì§€í•©ë‹ˆë‹¤")
    # "(í˜¸í¡ë³´í˜¸êµ¬)" ê°™ì€ ë¨¸ë¦¬ë§ ì œê±°
    s = re.sub(r"^\(([^)]+)\)\s*", "", s)
    return s.strip(" -â€¢â—\t")

def detect_topic(t: str) -> str:
    low = t.lower()
    if any(k in t for k in CHEM_KEYS):
        return "í™”í•™ë¬¼ì§ˆ ì¤‘ë… ì˜ˆë°©"
    if any(k in t for k in CONFINED_KEYS):
        return "ë°€íê³µê°„ ì§ˆì‹ì¬í•´ ì˜ˆë°©"
    if "ì˜¨ì—´" in t or "í­ì—¼" in t:
        return "ì˜¨ì—´ì§ˆí™˜ ì˜ˆë°©"
    if "ê°ì „" in t:
        return "ê°ì „ì‚¬ê³  ì˜ˆë°©"
    if "ì§€ë¶•" in t or "ì¬ë¼ì´íŠ¸" in t:
        return "ì§€ë¶• ì‘ì—… ì¶”ë½ì‚¬ê³  ì˜ˆë°©"
    if "ì»¨ë² ì´ì–´" in t or "ë¼ì„" in t:
        return "ì»¨ë² ì´ì–´ ë¼ì„ì‚¬ê³  ì˜ˆë°©"
    return "ì•ˆì „ë³´ê±´ êµìœ¡"

def to_case_sentence(s: str) -> str:
    """
    '2024.09.26.<1ëª… ì‚¬ë§> ê¸°ê³„ì‹ ì£¼ì°¨ì¥ì—ì„œ ë„ì¥ì‘ì—… ì¤‘ í†¨ë£¨ì—” ì¤‘ë…, 1ëª… ì‚¬ë§ ë° 2ëª… ì˜ì‹ë¶ˆëª…'
    â†’ '2024ë…„ 9ì›” 26ì¼, ê¸°ê³„ì‹ ì£¼ì°¨ì¥ ë„ì¥ì‘ì—… ì¤‘ í†¨ë£¨ì—” ì¤‘ë… ì‚¬ê³ ë¡œ 1ëª… ì‚¬ë§, 2ëª… ì˜ì‹ë¶ˆëª…ì´ ë°œìƒí–ˆìŠµë‹ˆë‹¤.'
    """
    orig = s
    # ë‚ ì§œ
    m = re.search(r"(\d{4})\.(\d{2})\.(\d{2})", s)
    date_txt = ""
    if m:
        y, mo, d = m.groups()
        date_txt = f"{int(y)}ë…„ {int(mo)}ì›” {int(d)}ì¼, "
        s = s.replace(m.group(0), "").strip()
    # êº½ì‡  ì‚¬ë§ì
    for pat in TRAILING_CODES:
        s = re.sub(pat, "", s).strip()
    # ì½¤ë§ˆ ì „ì²˜ë¦¬
    s = s.replace(" ,", ",").replace("  ", " ").strip(" ,.")
    # 'ì¤‘'â†’ 'ì¤‘ì—' ìì—°í™”
    s = s.replace("ì¤‘ ", "ì¤‘ ").replace("ì¤‘,", "ì¤‘,")
    # ë¬¸ì¥ ì¢…ê²°
    if not s.endswith(("ë‹¤", "ë‹¤.", ".", "ì˜€ìŠµë‹ˆë‹¤", "í–ˆìŠµë‹ˆë‹¤")):
        s = s.rstrip(" .") + " ì‚¬ê³ ê°€ ìˆì—ˆìŠµë‹ˆë‹¤."
    return (date_txt + s).strip()

def make_structured_script(text: str, max_points: int = 6) -> str:
    """
    3ë¶„í˜• êµìœ¡ëŒ€ë³¸ ìë™ êµ¬ì„±:
    - ì „ì²˜ë¦¬ â†’ ìš”ì•½ â†’ ì‚¬ê±´/ìœ„í—˜/ì¡°ì¹˜/ì§ˆë¬¸ ë¶„ë¥˜
    - ì‚¬ê±´ ë¬¸ì¥ ìì—°ì–´í™”(to_case_sentence)
    - ë„ì…~êµ¬í˜¸ êµ¬ì¡° ì¡°ë¦½
    """
    topic = detect_topic(text)
    core = [soften(s) for s in ai_extract_summary(text, max_points)]
    if not core:
        return "ë³¸ë¬¸ì´ ì¶©ë¶„í•˜ì§€ ì•Šì•„ ëŒ€ë³¸ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

    case, risk, act, ask, misc = [], [], [], [], []
    for s in core:
        if re.search(r"\d{4}\.\d{2}\.\d{2}", s) or any(k in s for k in ["ì‚¬ë§", "ì˜ì‹ë¶ˆëª…", "ì‚¬ê³ ", "ë°œìƒ", "ì‚¬ë¡€"]):
            case.append(to_case_sentence(s))
        elif any(k in s for k in ["ìœ„í—˜", "ìš”ì¸", "ì›ì¸", "ì¤‘ë…ì¦ìƒ"]):
            risk.append(s)
        elif any(k in s for k in ["ì¡°ì¹˜", "ì˜ˆë°©", "ì°©ìš©", "ì„¤ì¹˜", "ì ê²€", "í™˜ê¸°", "ë¹„ì¹˜", "êµìœ¡"]) or any(v in s for v in ACTION_VERBS):
            act.append(s)
        elif "?" in s or "í™•ì¸" in s:
            ask.append(s if s.endswith("?") else (s + " ë§ìŠµë‹ˆê¹Œ?"))
        else:
            misc.append(s)

    # í™”í•™ë¬¼ì§ˆ/ì§ˆì‹ ì£¼ì œì¼ ë•Œ ì•¡ì…˜ ë³´ê°•: MSDS/êµ­ì†Œë°°ê¸°/í˜¸í¡ë³´í˜¸êµ¬/í™˜ê¸°
    if topic in ["í™”í•™ë¬¼ì§ˆ ì¤‘ë… ì˜ˆë°©", "ë°€íê³µê°„ ì§ˆì‹ì¬í•´ ì˜ˆë°©"]:
        stock_actions = [
            "ì‘ì—… ì „ MSDSë¥¼ í™•ì¸í•˜ê³  êµìœ¡ë°›ìŠµë‹ˆë‹¤.",
            "êµ­ì†Œë°°ê¸°ì¥ì¹˜ë¥¼ ê°€ë™í•˜ê³  í™˜ê¸° ê²½ë¡œë¥¼ í™•ë³´í•©ë‹ˆë‹¤.",
            "ì†¡ê¸°ë§ˆìŠ¤í¬ ë˜ëŠ” ê³µê¸°í˜¸í¡ê¸° ë“± ì í•©í•œ í˜¸í¡ë³´í˜¸êµ¬ë¥¼ ì°©ìš©í•©ë‹ˆë‹¤.",
            "ìœ í•´ê°€ìŠ¤Â·ì‚°ì†Œ ë†ë„ë¥¼ ì¸¡ì •í•˜ê³  ê¸°ë¡í•©ë‹ˆë‹¤.",
            "ê°ì‹œìë¥¼ ë°°ì¹˜í•˜ê³  í†µì‹ ì„ ìœ ì§€í•©ë‹ˆë‹¤."
        ]
        # ì¤‘ë³µ ì—†ì´ ë³´ê°•
        for a in stock_actions:
            if a not in act:
                act.append(a)

    # ì•¡ì…˜ ë¬¸ì¥ ìƒí•œ
    act = act[:5]

    # ë„ì… ë¬¸êµ¬(ì£¼ì œë³„ ìì—°í™”)
    topic_intro = {
        "í™”í•™ë¬¼ì§ˆ ì¤‘ë… ì˜ˆë°©": "ì˜¤ëŠ˜ì€ ìœ ê¸°ìš©ì œÂ·ê°€ìŠ¤ ë“± í™”í•™ë¬¼ì§ˆë¡œ ì¸í•œ ì¤‘ë…ì„ ì˜ˆë°©í•˜ëŠ” ë°©ë²•ì„ ì •ë¦¬í•˜ê² ìŠµë‹ˆë‹¤.",
        "ë°€íê³µê°„ ì§ˆì‹ì¬í•´ ì˜ˆë°©": "ì˜¤ëŠ˜ì€ íƒ±í¬Â·ë§¨í™€ ë“± ë°€íê³µê°„ì—ì„œ ë°˜ë³µë˜ëŠ” ì§ˆì‹ì¬í•´ë¥¼ ë‹¤ë£¨ê² ìŠµë‹ˆë‹¤.",
        "ì˜¨ì—´ì§ˆí™˜ ì˜ˆë°©": "ì˜¤ëŠ˜ì€ í­ì—¼ ì‹œ ì˜¨ì—´ì§ˆí™˜ì„ ë§‰ê¸° ìœ„í•œ í•µì‹¬ì„ ì§šì–´ë³´ê² ìŠµë‹ˆë‹¤.",
        "ê°ì „ì‚¬ê³  ì˜ˆë°©": "ì˜¤ëŠ˜ì€ ê°ì „ì‚¬ê³ ë¥¼ ì˜ˆë°©í•˜ê¸° ìœ„í•œ ê¸°ë³¸ ì ˆì°¨ë¥¼ í™•ì¸í•˜ê² ìŠµë‹ˆë‹¤.",
        "ì§€ë¶• ì‘ì—… ì¶”ë½ì‚¬ê³  ì˜ˆë°©": "ì˜¤ëŠ˜ì€ ì§€ë¶• ì‘ì—… ì¤‘ ë°œìƒí•˜ëŠ” ì¶”ë½ì‚¬ê³ ë¥¼ ë§‰ëŠ” ë°©ë²•ì„ ì´ì•¼ê¸°í•˜ê² ìŠµë‹ˆë‹¤.",
        "ì»¨ë² ì´ì–´ ë¼ì„ì‚¬ê³  ì˜ˆë°©": "ì˜¤ëŠ˜ì€ ì»¨ë² ì´ì–´ì—ì„œ ìì£¼ ë°œìƒí•˜ëŠ” ë¼ì„ì‚¬ê³ ë¥¼ ì˜ˆë°©í•˜ëŠ” ë°©ë²•ì„ ì•ˆë‚´í•˜ê² ìŠµë‹ˆë‹¤.",
        "ì•ˆì „ë³´ê±´ êµìœ¡": "ì˜¤ëŠ˜ì€ ì•ˆì „ë³´ê±´ ê¸°ë³¸ì„ ê°„ë‹¨íˆ ì •ë¦¬í•˜ê² ìŠµë‹ˆë‹¤."
    }
    intro = topic_intro.get(topic, f"ì˜¤ëŠ˜ì€ {topic}ì— ëŒ€í•´ ì´ì•¼ê¸°í•˜ê² ìŠµë‹ˆë‹¤.").strip()

    # ì¡°ë¦½
    lines = []
    lines.append(f"ğŸ¦º TBM êµìœ¡ëŒ€ë³¸ â€“ {topic}\n")
    lines.append("â— ë„ì…")
    lines.append(intro + "\n")

    if case:
        lines.append("â— ì‚¬ê³  ì‚¬ë¡€")
        for c in case:
            lines.append(f"- {c}")
        lines.append("")

    if risk:
        lines.append("â— ì£¼ìš” ìœ„í—˜ìš”ì¸")
        for r in risk:
            lines.append(f"- {r}")
        lines.append("")

    if act:
        lines.append("â— ì˜ˆë°©ì¡°ì¹˜ / ì‹¤ì²œ ìˆ˜ì¹™")
        for i, a in enumerate(act, 1):
            lines.append(f"{i}ï¸âƒ£ {a}")
        lines.append("")

    # ì§ˆë¬¸ì´ í•˜ë‚˜ë„ ì—†ìœ¼ë©´ ê¸°ë³¸ ì§ˆë¬¸ ì œê³µ(ì£¼ì œë³„)
    if not ask:
        if topic == "í™”í•™ë¬¼ì§ˆ ì¤‘ë… ì˜ˆë°©":
            ask = [
                "MSDS ë¹„ì¹˜ì™€ êµìœ¡ì´ í˜„ì¥ì—ì„œ ì‹¤ì œë¡œ ì´ë£¨ì–´ì§€ê³  ìˆìŠµë‹ˆê¹Œ?",
                "êµ­ì†Œë°°ê¸°ì¥ì¹˜ê°€ ì •ìƒ ê°€ë™ë˜ê³  ìˆìŠµë‹ˆê¹Œ?",
                "í˜¸í¡ë³´í˜¸êµ¬(ì •í™”í†µ/ì†¡ê¸°)ê°€ ì‘ì—…ì— ì í•©í•˜ê³  ê´€ë¦¬ê°€ ë˜ê³  ìˆìŠµë‹ˆê¹Œ?"
            ]
        elif topic == "ë°€íê³µê°„ ì§ˆì‹ì¬í•´ ì˜ˆë°©":
            ask = [
                "ì‘ì—… ì „ ì‚°ì†Œ/ìœ í•´ê°€ìŠ¤ ë†ë„ ì¸¡ì •ì„ í–ˆìŠµë‹ˆê¹Œ?",
                "ê°ì‹œì ë°°ì¹˜ì™€ í†µì‹  ìˆ˜ë‹¨ì´ ì¤€ë¹„ë˜ì–´ ìˆìŠµë‹ˆê¹Œ?",
                "í™˜ê¸°ì¥ì¹˜ì™€ ì¶œì…í†µì œê°€ ì‘ë™ ì¤‘ì…ë‹ˆê¹Œ?"
            ]

    if ask:
        lines.append("â— í˜„ì¥ ì ê²€ ì§ˆë¬¸")
        for q in ask:
            lines.append(f"- {q}")
        lines.append("")

    lines.append("â— ë§ˆë¬´ë¦¬ ë‹¹ë¶€")
    lines.append("ì•ˆì „ì€ í•œìˆœê°„ì˜ ê´€ì‹¬ì—ì„œ ì‹œì‘ë©ë‹ˆë‹¤. ì˜¤ëŠ˜ ì‘ì—… ì „ ì„œë¡œ í•œ ë²ˆ ë” í™•ì¸í•©ì‹œë‹¤.")
    lines.append("â— êµ¬í˜¸")
    lines.append("â€œí•œ ë²ˆ ë” í™•ì¸! í•œ ë²ˆ ë” ì ê²€!â€")
    return "\n".join(lines)

# ==========================================================
# 5) DOCX ë‚´ë³´ë‚´ê¸° (XML ê¸ˆì§€ë¬¸ì í•„í„°)
# ==========================================================
_XML_FORBIDDEN = r"[\x00-\x08\x0B\x0C\x0E-\x1F\uD800-\uDFFF\uFFFE\uFFFF]"
def _xml_safe(s: str) -> str:
    if not isinstance(s, str):
        s = "" if s is None else str(s)
    return rxx.sub(_XML_FORBIDDEN, "", s)

def to_docx_bytes(script: str) -> bytes:
    doc = Document()
    try:
        style = doc.styles["Normal"]
        style.font.name = "Malgun Gothic"
        style.font.size = Pt(11)
    except Exception:
        pass
    for raw in script.split("\n"):
        line = _xml_safe(raw)
        p = doc.add_paragraph(line)
        for run in p.runs:
            try:
                run.font.name = "Malgun Gothic"
                run.font.size = Pt(11)
            except Exception:
                pass
    bio = io.BytesIO()
    doc.save(bio)
    bio.seek(0)
    return bio.read()

# ==========================================================
# 6) Streamlit UI (ê·¸ëŒ€ë¡œ ìœ ì§€)
# ==========================================================
st.set_page_config(page_title="OPS2TBM", page_icon="ğŸ¦º", layout="wide")

with st.sidebar:
    st.header("â„¹ï¸ ì†Œê°œ / ì‚¬ìš©ë²•")
    st.markdown("""
**AI ê¸°ëŠ¥(ì™„ì „ ë¬´ë£Œ)**  
- TextRank + MMR ê¸°ë°˜ ìš”ì•½(ì „ì²˜ë¦¬ ê°•í™”)  
- ì‚¬ê±´ë¬¸ì¥ ìì—°ì–´í™”(ë‚ ì§œ/ì‚¬ë§ì í‘œê¸° ì •ë¦¬)  
- í™”í•™ë¬¼ì§ˆ/ì§ˆì‹ ë“± ì£¼ì œë³„ ì•¡ì…˜ ë³´ê°•

**ì´ˆê¸°í™”**  
- ìš°ìƒë‹¨ **ğŸ§¹ ì´ˆê¸°í™”** ë²„íŠ¼
""")

st.title("ğŸ¦º OPS2TBM â€” OPS/í¬ìŠ¤í„°ë¥¼ êµìœ¡ ëŒ€ë³¸ìœ¼ë¡œ ìë™ ë³€í™˜ (ì™„ì „ ë¬´ë£Œ)")

def reset_all():
    st.session_state.pop("manual_text", None)
    st.session_state.pop("edited_text", None)
    st.session_state.pop("zip_choice", None)
    st.session_state.uploader_key += 1
    st.rerun()

col_top1, col_top2 = st.columns([4, 1])
with col_top2:
    st.button("ğŸ§¹ ì´ˆê¸°í™”", on_click=reset_all, use_container_width=True)

st.markdown("""
**ì•ˆë‚´**  
- í…ìŠ¤íŠ¸ê°€ í¬í•¨ëœ PDF ë˜ëŠ” ë³¸ë¬¸ í…ìŠ¤íŠ¸ë¥¼ ê¶Œì¥í•©ë‹ˆë‹¤.  
- ZIP ì—…ë¡œë“œ ì‹œ ë‚´ë¶€ì˜ PDFë“¤ì„ ìë™ ì¸ì‹í•˜ì—¬ ì„ íƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.  
- ì´ë¯¸ì§€/ìŠ¤ìº”í˜• PDFëŠ” í˜„ì¬ OCR ë¯¸ì§€ì›ì…ë‹ˆë‹¤.
""")

col1, col2 = st.columns([1, 1], gap="large")

# ì¢Œì¸¡: ì…ë ¥
with col1:
    uploaded = st.file_uploader("OPS ì—…ë¡œë“œ (PDF ë˜ëŠ” ZIP) â€¢ í…ìŠ¤íŠ¸ PDF ê¶Œì¥",
                                type=["pdf", "zip"], key=f"uploader_{st.session_state.uploader_key}")
    manual_text = st.text_area("ë˜ëŠ” OPS í…ìŠ¤íŠ¸ ì§ì ‘ ë¶™ì—¬ë„£ê¸°", key="manual_text",
                               height=220, placeholder="ì˜ˆ: í­ì—¼ ì‹œ ì˜¨ì—´ì§ˆí™˜ ì˜ˆë°©ì„ ìœ„í•´â€¦")

    # ZIP ì²˜ë¦¬
    zip_pdfs: Dict[str, bytes] = {}
    selected_zip_pdf = None
    if uploaded and uploaded.name.lower().endswith(".zip"):
        try:
            with zipfile.ZipFile(uploaded, "r") as zf:
                for name in zf.namelist():
                    if name.lower().endswith(".pdf"):
                        zip_pdfs[name] = zf.read(name)
        except Exception:
            st.error("ZIPì„ í•´ì œí•˜ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. íŒŒì¼ì„ í™•ì¸í•´ ì£¼ì„¸ìš”.")
        if zip_pdfs:
            selected_zip_pdf = st.selectbox("ZIP ë‚´ PDF ì„ íƒ", list(zip_pdfs.keys()), key="zip_choice")
        else:
            st.warning("ZIP ì•ˆì—ì„œ PDFë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")

    extracted = ""
    if uploaded and uploaded.name.lower().endswith(".pdf"):
        with st.spinner("PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ ì¤‘..."):
            extracted = read_pdf_text(uploaded.read())
    elif selected_zip_pdf:
        with st.spinner("ZIP ë‚´ë¶€ PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ ì¤‘..."):
            extracted = read_pdf_text(zip_pdfs[selected_zip_pdf])

    base_text = st.session_state.get("manual_text", "").strip() or extracted.strip()
    st.markdown("**ì¶”ì¶œ/ì…ë ¥ í…ìŠ¤íŠ¸ ë¯¸ë¦¬ë³´ê¸°**")
    edited_text = st.text_area("í…ìŠ¤íŠ¸", value=base_text, height=240, key="edited_text")

# ìš°ì¸¡: ì˜µì…˜/ìƒì„±/ë‹¤ìš´ë¡œë“œ
with col2:
    use_ai = st.toggle("ğŸ”¹ AI ìš”ì•½(TextRank+MMR) ì‚¬ìš©", value=True)
    tmpl_choice = st.selectbox("ğŸ§© í…œí”Œë¦¿", ["ìë™ ì„ íƒ", "ì‚¬ê³ ì‚¬ë¡€í˜•", "ê°€ì´ë“œí˜•"])
    gen_mode = st.selectbox("ğŸ§  ìƒì„± ëª¨ë“œ", ["TBM ê¸°ë³¸(í˜„í–‰)", "ìì—°ìŠ¤ëŸ¬ìš´ êµìœ¡ëŒ€ë³¸(ë¬´ë£Œ)"])
    max_points = st.slider("ìš”ì•½ ê°•ë„(í•µì‹¬ë¬¸ì¥ ê°œìˆ˜)", 3, 10, 6)

    if st.button("ğŸ› ï¸ ëŒ€ë³¸ ìƒì„±", type="primary", use_container_width=True):
        text_for_gen = (st.session_state.get("edited_text") or "").strip()
        if not text_for_gen:
            st.warning("í…ìŠ¤íŠ¸ê°€ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤. PDF/ZIP ì—…ë¡œë“œ ë˜ëŠ” í…ìŠ¤íŠ¸ ì…ë ¥ í›„ ì‹œë„í•˜ì„¸ìš”.")
        else:
            # í…œí”Œë¦¿ ìë™/ìˆ˜ë™(í‘œì‹œìš©)
            if tmpl_choice == "ìë™ ì„ íƒ":
                detected = "ê°€ì´ë“œí˜•" if ("ê°€ì´ë“œ" in text_for_gen or "ì•ˆë‚´" in text_for_gen) else "ì‚¬ê³ ì‚¬ë¡€í˜•"
            else:
                detected = tmpl_choice

            with st.spinner("ëŒ€ë³¸ ìƒì„± ì¤‘..."):
                if gen_mode == "ìì—°ìŠ¤ëŸ¬ìš´ êµìœ¡ëŒ€ë³¸(ë¬´ë£Œ)":
                    script = make_structured_script(text_for_gen, max_points=max_points)
                    subtitle = f"{detected} Â· ìì—°ìŠ¤ëŸ¬ìš´ êµìœ¡ëŒ€ë³¸(ë¬´ë£Œ)"
                else:
                    # TBM ê¸°ë³¸(í˜„í–‰)ë„ ì „ì²˜ë¦¬ëœ ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½í•´ì„œ í’ˆì§ˆ ê°œì„ 
                    sents = ai_extract_summary(text_for_gen, max_points if use_ai else 6)
                    sents = [soften(s) for s in sents]
                    script = "\n".join([f"- {s}" for s in sents]) if sents else "í…ìŠ¤íŠ¸ì—ì„œ í•µì‹¬ ë¬¸ì¥ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."
                    subtitle = f"{detected} Â· TBM ê¸°ë³¸"

            st.success(f"ëŒ€ë³¸ ìƒì„± ì™„ë£Œ! ({subtitle})")
            st.text_area("ëŒ€ë³¸ ë¯¸ë¦¬ë³´ê¸°", value=script, height=420)

            c3, c4 = st.columns(2)
            with c3:
                st.download_button("â¬‡ï¸ TXT ë‹¤ìš´ë¡œë“œ", data=_xml_safe(script).encode("utf-8"),
                                   file_name="tbm_script.txt", use_container_width=True)
            with c4:
                st.download_button("â¬‡ï¸ DOCX ë‹¤ìš´ë¡œë“œ", data=to_docx_bytes(script),
                                   file_name="tbm_script.docx", use_container_width=True)

st.caption("ì™„ì „ ë¬´ë£Œ. ì „ì²˜ë¦¬ ê°•í™”(TextRank+MMRÂ·ë…¸ì´ì¦ˆ ì œê±°) + ì‚¬ê±´ë¬¸ì¥ ìì—°ì–´í™” + ì£¼ì œë³„ ì•¡ì…˜ ë³´ê°•. UI ë³€ê²½ ì—†ìŒ. OCR ë¯¸ì§€ì›.")
