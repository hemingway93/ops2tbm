# ==========================================================
# OPS2TBM â€” OPS/í¬ìŠ¤í„° â†’ TBM êµìœ¡ ëŒ€ë³¸ ìë™ ë³€í™˜ (ì™„ì „ ë¬´ë£Œ)
# v2025-11-05-c (tech-notes)
#
# [í”„ë¡œì íŠ¸/ê¸°ìˆ  ìŠ¤íƒ ì£¼ì„ â€” ì œì¶œìš©]
# - êµ¬í˜„ í˜•íƒœ: Web App (Streamlit)
# - ì‚¬ìš© ì–¸ì–´: Python 3
# - ì˜¤í”ˆì†ŒìŠ¤ ë¼ì´ë¸ŒëŸ¬ë¦¬(ëª¨ë‘ ë¬´ë£Œ, í‘œì¤€ ë¼ì´ì„ ìŠ¤):
#   * streamlit .......... ì›¹ UI/ìƒíƒœ ê´€ë¦¬ (ìˆœìˆ˜ íŒŒì´ì¬, ì„œë²„ë¦¬ìŠ¤ í´ë¼ìš°ë“œ ë°°í¬ í˜¸í™˜)
#   * pdfminer.six ........ í…ìŠ¤íŠ¸ ê¸°ë°˜ PDFì˜ ë³¸ë¬¸ í…ìŠ¤íŠ¸ ì¶”ì¶œ
#   * pypdfium2 ........... PDF ì´ë¯¸ì§€/ìŠ¤ìº” ì—¬ë¶€ íŒë‹¨(ê°„ë‹¨ ì§„ë‹¨) ë° ì²˜ë¦¬ë¥¼ ìœ„í•œ ë°±ì—”ë“œ(ì´ë²ˆ ë²„ì „ì€ OCR ë¯¸ì‚¬ìš©)
#   * python-docx .......... ìƒì„±ëœ ëŒ€ë³¸ì„ DOCXë¡œ ë‚´ë³´ë‚´ê¸°
#   * regex(=regex íŒ¨í‚¤ì§€) .. í•œêµ­ì–´/ìœ ë‹ˆì½”ë“œ ì¹œí™” ì •ê·œì‹(íŒŒì´ì¬ re ë³´ê°•)
#   * numpy ............... TF-IDF/ì½”ì‚¬ì¸ ìœ ì‚¬ë„/í…ìŠ¤íŠ¸ë­í¬ ë²¡í„° ê³„ì‚°
#
# - AI/ì•Œê³ ë¦¬ì¦˜(ìœ ë£Œ API ì „í˜€ ì—†ìŒ / ì™„ì „ ë¬´ë£ŒÂ·ë¡œì»¬):
#   * í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬: ì¡ìŒ ì œê±°, í—¤ë” ê°ì§€, ì¤„ ë³‘í•©, ë‚ ì§œ-ì‚¬ê³  ê²°í•©
#   * ë¬¸ì¥ ë¶„í•  + ì—°ê²°: OPS ë¬¸ì„œ íŠ¹ìœ ì˜ ë¶ˆë¦¿/ë‹¨ë‹µì„ ìì—°ë¬¸ìœ¼ë¡œ ì¬êµ¬ì„±
#   * ìš”ì•½: TextRank + MMR(ë‹¤ì–‘ì„± ì¡°ì ˆ) â€” ê³ ì „ ê·¸ë˜í”„ ê¸°ë°˜ ìš”ì•½, LLM ë¶ˆí•„ìš”
#   * ì˜ë¯¸ ê°€ì¤‘ì¹˜: ì„¸ì…˜ KB(ì‚¬ìš©ìê°€ ì˜¬ë¦° PDF/í…ìŠ¤íŠ¸ì—ì„œ ë™ì  ìš©ì–´ í†µê³„)ë¡œ TF-IDFì— ê°€ì¤‘ì¹˜
#   * ì„¹ì…˜ ì¶”ì¶œ: 
#     - (1) í—¤ë” ê¸°ë°˜ íŒŒì„œ(ì‚¬ê³ ì‚¬ë¡€/ì˜ˆë°©ìˆ˜ì¹™ ë“± ì œëª©ì„ ì§ì ‘ ì¸ì‹)
#     - (2) í—¤ë” ì—†ì´ë„ ë¶ˆë¦¿ í´ëŸ¬ìŠ¤í„°ë¥¼ ì‚¬ë¡€í˜•/ì˜ˆë°©í˜•ìœ¼ë¡œ ìë™ ë¶„ë¥˜ (í–‰ë™ë™ì‚¬ íŒ¨í„´)
#     - (3) Fallback: í‚¤ì›Œë“œ/ë‚ ì§œ ê¸°ë°˜ ì‚¬ê³ ë¬¸ì¥ê³¼ í–‰ë™ë¬¸ì„ ìë™ ìˆ˜ì§‘
#   * ê·œì¹™í˜• NLG: ì¡°ì‚¬/ì¢…ê²°/ë„ì–´ì“°ê¸° ë³´ì •, â€œ~ í•©ë‹ˆë‹¤.â€ ì¼ê´€ ë¬¸ì²´í™”, 
#                ì§§ì€ ë¶ˆì™„ì „ ë¬¸ì¥ ë³‘í•©(ì˜ˆ: 'ì˜ˆë°© ì‹¤ì‹œ' â†’ 'ì˜ˆë°© ì¡°ì¹˜ë¥¼ ì‹¤ì‹œí•©ë‹ˆë‹¤.')
#
# - ë°ì´í„°/í•™ìŠµ(ê²½ëŸ‰):
#   * ì„¸ì…˜ KB(ì„ì‹œ): ì—…ë¡œë“œëœ PDF/í…ìŠ¤íŠ¸ì—ì„œ ìœ„í—˜ì–´/í–‰ë™ë¬¸ì„ ëˆ„ì  ìˆ˜ì§‘(ì„¸ì…˜ ë²”ìœ„ì— í•œí•¨)
#   * ì‹œë“œ KB(ê³ ì •): ì•ˆì „Â·ì‚°ì—… ì¬í•´ ê´€ë ¨ ê¸°ì´ˆ ë¦¬ìŠ¤í¬ í‚¤ì›Œë“œ/ì§ˆë¬¸/ì˜ˆë°©ë¬¸ì¥ ì´ˆê¸°ê°’
#   * ì™¸ë¶€ LLM/ë²¡í„°DB/ì„œë²„ ì˜ì¡´ ì—†ìŒ (ì˜¤í”„ë¼ì¸/ë¬´ë£Œ ì‹¤í–‰ ê°€ëŠ¥)
#
# - ì•„í‚¤í…ì²˜ í¬ì¸íŠ¸:
#   * UIëŠ” ë°”ê¾¸ì§€ ì•ŠìŒ(ìš”ì²­ ì •ì±… ì¤€ìˆ˜). ë‚´ë¶€ íŒŒì´í”„ë¼ì¸ë§Œ ê°œì„ /ì£¼ì„ ì¶”ê°€.
#   * ì´ë¯¸ì§€ ìŠ¤ìº” PDFëŠ” OCR ë¯¸ì§€ì›(ëª…í™•íˆ í‘œì‹œ). í…ìŠ¤íŠ¸ PDF/ë³µë¶™ í…ìŠ¤íŠ¸ì— ìµœì í™”.
#   * ê²°ê³¼ë¬¼ì€ TXT/DOCXë¡œ ì¦‰ì‹œ ë‹¤ìš´ë¡œë“œ ê°€ëŠ¥.
#
# - ë³´ì•ˆ/ìš´ì˜ ìœ ì˜ì :
#   * ëª¨ë“  ì²˜ë¦¬ëŠ” ì„¸ì…˜ ë‚´ ë©”ëª¨ë¦¬ì—ì„œë§Œ ë™ì‘(ê°œì¸ í† í°Â·í´ë¼ìš°ë“œ API ë¶ˆí•„ìš”)
#   * ë¬¸ì„œ ì—…ë¡œë“œ ë°ì´í„°ëŠ” ì„œë²„ ë©”ëª¨ë¦¬/ì„¸ì…˜ ë²”ìœ„ì—ë§Œ ì¡´ì¬(ì„¸ì…˜ ì¢…ë£Œ ì‹œ ì†Œë©¸)
#   * ê³µë‹¨ í´ë¼ìš°ë“œ/LMS ì—°ë™ì€ ì¶”í›„ í™•ì¥ ë‹¨ê³„ì—ì„œ REST/SSO ë“±ì˜ ë°©ë²•ìœ¼ë¡œ ê°€ëŠ¥(ë³¸ ë²„ì „ì€ ë…ë¦½í˜•)
# ==========================================================

import io
import zipfile
import re
from collections import Counter
from typing import List, Dict, Tuple

import numpy as np
import regex as rxx
import streamlit as st
from docx import Document
from docx.shared import Pt

# ---------- [PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ ê³„ì¸µ â€” ì˜¤í”ˆì†ŒìŠ¤ ì¡°í•© ì„¤ëª…] ----------
# 1) pdfminer.six ì¶”ì¶œ ìš°ì„ : í…ìŠ¤íŠ¸ ê¸°ë°˜ PDFì—ì„œ ë³¸ë¬¸ í…ìŠ¤íŠ¸ë¥¼ êµ¬ì¡°ì ìœ¼ë¡œ ì¶”ì¶œ.
# 2) ì‹¤íŒ¨/ë¶€ì¡± ì‹œ pypdfium2ë¡œ ê°„ë‹¨ ë¡œë“œí•˜ì—¬ "ì´ë¯¸ì§€/ìŠ¤ìº”" ê°€ëŠ¥ì„±ë§Œ ê°ì§€(ì´ë²ˆ ë²„ì „ OCR ë¯¸ì‚¬ìš©).
#    - ì¶”í›„ ë¬´ë£Œ OCR(ì˜ˆ: tesseract + pyocr/ocrmypdf) ì¡°í•©ìœ¼ë¡œ í™•ì¥ ê°€ëŠ¥.
pdf_extract_text = None
try:
    from pdfminer.high_level import extract_text as _extract_text
    pdf_extract_text = _extract_text
except Exception:
    try:
        from pdfminer_high_level import extract_text as _extract_text_compat  # type: ignore
        pdf_extract_text = _extract_text_compat
    except Exception:
        pdf_extract_text = None

import pypdfium2 as pdfium

# ---------- [Streamlit UI ì„¤ì • â€” ë³€ê²½ ê¸ˆì§€ ì§€ì¹¨ ì¤€ìˆ˜] ----------
st.set_page_config(page_title="OPS2TBM", page_icon="ğŸ¦º", layout="wide")

# -------------------- ì‹œë“œ KB(ì •ì ) --------------------
# - ë¦¬ìŠ¤í¬ í‚¤ì›Œë“œ/í–‰ë™ ìˆ˜ì¹™/ì ê²€ ì§ˆë¬¸ì˜ "ì´ˆê¸°ê°’"ì„ ì£¼ì…(ë¬¸ì„œ ë„ë©”ì¸ì— ë§ì¶˜ ê°€ë²¼ìš´ prior).
SEED_RISK_MAP = {
    "ì¤‘ë…":"ì¤‘ë…","ë–¨ì–´ì§":"ë–¨ì–´ì§","ë¼ì„":"ë¼ì„","ì§ˆì‹":"ì§ˆì‹","í™”ì¬":"í™”ì¬","ê¹”ë¦¼":"ê¹”ë¦¼",
    "ë§ìŒ":"ë§ìŒ","ê°ì „":"ê°ì „","ì§€ë¶•":"ì§€ë¶•ì‘ì—…","ì˜ˆì´ˆ":"ì˜ˆì´ˆ","í­ë°œ":"í­ë°œ","ì²œê³µê¸°":"ì²œê³µ",
    "ì„ ë°˜":"ì ˆì‚­","ì»¨ë² ì´ì–´":"í˜‘ì°©","ë¶€ë”ªí˜":"ì¶©ëŒ","ë¯¸ì„¸ë¨¼ì§€":"ë¯¸ì„¸ë¨¼ì§€","í¬ë ˆì¸":"ì–‘ì¤‘",
    "ë¬´ë„ˆì§":"ë¶•ê´´","ë¹„ê³„":"ë¹„ê³„","ì¶”ë½":"ì¶”ë½","í­ì—¼":"í­ì—¼","ë²Œëª©":"ë²Œëª©","ë‚™í•˜":"ë‚™í•˜","ë¶•ê´´":"ë¶•ê´´",
    "ê°±í¼":"ë¹„ê³„","ë°œíŒ":"ë¹„ê³„","í™”í•™ë¬¼ì§ˆ":"í™”í•™ë¬¼ì§ˆ","ë°€íê³µê°„":"ë°€íê³µê°„"
}
SEED_ACTIONS = [
    "ë°€íê³µê°„ì‘ì—… êµìœ¡ ë° í›ˆë ¨ ì‹¤ì‹œ","ì¶œì… ì „ ì¶©ë¶„í•œ í™˜ê¸° ì‹¤ì‹œ","ì‘ì—… ì „ ê°€ìŠ¤ë†ë„ ì¸¡ì • ë° ê¸°ë¡",
    "ì‘ì—… ìƒí™© ê°ì‹œì ë°°ì¹˜","ì¶œì…Â·í‡´ì¥ ì¸ì› ì ê²€","ë³´í˜¸ì¥êµ¬ ì—†ì´ êµ¬ì¡° ê¸ˆì§€",
    "MSDS í™•ì¸ ë° ìœ í•´ì„± êµìœ¡ ì‹¤ì‹œ","êµ­ì†Œë°°ê¸°ì¥ì¹˜ ì„¤ì¹˜Â·ê°€ë™","í™˜ê¸°ê°€ ë¶ˆì¶©ë¶„í•œ ê³µê°„ì—ì„œëŠ” ê¸‰ê¸°/ë°°ê¸°íŒ¬ ì‚¬ìš©",
    "ìœ ê¸°í™”í•©ë¬¼ ì·¨ê¸‰ ì‹œ ë°©ë…ë§ˆìŠ¤í¬(ê°ˆìƒ‰ ì •í™”í†µ) ì°©ìš©","ì†¡ê¸°ë§ˆìŠ¤í¬Â·ê³µê¸°í˜¸í¡ê¸° ì ì • ì‚¬ìš©",
    "ì˜ˆì´ˆê¸° ì •ì§€ í›„ ì´ë¬¼ì§ˆ ì œê±°Â·ì ê²€","ì˜ˆì´ˆÂ·ë²Œëª© ì‘ì—… ì‹œ ì•ˆì „ê±°ë¦¬ ìœ ì§€ ë° ëŒ€í”¼ë¡œ í™•ë³´",
    "ì‘ì—…ë°œíŒ ê²¬ê³ íˆ ì„¤ì¹˜ ë° ìƒíƒœ ì ê²€","ê°œêµ¬ë¶€Â·ê°œêµ¬ì°½ ì¶”ë½ ìœ„í—˜ êµ¬ê°„ ì•ˆì „ë‚œê°„ ì„¤ì¹˜",
    "ì•ˆì „ëŒ€ ì§€ì§€ì  ì—°ê²° ë° ë¼ì´í”„ë¼ì¸ ì‚¬ìš©","ìœ„í—˜êµ¬ì—­ ì„¤ì •Â·ì¶œì…í†µì œÂ·ê°ì‹œì ë°°ì¹˜",
    "ì–‘ì¤‘ ê³„íš ìˆ˜ë¦½ ë° ì‹ í˜¸ìˆ˜ ì§€ì •Â·í†µì‹  ìœ ì§€","íšŒì „ì²´Â·ë¬¼ë¦¼ì  ë°©í˜¸ì¥ì¹˜ ì„¤ì¹˜ ë° ì ê²€",
    "ì‘ì—… ì „ ì‘ì—…ê³„íšì„œ ì‘ì„± ë° ì‘ì—…ì§€íœ˜ì ì§€ì •","ê°œì¸ë³´í˜¸êµ¬ ì°©ìš©(ì•ˆì „ëª¨Â·ë³´í˜¸ì•ˆê²½Â·ì•ˆì „í™” ë“±)",
    "í™”ê¸°ì‘ì—… í—ˆê°€ ë° ì•ˆì „ì ê²€","ì •ë¹„Â·ì²­ì†ŒÂ·ì ê²€ ì‹œ ê¸°ê³„ ì „ì› ì°¨ë‹¨",
    "ë°€íê³µê°„ ì‘ì—… ì‹œ ì‚°ì†ŒÂ·ìœ í•´ê°€ìŠ¤ ë†ë„ ì¸¡ì •","ìœ„í—˜ë¬¼ì§ˆ ì·¨ê¸‰ ì‹œ MSDS ë¹„ì¹˜Â·ê²Œì‹œ ë° êµìœ¡",
    "ìœ„í—˜ë¬¼ì§ˆ ì·¨ê¸‰ ì‹œ ë¶ˆì¹¨íˆ¬ì„± ë³´í˜¸ë³µÂ·ë°©ë…ë§ˆìŠ¤í¬ ì°©ìš©","í™˜ê¸° ì‹¤ì‹œ ë° ê°ì‹œì¸ ë°°ì¹˜"
]
SEED_QUESTIONS = [
    "ì‘ì—… ì „ ì‘ì—…ê³„íšì„œì™€ ìœ„í—˜ì„±í‰ê°€ë¥¼ ê²€í† í–ˆìŠµë‹ˆê¹Œ?",
    "ê°œêµ¬ë¶€Â·ê°œêµ¬ì°½ ë“± ì¶”ë½ ìœ„í—˜ êµ¬ê°„ì— ì•ˆì „ë‚œê°„ì„ ì„¤ì¹˜í–ˆìŠµë‹ˆê¹Œ?",
    "ì‘ì—…ë°œíŒì´ ê²¬ê³ í•˜ê²Œ ì„¤ì¹˜ë˜ê³  ìƒíƒœê°€ ì–‘í˜¸í•©ë‹ˆê¹Œ?",
    "ì•ˆì „ëŒ€ ì—°ê²°ì ê³¼ ë¼ì´í”„ë¼ì¸ì´ í™•ë³´ë˜ì—ˆìŠµë‹ˆê¹Œ?",
    "êµ­ì†Œë°°ê¸°ì¥ì¹˜ë¥¼ ê°€ë™í•˜ê³  í™˜ê¸° ê²½ë¡œê°€ í™•ë³´ë˜ì—ˆìŠµë‹ˆê¹Œ?",
    "í˜¸í¡ë³´í˜¸êµ¬ê°€ ì‘ì—…ì— ì í•©í•˜ë©° ê´€ë¦¬ê°€ ë˜ê³  ìˆìŠµë‹ˆê¹Œ?",
    "ë°€íê³µê°„ ì¶œì…Â·í‡´ì¥ ì¸ì› ì ê²€ì´ ì´ë£¨ì–´ì§€ê³  ìˆìŠµë‹ˆê¹Œ?",
    "ì˜ˆì´ˆÂ·ë²Œëª© ì‘ì—… ì‹œ ì•ˆì „ê±°ë¦¬ì™€ ëŒ€í”¼ë¡œë¥¼ í™•ë³´í–ˆìŠµë‹ˆê¹Œ?",
    "ì–‘ì¤‘ ì‘ì—…ì— ì‹ í˜¸ìˆ˜ ì§€ì • ë° í†µì‹ ì²´ê³„ê°€ ë§ˆë ¨ë˜ì—ˆìŠµë‹ˆê¹Œ?",
    "íšŒì „ì²´Â·ë¬¼ë¦¼ì  ë°©í˜¸ì¥ì¹˜ê°€ ì •ìƒ ë™ì‘í•©ë‹ˆê¹Œ?"
]

# ---------- [ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™” â€” ì €ì¥ì†Œ/KB/ìºì‹œ] ----------
def _init_once():
    ss = st.session_state
    ss.setdefault("uploader_key", 0)
    ss.setdefault("kb_terms", Counter())     # ë™ì  ìš©ì–´(í† í°) ì¶œí˜„ ë¹ˆë„ â€” TF-IDF ê°€ì¤‘ì¹˜ì— ë°˜ì˜
    ss.setdefault("kb_actions", [])          # ë™ì ìœ¼ë¡œ ìˆ˜ì§‘ëœ í–‰ë™í˜• ìˆ˜ì¹™ ë¬¸ì¥
    ss.setdefault("kb_questions", [])        # ë™ì ìœ¼ë¡œ ìˆ˜ì§‘ëœ ì ê²€í˜• ì§ˆë¬¸
    ss.setdefault("domain_toggle", False)    # í…œí”Œë¦¿ ê°•í™” í† ê¸€(ë³´ìˆ˜ì  ì ìš©)
    ss.setdefault("seed_loaded", False)      # ì‹œë“œ KB 1íšŒ ì£¼ì… ì—¬ë¶€
    ss.setdefault("last_file_diag", {})      # íŒŒì¼ ì§„ë‹¨(í¬ê¸°/ì¶”ì¶œë¬¸ììˆ˜/ë©”ëª¨)
    ss.setdefault("last_extracted_cache", "")# ìµœê·¼ ì¶”ì¶œ í…ìŠ¤íŠ¸ ìºì‹œ
_init_once()

# -------------------- í•œêµ­ì–´ ì¡°ì‚¬/ë„ì–´ì“°ê¸° ë³´ì • --------------------
# - ê·œì¹™í˜• NLGì˜ í•µì‹¬: ëª…ì‚¬ + ëª©ì ê²© ì¡°ì‚¬ ìë™ ë¶€ì°©, ë„ì–´ì“°ê¸°/ì¢…ê²°çµ±ä¸€
def _has_final_consonant(k: str) -> bool:
    if not k: return False
    ch = k[-1]
    base = ord('ê°€'); code = ord(ch) - base
    if code < 0 or code > 11171: return False
    jong = code % 28
    return jong != 0

def add_obj_particle(noun: str) -> str:
    noun = noun.strip()
    if not noun: return noun
    particle = "ì„" if _has_final_consonant(noun[-1]) else "ë¥¼"
    return f"{noun}{particle}"

def tidy_korean_spaces(s: str) -> str:
    s = re.sub(r"\s+", " ", s)
    s = s.replace("ì „ì¶©ë¶„í•œ","ì „ ì¶©ë¶„í•œ").replace("ì „ì¶©ë¶„íˆ","ì „ ì¶©ë¶„íˆ")
    s = re.sub(r"\s([,.])", r"\1", s)
    return s.strip()

# -------------------- ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ --------------------
# - OPS ë¬¸ì„œ íŠ¹ì„±(ë¶ˆë¦¿, ì¤„ë°”ê¿ˆ ë¶„ì ˆ, ë¨¸ë¦¬ê¸€ íŒŒí¸í™”)ì„ ìì—°ë¬¸ìœ¼ë¡œ ë¬¶ëŠ” ë‹¨ê³„
NOISE_PATTERNS = [
    r"^ì œ?\s?\d{4}\s?[-.]?\s?\d+\s?í˜¸$",
    r"^(ë™ì ˆê¸°\s*ì£¼ìš”ì‚¬ê³ |ì•ˆì „ì‘ì—…ë°©ë²•|ì½˜í…ì¸ \s*ë§í¬|ì±…ì\s*OPS|ìˆí¼\s*OPS)$",
    r"^(í¬ìŠ¤í„°|ì±…ì|ìŠ¤í‹°ì»¤|ì½˜í…ì¸  ë§í¬)$",
    r"^(ìŠ¤ë§ˆíŠ¸í°\s*APP|ì¤‘ëŒ€ì¬í•´\s*ì‚¬ì´ë Œ|ì‚°ì—…ì•ˆì „í¬í„¸|ê³ ìš©ë…¸ë™ë¶€)$",
    r"^https?://\S+$", r"^\(?\s*PowerPoint\s*í”„ë ˆì  í…Œì´ì…˜\s*\)?$",
    r"^ì•ˆì „ë³´ê±´ìë£Œì‹¤.*$", r"^ë°°í¬ì²˜\s+.*$", r"^í™ˆí˜ì´ì§€\s+.*$",
    r"^VR\s+.*$", r"^ë¦¬í”Œë¦¿\s+.*$", r"^ë™ì˜ìƒ\s+.*$", r"^APP\s+.*$",
    r".*ê²€ìƒ‰í•´\s*ë³´ì„¸ìš”.*$",
]
BULLET_PREFIX = r"^[\s\-\â€¢\â—\â–ª\â–¶\â–·\Â·\*\u25CF\u25A0\u25B6\u25C6\u2022\u00B7\u279C\u27A4\u25BA\u25AA\u25AB\u2611\u2713\u2714\u2716\u2794\u27A2]+"
DATE_PAT = r"([â€™']?\d{2,4})\.\s?(\d{1,2})\.\s?(\d{1,2})\.?"
META_PATTERNS = [
    r"<\s*\d+\s*ëª…\s*ì‚¬ë§\s*>", r"<\s*\d+\s*ëª…\s*ì‚¬ìƒ\s*>", r"<\s*\d+\s*ëª…\s*ì˜ì‹ë¶ˆëª…\s*>",
    r"<\s*ì‚¬ë§\s*\d+\s*ëª…\s*>", r"<\s*ì‚¬ìƒ\s*\d+\s*ëª…\s*>"
]
STOP_TERMS = set("""
ë° ë“± ê´€ë ¨ ì‚¬í•­ ë‚´ìš© ì˜ˆë°© ì•ˆì „ ì‘ì—… í˜„ì¥ êµìœ¡ ë°©ë²• ê¸°ì¤€ ì¡°ì¹˜
ì‹¤ì‹œ í™•ì¸ í•„ìš” ê²½ìš° ëŒ€ìƒ ì‚¬ìš© ê´€ë¦¬ ì ê²€ ì ìš© ì •ë„ ì£¼ì˜ ì¤‘ ì „ í›„
ì£¼ìš” ì‚¬ë¡€ ì•ˆì „ì‘ì—…ë°©ë²• í¬ìŠ¤í„° ë™ì˜ìƒ ë¦¬í”Œë¦¿ ê°€ì´ë“œ ìë£Œì‹¤ ê²€ìƒ‰
í‚¤ë©”ì„¸ì§€ êµìœ¡í˜ì‹ ì‹¤ ì•ˆì „ë³´ê±´ê³µë‹¨ ê³µë‹¨ ìë£Œ êµ¬ë… ì•ˆë‚´ ì—°ë½ ì°¸ê³  ì¶œì²˜
ì†Œì¬ ì†Œì¬ì§€ ìœ„ì¹˜ ì¥ì†Œ ì§€ì—­ ì‹œêµ°êµ¬ ì„œìš¸ ì¸ì²œ ë¶€ì‚° ëŒ€êµ¬ ëŒ€ì „ ê´‘ì£¼ ìš¸ì‚° ì„¸ì¢… ê²½ê¸°ë„ ì¶©ì²­ ì „ë¼ ê²½ìƒ ê°•ì› ì œì£¼
ëª… ê±´ í˜¸ í˜¸ì°¨ í˜¸ìˆ˜ í˜ì´ì§€ ìª½ ë¶€ë¡ ì°¸ê³  ê·¸ë¦¼ í‘œ ëª©ì°¨
ì•ˆì „ë³´ê±´ ops í‚¤ ë©”ì„¸ì§€ í‚¤ë©”ì„¸ì§€ ìë£Œ opsêµì•ˆ êµì•ˆ
""".split())
LABEL_DROP_PAT = [
    r"^\d+$", r"^\d{2,4}[-_]\d{1,}$", r"^\d{4}$", r"^(ì œ)?\d+í˜¸$", r"^(í˜¸|í˜¸ìˆ˜|í˜¸ì°¨)$",
    r"^(ì‚¬ì—…ì¥|ì—…ì²´|ì†Œì¬|ì†Œì¬ì§€|ì¥ì†Œ|ì§€ì—­)$", r"^\d+\s*(ëª…|ê±´)$"
]

RISK_KEYWORDS = dict(SEED_RISK_MAP)  # ì„¸ì…˜ ìˆ˜ì§‘ìœ¼ë¡œ ê³„ì† ë³´ê°•

def tokens(s: str) -> List[str]:
    return rxx.findall(r"[ê°€-í£a-z0-9]{2,}", s.lower())

def normalize_text(t: str) -> str:
    t = t.replace("\x0c","\n")
    t = re.sub(r"[ \t]+\n","\n", t)
    t = re.sub(r"\n{3,}","\n\n", t)
    return t.strip()

def strip_noise_line(line: str) -> str:
    s = (line or "").strip()
    if not s: return ""
    s = re.sub(BULLET_PREFIX,"", s).strip()
    for pat in NOISE_PATTERNS:
        if re.match(pat, s, re.IGNORECASE):
            return ""
    s = re.sub(r"https?://\S+","", s).strip()
    s = s.strip("â€¢â—â–ªâ–¶â–·Â·-â€”â€“")
    return s

def _looks_like_heading(s: str) -> bool:
    return bool(re.search(r"(ë°©ë²•|ìˆ˜ì¹™|ëŒ€ì±…|ì•ˆì „ì¡°ì¹˜|ì˜ˆë°©|ì‘ì—…ë°©ë²•|ì‚¬ê³ ì‚¬ë¡€|ì£¼ìš”\s*ì‚¬ê³ ì‚¬ë¡€)\s*[:ï¼š]?$", s))

def merge_broken_lines(lines: List[str]) -> List[str]:
    # ë¶ˆì™„ì „ í—¤ë”/ì§§ì€ ë¬¸ì¥ë“¤ì„ ì˜† ì¤„ê³¼ í•©ì³ ìì—°ë¬¸ìœ¼ë¡œ ë§Œë“ ë‹¤.
    out, buf = [], ""
    for raw in lines:
        s = strip_noise_line(raw)
        if not s: continue
        if _looks_like_heading(s) or s.endswith((":", "ï¼š", "-", "Â·")):
            if buf: out.append(buf)
            buf = s; continue
        if buf:
            if buf.endswith((":", "ï¼š", "-", "Â·")):
                buf = tidy_korean_spaces(buf.rstrip(" :ï¼š-Â·") + " " + s); continue
            if (len(buf) < 20 and not re.search(r"[.?!ë‹¤]$", buf)) or (len(s) < 20 and not re.search(r"[.?!ë‹¤]$", s)):
                buf = tidy_korean_spaces(buf + " " + s); continue
            if not re.search(r"[.?!ë‹¤]$", buf):
                buf = tidy_korean_spaces(buf + " " + s); continue
            out.append(buf); buf = s
        else:
            buf = s
    if buf: out.append(buf)
    return out

def combine_date_with_next(lines: List[str]) -> List[str]:
    # ë‚ ì§œ í•œ ì¤„ + ë‹¤ìŒ ì¤„ ì‚¬ê³  ì„¤ëª… â†’ í•œ ì¤„ë¡œ í•©ì³ ê°€ë…ì„±/ì˜ë¯¸ ìœ ì§€
    out = []; i = 0
    while i < len(lines):
        cur = strip_noise_line(lines[i])
        if re.search(DATE_PAT, cur) and (i+1) < len(lines):
            nxt = strip_noise_line(lines[i+1])
            if re.search(r"(ì‚¬ë§|ì‚¬ìƒ|ì‚¬ê³ |ì¤‘ë…|í™”ì¬|ë¶•ê´´|ì§ˆì‹|ì¶”ë½|ê¹”ë¦¼|ë¶€ë”ªí˜|ê°ì „|í­ë°œ)", nxt):
                m = re.search(DATE_PAT, cur)
                y, mo, d = m.groups()
                y = int(str(y).replace("â€™","").replace("'","")); y = 2000 + y if y < 100 else y
                out.append(f"{int(y)}ë…„ {int(mo)}ì›” {int(d)}ì¼, {nxt}")
                i += 2; continue
        out.append(cur); i += 1
    return out

# ì—°ê²°ì–´/ì‚¬ê³ í‚¤ì›Œë“œ ê¸°ë°˜ìœ¼ë¡œ ì—°ì† ì„œìˆ ì„ í•˜ë‚˜ì˜ ì‚¬ë¡€ë¬¸ìœ¼ë¡œ ë´‰í•©
CASE_JOIN_TRIG = ("ì“°ëŸ¬ì§€ì","êµ¬ì¡°í•˜ë˜ ì¤‘","ì°¨ë¡€ë¡œ","ì´ì–´","ì´í›„","ë™ì‹œì—","ê²°êµ­","ê·¸ ê³¼ì •ì—ì„œ","ì™¸ë¶€ì— ìˆë˜","í˜„ì¥ì— ìˆë˜")
CASE_KEYWORDS = ("ì‚¬ë§","ì‚¬ìƒ","ì¤‘ë…","ì¶”ë½","ë¶•ê´´","ë‚™í•˜","ì§ˆì‹","ë¼ì„","ê¹”ë¦¼","ë¶€ë”ªí˜","ê°ì „","í­ë°œ","ì‚¬ê³ ")

def stitch_case_blocks(sents: List[str]) -> List[str]:
    if not sents: return sents
    out = []; i = 0
    while i < len(sents):
        cur = sents[i].strip(); merged = cur; j = i + 1; merged_any = False
        while j < len(sents):
            nxt = sents[j].strip()
            cond_keyword = (any(k in cur for k in CASE_KEYWORDS) and any(k in nxt for k in CASE_KEYWORDS))
            cond_trigger = (any(t in nxt for t in CASE_JOIN_TRIG) or any(t in cur for t in CASE_JOIN_TRIG))
            if cond_keyword or cond_trigger:
                sep = ", " if not merged.endswith(("ë‹¤.","ìŠµë‹ˆë‹¤.","í–ˆë‹¤.",".")) else " "
                merged = tidy_korean_spaces(merged.rstrip(" .") + sep + nxt.lstrip(" ,"))
                cur = merged; j += 1; merged_any = True
            else:
                break
        out.append(merged); i = j if merged_any else i + 1
    seen, dedup = set(), []
    for s in out:
        k = re.sub(r"\s+","", s)
        if k not in seen:
            seen.add(k); dedup.append(s)
    return dedup

def preprocess_text_to_sentences(text: str) -> List[str]:
    # [ì „ì²˜ë¦¬ í•µì‹¬ ë‹¨ê³„] ë…¸ì´ì¦ˆ ì œê±° â†’ ì¤„ ë³‘í•© â†’ ë‚ ì§œ-ì‚¬ê³  ê²°í•© â†’ ë¬¸ì¥í™” â†’ ì‚¬ë¡€ë¬¸ ë´‰í•©
    text = normalize_text(text)
    raw_lines = [ln for ln in text.splitlines() if ln.strip()]
    lines = merge_broken_lines(raw_lines)
    lines = combine_date_with_next(lines)
    joined = "\n".join(lines)
    raw = rxx.split(r"(?<=[\.!\?]|ë‹¤\.)\s+|\n+", joined)
    sents = []
    for s in raw:
        s2 = strip_noise_line(s)
        if not s2: continue
        if re.search(r"(ì£¼ìš”ì‚¬ê³ |ì•ˆì „ì‘ì—…ë°©ë²•|ì½˜í…ì¸ ë§í¬|ì£¼ìš” ì‚¬ê³ ê°œìš”)$", s2): continue
        if len(re.sub(r"\s+","", s2)) < 4:
            continue
        sents.append(s2)
    sents = stitch_case_blocks(sents)
    return sents

# -------------------- (1) í—¤ë” ê¸°ë°˜ ì„¹ì…˜ íŒŒì„œ --------------------
SECTION_HEADERS_CASE = [r"ì£¼ìš”\s*ì‚¬ê³ ì‚¬ë¡€", r"ì‚¬ê³ ì‚¬ë¡€", r"ì‚¬ê³ \s*ì‚¬ë¡€"]
SECTION_HEADERS_PREV = [
    r"ì•ˆì „\s*ì‘ì—…ë°©ë²•", r"ë°€íê³µê°„\s*ì‘ì—…\s*ì‹œ", r"ë°€íê³µê°„ì‘ì—…\s*ì‹œ",
    r"ìœ„í—˜ë¬¼ì§ˆ\s*ì·¨ê¸‰\s*ì‹œ", r"ì˜ˆë°©\s*ìˆ˜ì¹™", r"ì‹¤ì²œ\s*ìˆ˜ì¹™", r"ì˜ˆë°©\s*ì¡°ì¹˜",
    r"ì•ˆì „\s*ìˆ˜ì¹™", r"ì‘ì—…\s*ìˆ˜ì¹™"
]
def _compile_headers(headers: List[str]) -> List[re.Pattern]:
    return [re.compile(h, re.IGNORECASE) for h in headers]
HDR_CASE = _compile_headers(SECTION_HEADERS_CASE)
HDR_PREV = _compile_headers(SECTION_HEADERS_PREV)

def split_keep_lines(text: str) -> List[str]:
    t = normalize_text(text)
    lines = [ln.rstrip() for ln in t.splitlines()]
    return lines

def _is_header(line: str, hdrs: List[re.Pattern]) -> bool:
    s = strip_noise_line(line)
    return any(h.search(s) for h in hdrs)

def _is_bullet(line: str) -> bool:
    return bool(re.match(BULLET_PREFIX, line.strip()) or re.match(r"^\s*[\-Â·â€¢â–¶â–·\*]\s+", line.strip()))

def extract_section_bullets(text: str, which: str = "case") -> List[str]:
    # [ì„¹ì…˜ íŒŒì„œ] "ì‚¬ê³ ì‚¬ë¡€/ì•ˆì „ì‘ì—…ë°©ë²•" ê°™ì€ ì œëª© ë‹¤ìŒì˜ ë¶ˆë¦¿ì„ ëª¨ì€ë‹¤.
    lines = split_keep_lines(text)
    hdrs = HDR_CASE if which == "case" else HDR_PREV
    items: List[str] = []
    capture = False
    for i, raw in enumerate(lines):
        s = raw.strip()
        if not s:
            if capture: break
            continue
        if _is_header(raw, hdrs):
            capture = True
            continue
        if capture:
            if _is_header(raw, HDR_CASE + HDR_PREV):
                break
            clean = strip_noise_line(raw)
            if not clean:
                continue
            items.append(clean)
    merged = merge_broken_lines(items)
    return [x for x in merged if len(re.sub(r"\s+","", x)) >= 2]

# -------------------- (2) í—¤ë” ì—†ëŠ” ë¬¸ì„œ: ë¶ˆë¦¿ í´ëŸ¬ìŠ¤í„° + ìë™ ë¶„ë¥˜ --------------------
# - í–‰ë™ ë™ì‚¬ íŒ¨í„´ìœ¼ë¡œ ì˜ˆë°©í˜• ë¶„ë¥˜, ì‚¬ê³  í‚¤ì›Œë“œ/ë‚ ì§œë¡œ ì‚¬ë¡€í˜• ë¶„ë¥˜
ACTION_VERBS = [
    "ì„¤ì¹˜","ë°°ì¹˜","ì°©ìš©","ì ê²€","í™•ì¸","ì¸¡ì •","ê¸°ë¡","í‘œì‹œ","ì œê³µ","ë¹„ì¹˜","ë³´ê³ ","ì‹ ê³ ",
    "êµìœ¡","ì£¼ì§€","ì¤‘ì§€","í†µì œ","íœ´ì‹","í™˜ê¸°","ì°¨ë‹¨","êµëŒ€","ë°°ì œ","ë°°ë ¤","ê°€ë™","ì¤€ìˆ˜",
    "ìš´ì˜","ìœ ì§€","êµì²´","ì •ë¹„","ì²­ì†Œ","ê³ ì •","ê²©ë¦¬","ë³´í˜¸","ë³´ìˆ˜","ì‘ì„±","ì§€ì •"
]
ACTION_PAT = (
    r"(?P<obj>[ê°€-í£a-zA-Z0-9Â·\(\)\[\]\/\-\s]{2,}?)\s*(?P<verb>" + "|".join(ACTION_VERBS) + r"|ì‹¤ì‹œ|ìš´ì˜|ê´€ë¦¬)\b"
    r"|(?P<obj2>[ê°€-í£a-zA-Z0-9Â·\(\)\[\]\/\-\s]{2,}?)\s*(ì„|ë¥¼)\s*(?P<verb2>" + "|".join(ACTION_VERBS) + r"|ì‹¤ì‹œ|ìš´ì˜|ê´€ë¦¬)\b"
)

def cluster_bullets(text: str, win: int = 1) -> List[List[str]]:
    lines = split_keep_lines(text)
    clusters: List[List[str]] = []
    cur: List[str] = []
    for ln in lines:
        if _is_bullet(ln):
            cur.append(strip_noise_line(ln))
        else:
            if cur:
                clusters.append(merge_broken_lines(cur))
                cur = []
    if cur:
        clusters.append(merge_broken_lines(cur))
    cleaned = []
    for c in clusters:
        c2 = [x for x in c if x and len(re.sub(r"\s+","", x)) >= 2]
        if c2:
            cleaned.append(c2)
    return cleaned

def looks_case(s: str) -> bool:
    return bool(re.search(r"(ì‚¬ë§|ì‚¬ìƒ|ì‚¬ê³ |ì¤‘ë…|ì¶”ë½|ë¶•ê´´|ë‚™í•˜|ì§ˆì‹|ë¼ì„|ê¹”ë¦¼|ë¶€ë”ªí˜|ê°ì „|í­ë°œ)", s))

def looks_action(s: str) -> bool:
    return bool(re.search(ACTION_PAT, s) or re.search(r"(ì˜ˆë°©|ìˆ˜ì¹™|ì§€ì¹¨|ì•ˆì „ì¡°ì¹˜|ì‘ì—…ë°©ë²•)", s))

def classify_cluster(cluster: List[str]) -> str:
    case_hits = sum(1 for x in cluster if looks_case(x))
    act_hits  = sum(1 for x in cluster if looks_action(x))
    if case_hits > act_hits and case_hits >= 1:
        return "case"
    if act_hits >= max(1, case_hits):
        return "action"
    return "other"

def extract_clusters_by_type(text: str, kind: str) -> List[str]:
    clusters = cluster_bullets(text)
    out: List[str] = []
    for c in clusters:
        typ = classify_cluster(c)
        if typ == kind:
            out += c
    return out

# -------------------- PDF ì½ê¸°/ì§„ë‹¨ --------------------
def read_pdf_text_from_bytes(b: bytes, fname: str = "") -> str:
    # (1) pdfminer.six í…ìŠ¤íŠ¸ ì¶”ì¶œ â†’ ì‹¤íŒ¨ ì‹œ ë¹ˆ ë¬¸ìì—´
    t = ""
    try:
        if pdf_extract_text is not None:
            with io.BytesIO(b) as bio:
                t = pdf_extract_text(bio) or ""
        else:
            t = ""
    except Exception:
        t = ""
    t = normalize_text(t)
    # (2) í…ìŠ¤íŠ¸ê°€ ê±°ì˜ ì—†ì„ ë•Œ pypdfium2ë¡œ ê°„ë‹¨ ì§„ë‹¨(ì´ë¯¸ì§€Â·ìŠ¤ìº” ì¶”ì •)
    if len(t.strip()) < 10:
        try:
            with io.BytesIO(b) as bio:
                pdf = pdfium.PdfDocument(bio)
                if len(pdf) > 0 and not t.strip():
                    st.warning("âš ï¸ ì´ë¯¸ì§€/ìŠ¤ìº” PDFë¡œ ë³´ì…ë‹ˆë‹¤. í˜„ì¬ OCR ë¯¸ì§€ì›.")
        except Exception:
            pass
    st.session_state["last_file_diag"] = {
        "name": fname, "size_bytes": len(b), "extracted_chars": len(t),
        "note": "empty_or_scanned" if (len(t.strip()) < 10) else "ok"
    }
    return t

# -------------------- ìš”ì•½/ì„ë² ë”© ìœ ì‚¬ë„ ìœ í‹¸ --------------------
def tokens_for_vec(s: str) -> List[str]:
    return tokens(s)

def sentence_tfidf_vectors(sents: List[str], kb_boost: Dict[str, float] = None) -> Tuple[np.ndarray, List[str]]:
    # [TF-IDF] ì„¸ì…˜ KBì— ìˆëŠ” í† í°ì€ ê°€ì¤‘ì¹˜â†‘ â†’ ë¬¸ì„œ/ì—…ë¡œë“œ ë°ì´í„°ì— ë¯¼ê°í•œ ìš”ì•½
    toks = [tokens_for_vec(s) for s in sents]
    vocab: Dict[str,int] = {}
    for ts in toks:
        for t in ts:
            if t not in vocab: vocab[t] = len(vocab)
    if not vocab:
        return np.zeros((len(sents),0), dtype=np.float32), []
    M = np.zeros((len(sents), len(vocab)), dtype=np.float32)
    df = np.zeros((len(vocab),), dtype=np.float32)
    for i, ts in enumerate(toks):
        for t in ts:
            w = 1.0
            if kb_boost and t in kb_boost: w *= kb_boost[t]
            M[i, vocab[t]] += w
        for t in set(ts):
            df[vocab[t]] += 1.0
    N = float(len(sents))
    idf = np.log((N+1.0)/(df+1.0)) + 1.0
    M *= idf
    if kb_boost:
        for t, idx in vocab.items():
            if t in kb_boost: M[:, idx] *= (1.0 + 0.2*kb_boost[t])
    M /= (np.linalg.norm(M, axis=1, keepdims=True) + 1e-8)
    return M, list(vocab.keys())

def cosim(X: np.ndarray) -> np.ndarray:
    # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ í–‰ë ¬(ëŒ€ê°=0)
    if X.size == 0: return np.zeros((X.shape[0], X.shape[0]), dtype=np.float32)
    S = np.clip(X @ X.T, 0.0, 1.0); np.fill_diagonal(S, 0.0)
    return S

def textrank_scores(sents: List[str], X: np.ndarray, d: float=0.85, max_iter: int=60, tol: float=1e-4) -> List[float]:
    # [TextRank] ê·¸ë˜í”„ ê¸°ë°˜ ì „í†µ ìš”ì•½ â€” ë§í¬ ë¶„ì„ì²˜ëŸ¼ ì¤‘ìš” ë¬¸ì¥ ì ìˆ˜ ê³„ì‚°
    n = len(sents)
    if n == 0: return []
    W = cosim(X); row = W.sum(axis=1, keepdims=True)
    P = np.divide(W, row, out=np.zeros_like(W), where=row>0)
    r = np.ones((n,1), dtype=np.float32)/n; tel = np.ones((n,1), dtype=np.float32)/n
    for _ in range(max_iter):
        r2 = d*(P.T @ r) + (1-d)*tel
        if np.linalg.norm(r2-r,1) < tol: r = r2; break
        r = r2
    return [float(v) for v in r.flatten()]

def mmr_select(sents: List[str], scores: List[float], X: np.ndarray, k: int, lam: float=0.7) -> List[int]:
    # [MMR] ë‹¤ì–‘ì„± ì œì–´ â€” ì¤‘ë³µ ì¤„ì´ê³  í•µì‹¬ë§Œ ë½‘ìŒ
    S = cosim(X); sel: List[int] = []; rem = set(range(len(sents)))
    while rem and len(sel) < k:
        best, val = None, -1e9
        for i in rem:
            rel = scores[i]; div = max((S[i,j] for j in sel), default=0.0)
            sc = lam*rel - (1-lam)*div
            if sc > val: val, best = sc, i
        sel.append(best); rem.remove(best)
    return sel

def ai_extract_summary(text: str, limit: int=8) -> List[str]:
    # [ë¬´ë£Œ ìš”ì•½ AI] TextRank + MMR + ì„¸ì…˜KB ê°€ì¤‘ì¹˜
    sents = preprocess_text_to_sentences(text)
    if not sents: return []
    kb = st.session_state["kb_terms"]; total = sum(kb.values()) or 1
    kb_boost = {t: 1.0 + (cnt/total)*3.0 for t, cnt in kb.items()} if kb else None
    X, _ = sentence_tfidf_vectors(sents, kb_boost=kb_boost)
    scores = textrank_scores(sents, X)
    idx = mmr_select(sents, scores, X, limit, lam=0.7)
    return [sents[i] for i in idx]

# -------------------- ë„ë©”ì¸ í…œí”Œë¦¿/ìì—°í™” --------------------
def jaccard(a: set, b: set) -> float:
    return len(a & b) / (len(a | b) + 1e-8)

# - íŠ¹ì • ë„ë©”ì¸(ë¹„ê³„/ì–‘ì¤‘/ë°€íê³µê°„ ë“±)ì—ì„œ í”í•œ í‘œí˜„ì„ ë³´ìˆ˜ì ìœ¼ë¡œ ë³´ì •
DOMAIN_TEMPLATES = [
    ({"ë¹„ê³„","ë°œíŒ","ê°±í¼","ì¶”ë½"}, "ì‘ì—…ë°œíŒì„ ê²¬ê³ í•˜ê²Œ ì„¤ì¹˜í•˜ê³  ì•ˆì „ë‚œê°„ ë° ì¶”ë½ë°©í˜¸ë§ì„ í™•ë³´í•©ë‹ˆë‹¤."),
    ({"ì•ˆì „ë‚œê°„","ë‚œê°„","ê°œêµ¬ë¶€"}, "ê°œêµ¬ë¶€Â·ê°œêµ¬ì°½ ë“± ì¶”ë½ ìœ„í—˜ êµ¬ê°„ì— ì•ˆì „ë‚œê°„ì„ ì„¤ì¹˜í•©ë‹ˆë‹¤."),
    ({"MSDS","êµ­ì†Œë°°ê¸°","í™˜ê¸°"}, "ì·¨ê¸‰ ë¬¼ì§ˆì˜ MSDSë¥¼ í™•ì¸í•˜ê³  êµ­ì†Œë°°ê¸°ì¥ì¹˜ë¥¼ ê°€ë™í•˜ì—¬ ì¶©ë¶„íˆ í™˜ê¸°í•©ë‹ˆë‹¤."),
    ({"ì˜ˆì´ˆ","ë²Œëª©","ì˜ˆì´ˆê¸°"}, "ì˜ˆì´ˆÂ·ë²Œëª© ì‘ì—… ì‹œ ì‘ì—…ì ê°„ ì•ˆì „ê±°ë¦¬ë¥¼ ìœ ì§€í•˜ê³  ëŒ€í”¼ë¡œë¥¼ í™•ë³´í•©ë‹ˆë‹¤."),
    ({"í¬ë ˆì¸","ì–‘ì¤‘"}, "ì–‘ì¤‘ ê³„íšì„ ìˆ˜ë¦½í•˜ê³  ì‹ í˜¸ìˆ˜ë¥¼ ì§€ì •í•˜ì—¬ í†µì‹ ì„ ìœ ì§€í•©ë‹ˆë‹¤."),
    ({"ì»¨ë² ì´ì–´","í˜‘ì°©","íšŒì „ì²´"}, "íšŒì „ì²´Â·ë¬¼ë¦¼ì  ì ‘ì´‰ì„ ë°©ì§€í•˜ë„ë¡ ë°©í˜¸ì¥ì¹˜ë¥¼ ì„¤ì¹˜í•˜ê³  ì ê²€í•©ë‹ˆë‹¤."),
]

def _domain_template_apply(s: str, base_text: str) -> str:
    if not st.session_state.get("domain_toggle"): return s
    sent_toks = set(tokens(s)); base_toks = set(tokens(base_text))
    if jaccard(sent_toks, base_toks) < 0.05: return s
    best = None; best_hits = 0
    for triggers, render in DOMAIN_TEMPLATES:
        if (sent_toks & triggers) and (base_toks & triggers):
            hits = len((sent_toks | base_toks) & triggers)
            if hits > best_hits: best_hits = hits; best = render
    return best if best else s

def soften(s: str) -> str:
    # ë¬¸ì²´/ì¢…ê²°çµ±ä¸€ + ë©”íƒ€ í† í° ì œê±°
    s = s.replace("í•˜ì—¬ì•¼","í•´ì•¼ í•©ë‹ˆë‹¤").replace("í•œë‹¤","í•©ë‹ˆë‹¤").replace("í•œë‹¤.","í•©ë‹ˆë‹¤.")
    s = s.replace("ë°”ëë‹ˆë‹¤","í•´ì£¼ì„¸ìš”").replace("í™•ì¸ ë°”ëŒ","í™•ì¸í•´ì£¼ì„¸ìš”")
    s = s.replace("ê¸ˆì§€í•œë‹¤","ê¸ˆì§€í•©ë‹ˆë‹¤").replace("í•„ìš”í•˜ë‹¤","í•„ìš”í•©ë‹ˆë‹¤")
    s = re.sub(r"^\(([^)]+)\)\s*","", s)
    for pat in META_PATTERNS:
        s = re.sub(pat,"", s).strip()
    s = re.sub(BULLET_PREFIX,"", s).strip(" -â€¢â—\t")
    return tidy_korean_spaces(s)

def is_meaningful_sentence(s: str) -> bool:
    raw = re.sub(r"\s+","", s)
    if len(raw) < 4:
        return False
    if re.fullmatch(r"[ê°€-í£\s]*í•©ë‹ˆë‹¤\.", s.strip()):
        return False
    return True

def is_accident_sentence(s: str) -> bool:
    if any(w in s for w in ["ì˜ˆë°©","ëŒ€ì±…","ì§€ì¹¨","ìˆ˜ì¹™"]):
        return False
    return bool(re.search(DATE_PAT, s) or re.search(r"(ì‚¬ë§|ì‚¬ìƒ|ì‚¬ê³ |ì¤‘ë…|ì¶”ë½|ë¶•ê´´|ë‚™í•˜|ì§ˆì‹|ë¼ì„|ê¹”ë¦¼|ë¶€ë”ªí˜|ê°ì „|í­ë°œ)", s))

def is_prevention_sentence(s: str) -> bool:
    return any(w in s for w in ["ì˜ˆë°©","ëŒ€ì±…","ì§€ì¹¨","ìˆ˜ì¹™","ì•ˆì „ì¡°ì¹˜","ì‘ì—…ë°©ë²•"]) or bool(re.search(ACTION_PAT, s))

def is_risk_sentence(s: str) -> bool:
    return any(w in s for w in ["ìœ„í—˜","ìš”ì¸","ì›ì¸","ì¦ìƒ","ê²°ë¹™","ê°•í’","í­ì—¼","ë¯¸ì„¸ë¨¼ì§€","íšŒì „ì²´","ë¹„ì‚°","ë§ë¦¼","ì¶”ë½","ë‚™í•˜","í˜‘ì°©"])

def to_action_sentence(s: str, base_text: str) -> str:
    # [ê·œì¹™í˜• NLG] í–‰ë™ ë™ì‚¬/ëª©ì ì–´ íƒì§€ â†’ ìì—°ìŠ¤ëŸ¬ìš´ ìˆ˜ì¹™ í•œ ë¬¸ì¥ìœ¼ë¡œ ë³´ì •
    s2 = soften(s)
    s2 = re.sub(r"(ìœ„ê¸°íƒˆì¶œ\s*ì•ˆì „ë³´ê±´)", "", s2).strip()
    s2 = re.sub(r"\s*ì—\s*ë”°ë¥¸\s*", " ì‹œ ", s2)
    s2 = re.sub(r"\s*ì—\s*ë”°ë¼\s*", " ì‹œ ", s2)
    s2_tpl = _domain_template_apply(s2, base_text)
    if s2_tpl != s2:
        txt = s2_tpl
        if not txt.endswith(("ë‹¤.","í•©ë‹ˆë‹¤.","ìŠµë‹ˆë‹¤.")):
            txt = txt.rstrip(" .") + " í•©ë‹ˆë‹¤."
        return tidy_korean_spaces(txt)
    m = re.search(ACTION_PAT, s2)
    if not m:
        # ë™ì‚¬ ë¯¸íƒì§€ ì‹œ: ëª…ì‚¬êµ¬ + ì¶”ì • ë™ì‚¬ë¡œ ë³´ê°•
        nounish = re.sub(r"(ì˜|ì—|ì—ì„œ|ì„|ë¥¼|ì™€|ê³¼|ë°)$","", s2).strip()
        if nounish and len(nounish) >= 4:
            guess_verb = "ì„¤ì¹˜" if any(k in nounish for k in ["ë‚œê°„","ë°©í˜¸ë§","ë°œíŒ","ë°©í˜¸ì¥ì¹˜","ì¥ë¹„","ì¥ì¹˜","í‘œì§€"]) else "í™•ì¸"
            obj = add_obj_particle(nounish)
            return tidy_korean_spaces(f"{obj} {guess_verb} í•©ë‹ˆë‹¤.")
        txt = s2 if s2.endswith(("ë‹ˆë‹¤.","í•©ë‹ˆë‹¤.","ë‹¤.")) else (s2.rstrip(" .") + " í•©ë‹ˆë‹¤.")
        return tidy_korean_spaces(txt)
    obj = (m.group("obj") or m.group("obj2") or "").strip()
    verb = (m.group("verb") or m.group("verb2") or "ì‹¤ì‹œ").strip()
    if obj and not re.search(r"(ì„|ë¥¼|ì—|ì—ì„œ|ê³¼|ì™€|ì˜)$", obj):
        obj = add_obj_particle(obj)
    prefix = "ë°˜ë“œì‹œ " if "ì„¤ì¹˜" in verb else ("ì‘ì—… ì „ " if verb in ("í™•ì¸","ì ê²€","ì¸¡ì •","ê¸°ë¡","ì‘ì„±","ì§€ì •") else "")
    core = tidy_korean_spaces(f"{prefix}{obj} {verb}")
    if re.fullmatch(r"(ë°˜ë“œì‹œ |ì‘ì—… ì „ )?\s*(ì„|ë¥¼)\s*(ì‹¤ì‹œ|ê´€ë¦¬|ìš´ì˜)\s*$", core):
        if obj.strip():
            core = tidy_korean_spaces(f"{prefix}{obj} ì‹¤ì‹œ")
        else:
            core = "ì‘ì—… ì „ ì•ˆì „ì¡°ì¹˜ í™•ì¸"
    return core.rstrip(" .") + " í•©ë‹ˆë‹¤."

def repair_action_fragments(lines: List[str]) -> List[str]:
    # ì§§ì€/ë¶ˆì™„ì „ ìˆ˜ì¹™ì´ ì—°ì†ë  ë•Œ ë‹¤ìŒ ì¤„ê³¼ í•©ì³ ì™„ì „í•œ ë¬¸ì¥ìœ¼ë¡œ íšŒë³µ
    out = []
    i = 0
    while i < len(lines):
        cur = soften(lines[i])
        cur_no_sp = re.sub(r"\s+","", cur)
        has_verb = bool(re.search(ACTION_PAT, cur)) or any(v in cur for v in ["í•©ë‹ˆë‹¤","í•œë‹¤","ì‹¤ì‹œ","ì„¤ì¹˜","ì°©ìš©","ì ê²€","í™•ì¸","ë°°ì¹˜"])
        if (len(cur_no_sp) < 20) and (not has_verb):
            merged = cur
            j = i + 1
            while j < len(lines):
                nxt = soften(lines[j])
                merged = tidy_korean_spaces(merged + " " + nxt)
                if re.search(ACTION_PAT, merged) or any(v in merged for v in ["í•©ë‹ˆë‹¤","í•œë‹¤","ì‹¤ì‹œ","ì„¤ì¹˜","ì°©ìš©","ì ê²€","í™•ì¸","ë°°ì¹˜"]):
                    break
                j += 1
            out.append(merged); i = j + 1
        else:
            out.append(cur); i += 1
    return out

# -------------------- KB(ì„¸ì…˜ ë™ì  â€œê²½ëŸ‰ í•™ìŠµâ€) --------------------
def seed_kb_once():
    # ì‹œë“œ KBëŠ” 1íšŒë§Œ ì£¼ì… (ì„¸ì…˜ ê¸°ì¤€)
    if not st.session_state["seed_loaded"]:
        for t, k in SEED_RISK_MAP.items():
            if t not in RISK_KEYWORDS: RISK_KEYWORDS[t] = k
        for a in SEED_ACTIONS:
            if 2 <= len(a) <= 160:
                st.session_state["kb_actions"].append(a if a.endswith(("ë‹¤","ë‹¤.","í•©ë‹ˆë‹¤","í•©ë‹ˆë‹¤.")) else a + " í•©ë‹ˆë‹¤.")
        for q in SEED_QUESTIONS:
            st.session_state["kb_questions"].append(q if q.endswith("?") else q + "?")
        for t in SEED_RISK_MAP.keys():
            st.session_state["kb_terms"][t] += 5
        st.session_state["seed_loaded"] = True

def kb_ingest_text(text: str) -> None:
    # ì—…ë¡œë“œ í…ìŠ¤íŠ¸ì—ì„œ ìœ„í—˜ì–´/í–‰ë™ë¬¸/ì ê²€ì§ˆë¬¸ì„ ì¶”ì¶œí•˜ì—¬ ì„¸ì…˜ KBì— ì¶•ì 
    if not (text or "").strip(): return
    sents = preprocess_text_to_sentences(text)
    for s in sents:
        for t in tokens(s):
            if len(t) >= 2:
                st.session_state["kb_terms"][t] += 1
                if re.search(r"(ì¶”ë½|ë‚™í•˜|ê¹”ë¦¼|ë¼ì„|ì¤‘ë…|ì§ˆì‹|í™”ì¬|í­ë°œ|ê°ì „|í­ì—¼|ë¶•ê´´|ë¹„ê³„|ê°±í¼|ì˜ˆì´ˆ|ë²Œëª©|ì»¨ë² ì´ì–´|í¬ë ˆì¸|ì§€ë¶•|ì„ ë°˜|ì²œê³µ|í™”í•™ë¬¼ì§ˆ|ë°€íê³µê°„)", t):
                    if t not in RISK_KEYWORDS: RISK_KEYWORDS[t] = t
    action_candidates = [s for s in sents if (re.search(ACTION_PAT, s) or is_prevention_sentence(s))]
    action_candidates = repair_action_fragments(action_candidates)
    for s in action_candidates:
        cand = to_action_sentence(s, text)
        if 2 <= len(cand) <= 180:
            st.session_state["kb_actions"].append(cand)
    for s in sents:
        if "?" in s or "í™•ì¸" in s or "ì ê²€" in s:
            q = soften(s if s.endswith("?") else s + " ë§ìŠµë‹ˆê¹Œ?")
            if 2 <= len(q) <= 160:
                st.session_state["kb_questions"].append(q)

def kb_prune() -> None:
    # ì„¸ì…˜ KB í­ì£¼ ë°©ì§€ â€” ì¤‘ë³µ ì œê±° ë° ìƒí•œ ì ìš©
    def dedup_keep_order(lst: List[str]) -> List[str]:
        seen, out = set(), []
        for x in lst:
            k = re.sub(r"\s+","", x)
            if k not in seen:
                seen.add(k); out.append(x)
        return out
    st.session_state["kb_actions"] = dedup_keep_order(st.session_state["kb_actions"])[:2000]
    st.session_state["kb_questions"] = dedup_keep_order(st.session_state["kb_questions"])[:800]
    st.session_state["kb_terms"] = Counter(dict(st.session_state["kb_terms"].most_common(4000)))

def kb_match_candidates(cands: List[str], base_text: str, limit: int, min_sim: float = 0.12) -> List[str]:
    # ë¬¸ì„œ í† í°ê³¼ì˜ ìì¹´ë“œ ìœ ì‚¬ë„ë¡œ ì„¸ì…˜ KBì—ì„œ ì í•©í•œ ìˆ˜ì¹™/ì§ˆë¬¸ì„ ì„ íƒ
    bt = set(tokens(base_text))
    present_risks = {t for t in bt if (t in RISK_KEYWORDS or t in RISK_KEYWORDS.values())}
    scored: List[Tuple[float,str]] = []
    for c in cands:
        ct = set(tokens(c))
        cand_risks = {RISK_KEYWORDS.get(t, t) for t in ct if (t in RISK_KEYWORDS or t in RISK_KEYWORDS.values())}
        if cand_risks and not (cand_risks & present_risks):
            continue
        j = len(bt & ct) / (len(bt | ct) + 1e-8)
        if j >= min_sim:
            scored.append((j, c))
    scored.sort(key=lambda x: x[0], reverse=True)
    return [c for _, c in scored[:limit]]

# -------------------- ì‚¬ë¡€/ì˜ˆë°© ìì—°í™” ë³´ì¡° --------------------
def naturalize_case_sentence(s: str) -> str:
    s = soften(s)
    death = re.search(r"ì‚¬ë§\s*(\d+)\s*ëª…", s)
    inj = re.search(r"ì‚¬ìƒ\s*(\d+)\s*ëª…", s)
    unconscious = re.search(r"ì˜ì‹ë¶ˆëª…", s)
    info = []
    if death: info.append(f"ê·¼ë¡œì {death.group(1)}ëª… ì‚¬ë§")
    if inj and not death: info.append(f"{inj.group(1)}ëª… ì‚¬ìƒ")
    if unconscious: info.append("ì˜ì‹ë¶ˆëª… ë°œìƒ")
    m = re.search(DATE_PAT, s); date_txt=""
    if m:
        y, mo, d = m.groups()
        y = int(str(y).replace("â€™","").replace("'","")); y = 2000 + y if y < 100 else y
        date_txt = f"{int(y)}ë…„ {int(mo)}ì›” {int(d)}ì¼, "
        s = s.replace(m.group(0), "").strip()
    s = s.strip(" ,.-")
    if not re.search(r"(ë‹¤\.|ì…ë‹ˆë‹¤\.|í–ˆìŠµë‹ˆë‹¤\.)$", s):
        if re.search(r"(ì‚¬ë§|ì‚¬ìƒ|ì¤‘ë…|ì¶”ë½|ë‚™í•˜|ë¶•ê´´|ì§ˆì‹|ë¼ì„|ê¹”ë¦¼|ë¶€ë”ªí˜|ê°ì „|í­ë°œ)\s*$", s):
            s = s.rstrip(" .") + "í–ˆìŠµë‹ˆë‹¤."
        elif re.search(r"(ì‚¬ê±´|ì‚¬ê³ )\s*$", s):
            s = s.rstrip(" .") + "ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
        else:
            s = s.rstrip(" .") + " ì‚¬ê³ ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
    if info and not s.endswith("í–ˆìŠµë‹ˆë‹¤."):
        s = tidy_korean_spaces(s.rstrip(" .") + " " + (", ".join(info)) + "í–ˆìŠµë‹ˆë‹¤.")
    return tidy_korean_spaces((date_txt + s).strip())

# -------------------- Fallback ì¶”ì¶œê¸°(í—¤ë” ì—†ì„ ë•Œ) --------------------
def fallback_extract_cases(text: str, sents: List[str]) -> List[str]:
    from_cluster = extract_clusters_by_type(text, "case")
    from_sents = [x for x in sents if is_accident_sentence(x)]
    pool = from_cluster + from_sents
    pool = stitch_case_blocks(pool)
    seen, out = set(), []
    for x in pool:
        k = re.sub(r"\s+","", x)
        if k not in seen:
            seen.add(k); out.append(x)
    return out

def fallback_extract_preventions(text: str, sents: List[str]) -> List[str]:
    from_cluster = extract_clusters_by_type(text, "action")
    from_sents = [x for x in sents if is_prevention_sentence(x)]
    pool = from_cluster + from_sents
    pool = repair_action_fragments(pool)
    norm = [to_action_sentence(x, text) for x in pool if is_meaningful_sentence(x)]
    seen, out = set(), []
    for x in norm:
        k = re.sub(r"\s+","", x)
        if k not in seen:
            seen.add(k); out.append(x)
    return out

# -------------------- ë¼ë²¨ë§ --------------------
def drop_label_token(t: str) -> bool:
    if t in STOP_TERMS: return True
    for pat in LABEL_DROP_PAT:
        if re.match(pat, t): return True
    if t in {"ì†Œì¬","ì†Œì¬ì§€","ì§€ì—­","ì¥ì†Œ","ë²„ìŠ¤","ì˜ì—…ì†Œ","ì—…ì²´","ìë£Œ","í‚¤","ë©”ì„¸ì§€","ëª…","ì•ˆì „ë³´ê±´"}:
        return True
    return False

def top_terms_for_label(text: str, k: int=3) -> List[str]:
    doc_cnt = Counter([t for t in tokens(text) if not drop_label_token(t)])
    bonus = Counter()
    for t in list(doc_cnt.keys()):
        if t in RISK_KEYWORDS:
            bonus[RISK_KEYWORDS[t]] += doc_cnt[t]
    doc_cnt += bonus
    kb = st.session_state["kb_terms"]
    if kb:
        for t, c in kb.items():
            if not drop_label_token(t):
                doc_cnt[t] += 0.2 * c
    if not doc_cnt: return ["ì•ˆì „ë³´ê±´","êµìœ¡"]
    commons = {"ì•ˆì „","êµìœ¡","ì‘ì—…","í˜„ì¥","ì˜ˆë°©","ì¡°ì¹˜","í™•ì¸","ê´€ë¦¬","ì ê²€","ê°€ì´ë“œ","ì§€ì¹¨"}
    cand = [(t, doc_cnt[t]) for t in doc_cnt if t not in commons and len(t) >= 2]
    if not cand: cand = list(doc_cnt.items())
    cand.sort(key=lambda x: x[1], reverse=True)
    return [t for t,_ in cand[:k]]

def dynamic_topic_label(text: str) -> str:
    terms = top_terms_for_label(text, k=3)
    risks = [RISK_KEYWORDS.get(t, t) for t in terms if t in RISK_KEYWORDS or t in RISK_KEYWORDS.values()]
    extra = [t for t in terms if t not in risks]
    label_core = " ".join(sorted(set(risks), key=risks.index)) or "ì•ˆì „ë³´ê±´"
    tail = " ".join(extra[:1])
    label = (label_core + (" " + tail if tail else "")).strip()
    if "ì¬í•´ì˜ˆë°©" not in label:
        label += " ì¬í•´ì˜ˆë°©"
    return label

# -------------------- ìš”ì•½/ìƒì„±(LLM-FREE) --------------------
def ai_extract_summary(text: str, limit: int=8) -> List[str]:
    sents = preprocess_text_to_sentences(text)
    if not sents: return []
    kb = st.session_state["kb_terms"]; total = sum(kb.values()) or 1
    kb_boost = {t: 1.0 + (cnt/total)*3.0 for t, cnt in kb.items()} if kb else None
    X, _ = sentence_tfidf_vectors(sents, kb_boost=kb_boost)
    scores = textrank_scores(sents, X)
    idx = mmr_select(sents, scores, X, limit, lam=0.7)
    return [sents[i] for i in idx]

def make_structured_script(text: str, max_points: int=6) -> str:
    topic_label = dynamic_topic_label(text)

    core = [soften(s) for s in ai_extract_summary(text, max_points)] if max_points > 0 else []
    core_actions = [s for s in core if (re.search(ACTION_PAT, s) or is_prevention_sentence(s))]
    core_actions = repair_action_fragments(core_actions)

    # 1) í—¤ë” ê¸°ë°˜ ì„¹ì…˜
    case_block_raw = extract_section_bullets(text, which="case")
    prev_block_raw = extract_section_bullets(text, which="prev")

    # 2) í—¤ë” ì—†ì„ ë•Œ í´ëŸ¬ìŠ¤í„°/ë¬¸ì¥ ê¸°ë°˜ ì¶”ì¶œ
    sents_all = preprocess_text_to_sentences(text)
    if not case_block_raw:
        case_block_raw = fallback_extract_cases(text, sents_all)
    if not prev_block_raw:
        prev_block_raw = fallback_extract_preventions(text, sents_all)

    cases_block = [naturalize_case_sentence(s) for s in case_block_raw if is_meaningful_sentence(s)]
    prev_block_raw = repair_action_fragments(prev_block_raw)
    prev_block = [to_action_sentence(s, text) for s in prev_block_raw if is_meaningful_sentence(s)]

    # 3) ìš”ì•½ ë³´ì¡°
    case_aux, risk_aux, ask_aux = [], [], []
    for s in core:
        if is_accident_sentence(s): case_aux.append(naturalize_case_sentence(s))
        elif is_risk_sentence(s):   risk_aux.append(soften(s))
        elif ("?" in s or "í™•ì¸" in s or "ì ê²€" in s):
            ask_aux.append(soften(s if s.endswith("?") else s + " ë§ìŠµë‹ˆê¹Œ?"))

    # 4) í–‰ë™ ë³´ì¡°
    act_aux = [to_action_sentence(s, text) for s in core_actions if is_meaningful_sentence(s)]

    # 5) KB ë³´ê°•(ì˜ˆë°© ë¶€ì¡± ì‹œ ë¬´ì œí•œ í™•ì¥)
    acts = prev_block + act_aux
    if len(acts) < 3 and st.session_state["kb_actions"]:
        acts += kb_match_candidates(st.session_state["kb_actions"], text, 8, min_sim=0.10)

    def uniq_keep(seq: List[str]) -> List[str]:
        seen, out = set(), []
        for x in seq:
            k = re.sub(r"\s+","", x)
            if k not in seen:
                seen.add(k); out.append(x)
        return out

    cases = uniq_keep(cases_block + case_aux)   # ì‚¬ë¡€: ì œí•œ ì—†ìŒ
    risks  = uniq_keep(risk_aux)
    asks   = uniq_keep(ask_aux or kb_match_candidates(st.session_state["kb_questions"], text, 4, min_sim=0.10))
    acts   = uniq_keep(acts)                    # ì˜ˆë°©: ì „ë¶€ ì¶œë ¥

    lines = []
    lines.append(f"ğŸ¦º TBM êµìœ¡ëŒ€ë³¸ â€“ {topic_label}\n")
    lines.append("â— ë„ì…")
    lines.append(f"ì˜¤ëŠ˜ì€ ìµœê·¼ ë°œìƒí•œ '{topic_label.replace(' ì¬í•´ì˜ˆë°©','')}' ì‚¬ê³  ì‚¬ë¡€ë¥¼ ì¤‘ì‹¬ìœ¼ë¡œ, ìš°ë¦¬ í˜„ì¥ì—ì„œ ê°™ì€ ì‚¬ê³ ë¥¼ ì˜ˆë°©í•˜ê¸° ìœ„í•œ ì•ˆì „ì¡°ì¹˜ë¥¼ í•¨ê»˜ ì‚´í´ë³´ê² ìŠµë‹ˆë‹¤.\n")

    if cases:
        lines.append("â— ì‚¬ê³  ì‚¬ë¡€")
        for c in cases:
            lines.append(f"- {c}")
        lines.append("")

    if risks:
        lines.append("â— ì£¼ìš” ìœ„í—˜ìš”ì¸")
        for r in risks:
            lines.append(f"- {r}")
        lines.append("")

    if acts:
        lines.append("â— ì˜ˆë°©ì¡°ì¹˜ / ì‹¤ì²œ ìˆ˜ì¹™")
        for i, a in enumerate(acts, 1):
            lines.append(f"{i}ï¸âƒ£ {a}")
        lines.append("")

    if asks:
        lines.append("â— í˜„ì¥ ì ê²€ ì§ˆë¬¸")
        for q in asks:
            lines.append(f"- {q}")
        lines.append("")

    lines.append("â— ë§ˆë¬´ë¦¬ ë‹¹ë¶€")
    lines.append("ì˜ˆë°©ì¡°ì¹˜ëŠ” 'ì„ ì¡°ì¹˜ í›„ì‘ì—…'ì´ ì›ì¹™ì…ë‹ˆë‹¤. ì˜¤ëŠ˜ ì‘ì—… ì „, ê° ê³µì •ë³„ ìœ„í—˜ìš”ì¸ì„ ë‹¤ì‹œ í•œ ë²ˆ ì ê²€í•˜ê³  í•„ìš”í•œ ë³´í˜¸êµ¬ì™€ ì•ˆì „ì¡°ì¹˜ë¥¼ ë°˜ë“œì‹œ ì¤€ë¹„í•©ì‹œë‹¤.")
    lines.append("â— êµ¬í˜¸")
    lines.append("â€œí•œ ë²ˆ ë” í™•ì¸! í•œ ë²ˆ ë” ì ê²€!â€")
    return "\n".join(lines)

def make_concise_report(text: str, max_points: int=6) -> str:
    # [í•µì‹¬ìš”ì•½] ì‹¤í–‰ ê²½ë¡œ â€” ê¸°ì¡´ UI ìœ ì§€
    sents = ai_extract_summary(text, max_points)
    sents = [soften(s) for s in sents if not re.match(r"(ë°°í¬ì²˜|ì£¼ì†Œ|í™ˆí˜ì´ì§€|VR|ë¦¬í”Œë¦¿)", s)]
    cases_blk = [naturalize_case_sentence(s) for s in extract_section_bullets(text, "case")] or \
                [naturalize_case_sentence(s) for s in fallback_extract_cases(text, preprocess_text_to_sentences(text))]
    prev_blk  = [to_action_sentence(s, text) for s in repair_action_fragments(
                    extract_section_bullets(text, "prev") or fallback_extract_preventions(text, preprocess_text_to_sentences(text))
                 )]

    act_src = [s for s in sents if (not is_accident_sentence(s)) and (is_prevention_sentence(s) or re.search(ACTION_PAT, s))]
    act_src = repair_action_fragments(act_src)
    cases = [naturalize_case_sentence(s) for s in sents if is_accident_sentence(s)]
    risks  = [soften(s) for s in sents if (not is_accident_sentence(s)) and is_risk_sentence(s)]
    acts   = [to_action_sentence(s, text) for s in act_src]

    def uniq_keep(seq: List[str]) -> List[str]:
        seen, out = set(), []
        for x in seq:
            k = re.sub(r"\s+","", x)
            if k not in seen:
                seen.add(k); out.append(x)
        return out

    cases = uniq_keep(cases_blk + cases)[:6]
    risks  = uniq_keep(risks)[:6]
    acts   = uniq_keep(prev_blk + acts)[:12]

    topic = dynamic_topic_label(text)
    lines = [f"ğŸ“„ í•µì‹¬ìš”ì•½ â€” {topic}\n"]
    if cases:
        lines.append("ã€ì‚¬ê³  ê°œìš”ã€‘"); lines.append("ìë£Œì—ì„œ í™•ì¸ëœ ì£¼ìš” ì‚¬ê³ ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.")
        for c in cases: lines.append(f"- {c}")
        lines.append("")
    if risks:
        lines.append("ã€ì£¼ìš” ìœ„í—˜ìš”ì¸ã€‘"); lines.append("ìë£Œ ì „ë°˜ì—ì„œ ë‹¤ìŒ ìš”ì¸ì´ ë°˜ë³µì ìœ¼ë¡œ ë‚˜íƒ€ë‚¬ìŠµë‹ˆë‹¤.")
        for r in risks: lines.append(f"- {r}")
        lines.append("")
    if acts:
        lines.append("ã€ì˜ˆë°©/ì‹¤ì²œ ìš”ì•½ã€‘"); lines.append("í˜„ì¥ì—ì„œ ì¦‰ì‹œ ì ìš© ê°€ëŠ¥í•œ í•µì‹¬ ìˆ˜ì¹™ì…ë‹ˆë‹¤.")
        for a in acts: lines.append(f"- {a}")
        lines.append("")
    if not (cases or risks or acts):
        lines.append("ìë£Œì˜ í•µì‹¬ì„ ê°„ë‹¨íˆ ì •ë¦¬í•˜ë©´ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.")
        for s in sents: lines.append(f"- {s}")
    return "\n".join(lines)

# -------------------- DOCX ë‚´ë³´ë‚´ê¸° --------------------
_XML_FORBIDDEN = r"[\x00-\x08\x0B\x0C\x0E-\x1F\uD800-\uDFFF\uFFFE\uFFFF]"
def _xml_safe(s: str) -> str:
    if not isinstance(s, str): s = "" if s is None else str(s)
    return rxx.sub(_XML_FORBIDDEN, "", s)

def to_docx_bytes(script: str) -> bytes:
    # python-docx: ë§êµ½(ë§‘ì€ ê³ ë”•) ê¸°ë³¸ í°íŠ¸ ì ìš©, ì¤„ë°”ê¿ˆ/í•œê¸€ ì•ˆì „ ë¬¸ìë§Œ ì‚½ì…
    doc = Document()
    try:
        style = doc.styles["Normal"]; style.font.name = "Malgun Gothic"; style.font.size = Pt(11)
    except Exception: pass
    for raw in script.split("\n"):
        line = _xml_safe(raw)
        p = doc.add_paragraph(line)
        for run in p.runs:
            try:
                run.font.name = "Malgun Gothic"; run.font.size = Pt(11)
            except Exception: pass
    bio = io.BytesIO(); doc.save(bio); bio.seek(0)
    return bio.read()

# -------------------- UI(ê¸°ì¡´ êµ¬ì„± ìœ ì§€) --------------------
with st.sidebar:
    st.header("â„¹ï¸ ì†Œê°œ / ì‚¬ìš©ë²•")
    st.markdown("""
**AI íŒŒì´í”„ë¼ì¸(ì™„ì „ ë¬´ë£Œ, ì˜¤í”ˆì†ŒìŠ¤ë§Œ ì‚¬ìš©)**  
1) ì „ì²˜ë¦¬(ë…¸ì´ì¦ˆ ì œê±°/ì¤„ ë³‘í•©/ë‚ ì§œ-ì‚¬ê³  ê²°í•©)  
2) **ì‚¬ë¡€ ë¸”ë¡ ë³‘í•©**(ì—°ê²°ì–´Â·í‚¤ì›Œë“œë¡œ ì—°ì† ì„œìˆ ì„ í•œ ë¬¸ì¥ìœ¼ë¡œ)  
3) **í—¤ë” ì—†ì„ ë•Œë„** ë¶ˆë¦¿ í´ëŸ¬ìŠ¤í„° ìë™ ë¶„ë¥˜(ì‚¬ë¡€í˜•/ì˜ˆë°©í˜•)  
4) TextRank + MMR ìš”ì•½ (**ì„¸ì…˜ KB ê°€ì¤‘ì¹˜** ë°˜ì˜)  
5) ê·œì¹™í˜• NLG: ì¡°ì‚¬/ë„ì–´ì“°ê¸°Â·ì¢…ê²° ë³´ì •, **ì˜ˆë°©ì¡°ì¹˜ ì¤„ê²°í•© ë° ë¬¸ë§¥ ë³´ì •**  
6) ê²°ê³¼ í¬ë§·: **ìì—°ìŠ¤ëŸ¬ìš´ êµìœ¡ëŒ€ë³¸(ë¬´ë£Œ)** / **í•µì‹¬ìš”ì•½**  
*NEW: í—¤ë”ê°€ ì—†ì–´ë„ ì‚¬ë¡€Â·ì˜ˆë°©ì„ ìë™ ìˆ˜ì§‘í•©ë‹ˆë‹¤(ì„¹ì…˜ íŒŒì„œ + í´ëŸ¬ìŠ¤í„° + Fallback).*
""")
    st.session_state["domain_toggle"] = st.toggle(
        "ğŸ”§ ë„ë©”ì¸ í…œí”Œë¦¿ ê°•í™”(ì‹ ì¤‘ ì ìš©)",
        value=False,
        help="ë¬¸ì¥Â·ë³¸ë¬¸ íŠ¸ë¦¬ê±° ì¼ì¹˜ + ìœ ì‚¬ë„ ê¸°ì¤€ ì¶©ì¡± ì‹œì—ë§Œ í…œí”Œë¦¿ì„ ì ìš©í•©ë‹ˆë‹¤."
    )

seed_kb_once()
st.title("ğŸ¦º OPS/í¬ìŠ¤í„°ë¥¼ êµìœ¡ ëŒ€ë³¸ìœ¼ë¡œ ìë™ ë³€í™˜ (ì™„ì „ ë¬´ë£Œ)")

def reset_all():
    # ì „ì²´ ì´ˆê¸°í™”(ì„¸ì…˜ KB/ìºì‹œ/ì—…ë¡œë” í‚¤) â€” UIëŠ” ê·¸ëŒ€ë¡œ
    st.session_state.pop("manual_text", None)
    st.session_state.pop("edited_text", None)
    st.session_state.pop("zip_choice", None)
    st.session_state["kb_terms"] = Counter()
    st.session_state["kb_actions"] = []
    st.session_state["kb_questions"] = []
    st.session_state["uploader_key"] += 1
    st.session_state["seed_loaded"] = False
    st.session_state["last_file_diag"] = {}
    st.session_state["last_extracted_cache"] = ""
    st.rerun()

col_top1, col_top2 = st.columns([4,1])
with col_top2:
    st.button("ğŸ§¹ ì´ˆê¸°í™”", on_click=reset_all, use_container_width=True)

st.markdown("**ì•ˆë‚´**  \n- í…ìŠ¤íŠ¸ê°€ í¬í•¨ëœ PDF ë˜ëŠ” ë³¸ë¬¸ í…ìŠ¤íŠ¸ë¥¼ ê¶Œì¥í•©ë‹ˆë‹¤.  \n- ì´ë¯¸ì§€/ìŠ¤ìº” PDFëŠ” í˜„ì¬ OCR ë¯¸ì§€ì›ì…ë‹ˆë‹¤.")

col1, col2 = st.columns([1,1], gap="large")

with col1:
    uploaded = st.file_uploader(
        "OPS ì—…ë¡œë“œ (PDF ë˜ëŠ” ZIP) â€¢ í…ìŠ¤íŠ¸ PDF ê¶Œì¥",
        type=["pdf","zip"],
        key=f"uploader_{st.session_state['uploader_key']}"
    )
    manual_text = st.text_area(
        "ë˜ëŠ” OPS í…ìŠ¤íŠ¸ ì§ì ‘ ë¶™ì—¬ë„£ê¸°",
        key="manual_text",
        height=220,
        placeholder="ì˜ˆ: í˜„ì¥ ì•ˆë‚´ë¬¸ ë˜ëŠ” OPS ë³¸ë¬¸ í…ìŠ¤íŠ¸â€¦"
    )

    extracted: str = ""
    zip_pdfs: Dict[str, bytes] = {}

    if uploaded is not None:
        fname = (uploaded.name or "").lower()
        try:
            raw_bytes = uploaded.getvalue()
        except Exception:
            raw_bytes = uploaded.read()

        if fname.endswith(".zip"):
            try:
                with zipfile.ZipFile(io.BytesIO(raw_bytes), "r") as zf:
                    for name in zf.namelist():
                        if name.lower().endswith(".pdf"):
                            data = zf.read(name); zip_pdfs[name] = data
                if zip_pdfs:
                    for nm, data in zip_pdfs.items():
                        txt_all = read_pdf_text_from_bytes(data, fname=f"{fname}::{nm}")
                        if txt_all.strip():
                            kb_ingest_text(txt_all)
                    kb_prune()
                    first_name = sorted(zip_pdfs.keys())[0]
                    extracted = read_pdf_text_from_bytes(zip_pdfs[first_name], fname=first_name)
                    if extracted.strip():
                        st.session_state["edited_text"] = extracted
                        st.session_state["last_extracted_cache"] = extracted
                    st.success(f"ZIP ê°ì§€: {len(zip_pdfs)}ê°œ PDF, ì²« ë¬¸ì„œ ìë™ ì„ íƒ â†’ {first_name}")
                else:
                    st.error("ZIP ë‚´ì— PDFê°€ ì—†ìŠµë‹ˆë‹¤.")
            except Exception as e:
                st.error(f"ZIP í•´ì œ ì˜¤ë¥˜: {e}")

            if zip_pdfs:
                chosen = st.selectbox("ZIP ë‚´ PDF ì„ íƒ", sorted(zip_pdfs.keys()), key="zip_choice")
                if chosen and zip_pdfs.get(chosen):
                    extracted2 = read_pdf_text_from_bytes(zip_pdfs[chosen], fname=chosen)
                    if extracted2.strip():
                        st.session_state["edited_text"] = extracted2
                        st.session_state["last_extracted_cache"] = extracted2

        elif fname.endswith(".pdf"):
            extracted = read_pdf_text_from_bytes(raw_bytes, fname=fname)
            if extracted.strip():
                kb_ingest_text(extracted); kb_prune()
                st.session_state["edited_text"] = extracted
                st.session_state["last_extracted_cache"] = extracted
            else:
                st.warning("âš ï¸ PDFì—ì„œ ìœ íš¨í•œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        else:
            st.warning("ì§€ì›í•˜ì§€ ì•ŠëŠ” í˜•ì‹ì…ë‹ˆë‹¤. PDF ë˜ëŠ” ZIPì„ ì—…ë¡œë“œí•˜ì„¸ìš”.")

    pasted = (manual_text or "").strip()
    if pasted:
        kb_ingest_text(pasted); kb_prune()
        st.session_state["edited_text"] = pasted
        st.session_state["last_extracted_cache"] = pasted

    base_text = st.session_state.get("edited_text","")
    st.markdown("**ì¶”ì¶œ/ì…ë ¥ í…ìŠ¤íŠ¸ ë¯¸ë¦¬ë³´ê¸°**")
    edited_text = st.text_area("í…ìŠ¤íŠ¸", value=base_text, height=240, key="edited_text")

    with st.expander("ğŸ§ª íŒŒì¼ ì½ê¸° ì§„ë‹¨(Log-lite)", expanded=False):
        diag = st.session_state.get("last_file_diag", {})
        if diag:
            st.write({
                "íŒŒì¼ëª…": diag.get("name"),
                "í¬ê¸°(bytes)": diag.get("size_bytes"),
                "ì¶”ì¶œëœ ë¬¸ììˆ˜": diag.get("extracted_chars"),
                "ë©”ëª¨": diag.get("note"),
            })
        st.caption(f"í˜„ì¬ í…ìŠ¤íŠ¸ ë°•ìŠ¤ ê¸¸ì´: {len(st.session_state.get('edited_text',''))} chars")

with col2:
    gen_mode = st.selectbox("ğŸ§  ìƒì„± ëª¨ë“œ", ["í•µì‹¬ìš”ì•½","ìì—°ìŠ¤ëŸ¬ìš´ êµìœ¡ëŒ€ë³¸(ë¬´ë£Œ)"])
    max_points = st.slider("ìš”ì•½ ê°•ë„(í•µì‹¬ë¬¸ì¥ ê°œìˆ˜)", 3, 10, 6)

    if st.button("ğŸ› ï¸ ëŒ€ë³¸ ìƒì„±", type="primary", use_container_width=True):
        text_for_gen = (st.session_state.get("edited_text") or "").strip()
        if not text_for_gen:
            text_for_gen = (st.session_state.get("last_extracted_cache") or "").strip()
            if text_for_gen:
                st.info("ë¹ˆ ì…ë ¥ì„ ìµœê·¼ ì¶”ì¶œ í…ìŠ¤íŠ¸ë¡œ ìë™ ëŒ€ì²´í–ˆìŠµë‹ˆë‹¤.")
        if not text_for_gen:
            st.warning("PDF/ZIP ì—…ë¡œë“œ ë˜ëŠ” í…ìŠ¤íŠ¸ ì…ë ¥ í›„ ì‹œë„í•˜ì„¸ìš”.")
        else:
            with st.spinner("ìƒì„± ì¤‘..."):
                if gen_mode == "ìì—°ìŠ¤ëŸ¬ìš´ êµìœ¡ëŒ€ë³¸(ë¬´ë£Œ)":
                    script = make_structured_script(text_for_gen, max_points=max_points)
                    subtitle = "ìì—°ìŠ¤ëŸ¬ìš´ êµìœ¡ëŒ€ë³¸(ë¬´ë£Œ)"
                else:
                    script = make_concise_report(text_for_gen, max_points=max_points)
                    subtitle = "í•µì‹¬ìš”ì•½"
            st.success(f"ìƒì„± ì™„ë£Œ! ({subtitle})")
            st.text_area("ê²°ê³¼ ë¯¸ë¦¬ë³´ê¸°", value=script, height=420)
            c3, c4 = st.columns(2)
            with c3:
                st.download_button(
                    "â¬‡ï¸ TXT ë‹¤ìš´ë¡œë“œ",
                    data=_xml_safe(script).encode("utf-8"),
                    file_name="tbm_output.txt",
                    use_container_width=True
                )
            with c4:
                st.download_button(
                    "â¬‡ï¸ DOCX ë‹¤ìš´ë¡œë“œ",
                    data=to_docx_bytes(script),
                    file_name="tbm_output.docx",
                    use_container_width=True
                )

st.caption("ì™„ì „ ë¬´ë£Œ(LLM ë¯¸ì‚¬ìš©). í—¤ë” ìœ ë¬´ ìƒê´€ì—†ì´ ì‚¬ë¡€/ì˜ˆë°©ì„ ìë™ íƒì§€í•©ë‹ˆë‹¤(ì„¹ì…˜ íŒŒì„œ+í´ëŸ¬ìŠ¤í„°+Fallback).")
